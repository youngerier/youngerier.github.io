<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL开发规范]]></title>
    <url>%2F2021%2F04%2F14%2FMySQL%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[MySQL开发规范一 表设计1.1 命名库名、表名、字段名必须使用英文单词（单数）表示表的意义，一个单词不足以表示时两者之间使用”_”（表与字段）分割。 原则上不能超过20个字符 table命名:(单数，多个单词，”_”分隔） Bad Example: csw_creditcard_bills Good Example: csw_creditcard_bill index命名: 一般索引（Btree）命名： Bad Example: index_uid_accid_type(uid, type, acid) Good Example: idx_uid_type_acid(uid, type, acid)(索引表包含索引的所有信息，顺序一致，准确无误)。 唯一键索引命名： Bad Example: bill_no (bill_no) Good Example: uk_bill_no (bill_no)。 字段的命名: 定义意义完整的单词或组合单词作为字段名,”_”分隔不同意义单词 Bad Example： 字段名：name Good Example： 字段名：xxx_name 因为名称可以有多个，昵称，登录名等等。 1.2 字段类型使用VARCHAR类型( 0-65535 Bytes) 适用于动态长度字符串,尽量根据业务需求定义合适的字段长度，不要为了图省事，直接定义为varchar(1024)或更长等等。 Char类型（字符数0-255） 适合所有固定字符串，如UUID char(36).不要使用Varchar类型。 Text 类型 仅当字符数量可能超过 20000 个的时候，才可以使用 TEXT 类型来存放字符类数据 （原因在于varchar最多可存65535Bytes，utf8字符集下每字符占用3个字节，65535/3=20000+）所有使用 TEXT 类型的字段必须和原表进行分拆，与原表主键单独组成另外一个表进行存放.（原因在于，数据库按行扫描并获取数据，如果存在text字段，将大大增加行检索成本，分出去以后可按原表主键再获取text值。 禁用VARBINARY、BLOB存储图片、文件等 Enum类型 用 0， 1 ，2 等数字tinyint类型来代替Enum Date类型 所有需要精确到时间（时分秒）的字段均使用TIMESTAMP 所有只需要精确到天的字段全部使用 DATE 类型，而不应该使用 TIMESTAMP 或者DATETIME 类型。 Integer 类型 所有整数类型的字段尽量使用合适的大小，且明确标识出为无符号型(UNSIGNED)，除非确实会出现负数，仅仅当该字段数字取值会超过22亿，才使用 BIGINT 类型123456789101112类型 字节 最小值 最大值 (带符号的/无符号的) (带符号的/无符号的)TINYINT 1 -128 127 0 255SMALLINT 2 -32768 32767 0 65535MEDIUMINT 3 -8388608 8388607 0 16777215INT 4 -2147483648 2147483647 0 4294967295BIGINT 8 -9223372036854770000 9223372036854770000 0 18446744073709500000 浮点数类型 存储精确浮点数必须使用DECIMAL替代FLOAT和DOUBLE或转为整形(推荐)字段 1.3 设计 所有表必须包含三列，自增ID主键，unsigned数据类型,created_at(记录创建时间),updated_at(记录更新时间) 1234567Example：create table t1(id int unsigned NOT NULL AUTO_INCREMENT,created_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,updated_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,PRIMARY KEY (`id`)) ENGINE=InnoDB comment &apos;表描述&apos;; 尽量做好适当的字段冗余(例：uid) 123456789Example:create table t1 ( id int unsigned NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, uid int unsigned NOT NULL COMMENT &apos;用户ID&apos;, created_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, updated_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,PRIMARY KEY (id)) ENGINE=InnoDB comment &apos;table描述&apos;; MySQL5.6之后timestamp可以支持多列字段拥有自动插入时间和自动更新时间：DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP MySQL5.6之之前timestamp仅支持一列字段拥有自动插入时间和自动更新时间：DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP 索引字段属性设置为not null default 值（不能索引Null值） 添加字段时不可带after,before,新加字段在表末尾添加 表字符集采用utf8，不区分大小写（不使用utf8-bin） 表存储引擎Innodb 表及字段必须使用中文注释，简洁明了 线上涉及表结构变更需至少提前24小时通知dba，大表提前1周。（在开发阶段与dba讨论db设计) 状态型及类型列使用tinyint即可，原则上不能建索引（如status，type） 禁止使用force index 等加hint方式使用索引 二 索引设计 索引名称必须使用小写 非唯一索引必须按照“idx_字段名”进行命名 唯一索引必须按照“uk_字段名”进行命名 索引中的字段数建议不超过5个，单张表的索引数量控制在5个以内 ORDER BY，GROUP BY，DISTINCT的字段尽量使用索引 使用EXPLAIN判断SQL语句是否合理使用索引，尽量避免extra列出现：Using File Sort，Using Temporary 对长度过长的VARCHAR字段必须建立索引时，使用Hash字段建立索引或者使用前缀索引,例：(idx(clumn(8))) 合理创建联合索引，一定注意重复率低及高频查询字段作为前缀索引 Bad Example： 12345678910create table t1(id int unsigned NOT NULL AUTO_INCREMENT,uid int unsigned NOT NULL comment &apos;用户id&apos;status tinyint not null default 0 comment &apos;状态描述&apos;,type tinyint not null default 0 comment &apos;字段类型&apos;,created_time NOT NULL DEFAULT CURRENT_TIMESTAMP,updated_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,PRIMARY KEY (`id`),unique key uk_status_type_uid(status,type,uid)) ENGINE=InnoDB DEFAULT CHARSET=utf8 comment &apos;表描述&apos;; Good Example:12345678910create table t1(id int unsigned NOT NULL AUTO_INCREMENT,uid int unsigned NOT NULL comment &apos;用户id&apos;status tinyint not null default 0 comment &apos;状态描述&apos;,type tinyint not null default 0 comment &apos;字段类型&apos;,created_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,updated_time timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,PRIMARY KEY (`id`),zunique key uk_uid_status_type(uid,status,type)) ENGINE=InnoDB DEFAULT CHARSET=utf8 comment &apos;表描述&apos;; innodb 所有表必须建ID主键 索引设计尽可能够用就好，避免无效索引 三 SQL编写 WHERE条件中必须使用合适的数据类型，避免MySQL进行隐式类型转化 SELECT、INSERT语句必须显式的指明业务所需的字段名称，如果取值不是全表，希望指定列名，不允许select 避免复杂sql，降低业务耦合，方便之后的scale out 和sharding（如之后查询多个数据库等并发进行） 对于结果较多的分页查询 Bad Example： 1SELECT * FROM relation where biz_type =&apos;0&apos; AND end_time &gt;=&apos;2014-05-29&apos; ORDER BY id asc LIMIT 149420 ,20; Good Example：1SELECT a.* FROM relation a, (select id from relation where biz_type =&apos;0&apos; AND end_time &gt;=&apos;2014-05-29&apos; ORDER BY id asc LIMIT 149420 ,20 ) b where a.id=b.id;。 避免在SQL语句进行数学运算或者函数运算（应在程序端完成） 用in or join代替子查询，in包含的值不易超过2000(整形) 表连接规范避免使用JOIN 所有非外连接的 SQL 不要使用 Join … On … 方式进行连接，而直接通过普通的 Where条件关联方式。外连接的 SQL 语句，可以使用 Left Join … On 的 Join 方式，且所有外连接一律写成 Left Join，而不要使用 Right Join。 Bad Example:1select a.id,b.id from a join b on a.id = b.a_id where Good Example: 1select a.id,b.id from a,b where a.id = b.a_id and ... 禁止insert、update、delete中做跨库操作，例：insert … wac.tbl_user 业务逻辑中禁止使用存储过程、触发器、函数、外键等 禁止使用order by rand()，now()函数 如要insert … on duplicate key update需通知dba审核 禁止使用union，用union all代替 用union all 代替 or 以下查询不能索引(not,!=,&lt;&gt;,!&lt;,!&gt;,not exist,not in,not like)(%like) 可用explain观察sql执行计划，show profile 观察sql的性能损耗情况，目前我们环境中 5.6版本可对select，update，delete语句均支持执行计划]]></content>
  </entry>
  <entry>
    <title><![CDATA[gradle-生命周期]]></title>
    <url>%2F2020%2F12%2F15%2Fgradle-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[构建的生命周期任何Gradle的构建过程都分为三部分：初始化阶段、配置阶段和执行阶段。 通过如下代码向Gradle的构建过程添加监听：12345678910111213141516171819gradle.addBuildListener(new BuildListener() &#123; void buildStarted(Gradle var1) &#123; println '开始构建' &#125; void settingsEvaluated(Settings var1) &#123; println 'settings评估完成（settins.gradle中代码执行完毕）' // var1.gradle.rootProject 这里访问Project对象时会报错，还未完成Project的初始化 &#125; void projectsLoaded(Gradle var1) &#123; println '项目结构加载完成（初始化阶段结束）' println '初始化结束，可访问根项目：' + var1.gradle.rootProject &#125; void projectsEvaluated(Gradle var1) &#123; println '所有项目评估完成（配置阶段结束）' &#125; void buildFinished(BuildResult var1) &#123; println '构建结束 ' &#125;&#125;) hook 点 Gradle在构建的各个阶段都提供了很多回调，我们在添加对应监听时要注意，监听器一定要在回调的生命周期之前添加，比如我们在根项目的build.gradle中添加下面的代码就是错误的：123456789gradle.settingsEvaluated &#123; setting -&gt; // do something with setting&#125;gradle.projectsLoaded &#123; gradle.rootProject.afterEvaluate &#123; println 'rootProject evaluated' &#125;&#125; 当构建走到build.gradle时说明初始化过程已经结束了，所以上面的回调都不会执行，把上述代码移动到settings.gradle中就正确了。 通过一些例子来解释如何Hook Gradle的构建过程。 为所有子项目添加公共代码 在根项目的build.gradle中添加如下代码： 1234 gradle.beforeProject &#123; project -&gt; println 'apply plugin java for ' + project project.apply plugin: 'java'&#125; 这段代码的作用是为所有子项目应用Java插件，因为代码是在根项目的配置阶段执行的，所以并不会应用到根项目中。这里说明一下Gradle的beforeProject方法和Project的beforeEvaluate的执行时机是一样的，只是beforeProject应用于所有项目，而beforeEvaluate只应用于调用的Project，上面的代码等价于 123456allprojects &#123; beforeEvaluate &#123; project -&gt; println 'apply plugin java for ' + project project.apply plugin: 'java' &#125;&#125; after*也是同理的，但afterProject还有一点不一样，无论Project的配置过程是否出错，afterProject都会收到回调 为指定Task动态添加Action1234567891011 gradle.taskGraph.beforeTask &#123; task -&gt; task &lt;&lt; &#123; println '动态添加的Action' &#125;&#125;task Test &#123; doLast &#123; println '原始Action' &#125;&#125; 获取构建各阶段耗时情况123456789101112131415161718192021222324252627282930313233343536long beginOfSetting = System.currentTimeMillis()gradle.projectsLoaded &#123; println '初始化阶段，耗时：' + (System.currentTimeMillis() - beginOfSetting) + 'ms'&#125;def beginOfConfigdef configHasBegin = falsedef beginOfProjectConfig = new HashMap()gradle.beforeProject &#123; project -&gt; if (!configHasBegin) &#123; configHasBegin = true beginOfConfig = System.currentTimeMillis() &#125; beginOfProjectConfig.put(project, System.currentTimeMillis())&#125;gradle.afterProject &#123; project -&gt; def begin = beginOfProjectConfig.get(project) println '配置阶段，' + project + '耗时：' + (System.currentTimeMillis() - begin) + 'ms'&#125;def beginOfProjectExcutegradle.taskGraph.whenReady &#123; println '配置阶段，总共耗时：' + (System.currentTimeMillis() - beginOfConfig) + 'ms' beginOfProjectExcute = System.currentTimeMillis()&#125;gradle.taskGraph.beforeTask &#123; task -&gt; task.doFirst &#123; task.ext.beginOfTask = System.currentTimeMillis() &#125; task.doLast &#123; println '执行阶段，' + task + '耗时：' + (System.currentTimeMillis() - task.beginOfTask) + 'ms' &#125;&#125;gradle.buildFinished &#123; println '执行阶段，耗时：' + (System.currentTimeMillis() - beginOfProjectExcute) + 'ms'&#125; 将上述代码段添加到settings.gradle脚本文件的开头，再执行任意构建任务，你就可以看到各阶段、各任务的耗时情况。 动态改变Task依赖关系有时我们需要在一个已有的构建系统中插入我们自己的构建任务，比如在执行Java构建后我们想要删除构建过程中产生的临时文件，那么我们就可以自定义一个名叫cleanTemp的任务，让其依赖于build任务，然后调用cleanTemp任务即可。 寻找插入点如果你对一个构建的任务依赖关系不熟悉的话，可以使用一个插件来查看，在根项目的build.gradle中添加如下代码:123plugins&#123; id &quot;com.dorongold.task-tree&quot; version &quot;1.5&quot;&#125; 然后执行gradle &lt;任务名&gt; taskTree --no-repeat，即可看到指定Task的依赖关系，比如在Java构建中查看build任务的依赖关系： 动态插入自定义任务 我们先定义一个自定的任务cleanTemp，让其依赖于assemble。123456789task cleanTemp(dependsOn: assemble) &#123; doLast &#123; println &apos;清除所有临时文件&apos; &#125;&#125;afterEvaluate &#123; build.dependsOn cleanTemp&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[pwsh-刷新环境变量]]></title>
    <url>%2F2020%2F12%2F11%2Fpwsh-%E5%88%B7%E6%96%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[刷新环境变量code $profile 1234function RefreshEnv &#123; $env:Path = [System.Environment]::GetEnvironmentVariable(&quot;Path&quot;,&quot;Machine&quot;) + &quot;;&quot; + [System.Environment]::GetEnvironmentVariable(&quot;Path&quot;,&quot;User&quot;) Write-host(&apos;RefreshEnv&apos;)&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[jvm-cms垃圾回收]]></title>
    <url>%2F2020%2F10%2F27%2Fjvm-cms%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[浅谈CMS原理CMS（Concurrent Mark Sweep）收集器以获取最短回收停顿时间为目标，是HotSpot虚拟机中第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 过程 初始标记（CMS initial mark）：标记一下GC Roots能直接关联到的对象。需要STW，速度很快。 并发标记（CMS concurrent mark）：进行GC Roots Tracing。不需要STW。会产生浮动垃圾。 重新标记（CMS remark）：找到并发标记期间产生的浮动垃圾。需要STW，停顿时间一般会比初始标记稍长，但远比并发标记短。 并发清除（CMS concurrent sweep）：清除已标记的垃圾。不需要STW。会产生浮动垃圾，只能等下一次GC清理。 重新标记的过程？为什么比并发标记的时间短？不考虑标记对象年龄等操作，最容易想到的原因是： 对象数的区别：“并发标记”过程需要扫描所有对象，标记出不可达的对象（该清除）和可达的对象（不该清除）；“重新标记”只需要扫描在“并发标记”过程中被标记为可达或新创建的对象，检查其是否在“并发标记”过程中被标记为可达之后，由于用户使用变的不可达了并发环境的区别：“并发标记”是与用户线程一起工作的，并发瓶颈较窄（工作线程少+安全检查）；“重新标记”需要stop the world，之后仅有“重新标记”的线程在工作，并发瓶颈宽的多PS：关于对象数的区别要想清楚，在对象分配后，如果对象有一瞬间不可达，则该对象以后都将不可达，可对其清理。因此，重新标记时不需要检查这部分对象。 并发清除的过程？如何才能让用户边使用，边清除？还是那句话： 在对象分配后，如果对象有一瞬间不可达，则该对象以后都将不可达，可对其清理。 所以，已经在“并发标记”和“重新标记”过程被标记为不可达的对象，以后都不会再被用户使用，清除这些对象对用户完全无影响。 唯一可能有影响的是整理内存的过程，不过也只需要同步使用对象和整理对象两个动作。 CMS的优点、缺点？优点： 大部分时间可与用户线程并发工作低停顿缺点： 对CPU资源非常敏感。并发标记和并发清理与用户线程一起工作，如果用户线程也是CPU敏感的，那么必然影响用户线程。无法处理浮动垃圾（Floating Garbage）。并发标记与并发清除过程会产生浮动垃圾，如果CMS之前预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将退化使用Serial Old收集器，重新进行老年代的垃圾收集，这样停顿时间就很长了。可使用XX:CMSInitiatingOccupancyFraction参数设置触发CMS时的老年代空间比例（剩余空间就是预留空间），在JDK1.6中默认为92%。基于“标记-清除算法”，收集结束时会有大量空间碎片产生，导致明明剩余空间充足，却无法为大对象分配足够的连续内存。可打开-XX：+UseCMSCompactAtFullCollection开关参数（默认打开）在进行Full GC之前整理内存碎片（称为“压缩”）；使用-XX:CMSFullGCsBeforeCompaction参数（默认0）设置多少次不带压缩的Full CG之后才进行一次带压缩的Full GC。内存整理无法并行，还需要STW，需要适当调整内存整理的频率，在GC性能与空间利用率之间平衡。 PS： 浮动垃圾：由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处 理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。在这期间用户可能创建新的对象。为了处理这部分浮动垃圾和对象，CMS在并发清理之前，需要预留出足够空间给并发清理期间的用户线程使用。一般会显示使用-XX:CMSInitiatingOccupancyFraction参数设置触发CMS时的老年代空间比例，如果老年代增长不是太快，可以适当提高比例，以减少Full GC的次数。关于浮动垃圾和内存碎片的问题。HDFS namenode在堆内存达到100G规模时，通常设置75%触发Full GC，不开启压缩，优先考虑STW造成的延迟。]]></content>
  </entry>
  <entry>
    <title><![CDATA[jvm-oom异常原因]]></title>
    <url>%2F2020%2F10%2F27%2Fjvm-oom%E5%BC%82%E5%B8%B8%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[java.lang.OutOfMemoryError:Java heap space这是最常见的OOM原因。 堆中主要存放各种对象实例，还有常量池等结构。当JVM发现堆中没有足够的空间分配给新对象时，抛出该异常。具体来讲，在刚发现空间不足时，会先进行一次Full GC，如果GC后还是空间不足，再抛出异常。 引起空间不足的原因主要有： 业务高峰，创建对象过多内存泄露内存碎片严重，无法分配给大对象 java.lang.OutOfMemoryError:Metaspace方法区主要存储类的元信息，实现在元数据区。当JVM发现元数据区没有足够的空间分配给加载的类时，抛出该异常。 引起元数据区空间不足的原因主要有： 加载的类太多，常见于Tomcat等容器中但是元数据区被实现在堆外，主要受到进程本身的内存限制，这种实现下很难溢出。 java.lang.OutOfMemoryError:Unable to create new native thread以Linux系统为例，JVM创建的线程与操作系统中的线程一一对应，受到以下限制： 进程和操作系统的内存资源限制。其中，一个JVM线程至少要占用OS的线程栈+JVM的虚拟机栈 = 8MB + 1MB = 9MB（当然JVM实现可以选择不使用这1MB的JVM虚拟机栈）。进程和操作系统的线程数限制。Linux中的线程被实现为轻量级进程，因此，还受到pid数量的限制。当无法在操作系统中继续创建线程时，抛出上述异常。 解决办法从原因中找： 内存资源：调小OS的线程栈、JVM的虚拟机栈。线程数：增大线程数限制。pid：增大pid范围。 java.lang.OutOfMemoryError:GC overhead limit exceeded默认配置下，如果GC花费了98%的时间，回收的内存都不足2%的话，抛出该异常。 java.lang.OutOfMemoryError:Out of swap space如果JVM申请的内存大于可用物理内存，操作系统会将内存中的数据交换到磁盘上去（交换区）。如果交换区空间不足，抛出该异常。]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql-sql优化]]></title>
    <url>%2F2020%2F10%2F09%2Fmysql-sql%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[问题:对于小的偏移量，直接使用limit来查询没有什么问题，但随着数据量的增大，越往后分页，limit语句的偏移量就会越大，速度也会明显变慢。优化思想:避免数据量大时扫描过多的记录 解决:子查询的分页方式或者JOIN分页方式。JOIN分页和子查询分页的效率基本在一个等级上，消耗的时间也基本一致。下面举个例子。一般MySQL的主键是自增的数字类型，这种情况下可以使用下面的方式进行优化。下面以真实的生产环境的80万条数据的一张表为例，比较一下优化前后的查询耗时: 123456789101112131415161718192021-- 传统limit，文件扫描[SQL]SELECT * FROM tableName ORDER BY id LIMIT 500000,2;受影响的行: 0时间: 5.371s-- 子查询方式，索引扫描[SQL]SELECT * FROM tableNameWHERE id &gt;= (SELECT id FROM tableName ORDER BY id LIMIT 500000 , 1)LIMIT 2;受影响的行: 0时间: 0.274s-- JOIN分页方式[SQL]SELECT *FROM tableName AS t1JOIN (SELECT id FROM tableName ORDER BY id desc LIMIT 500000, 1) AS t2WHERE t1.id &lt;= t2.id ORDER BY t1.id desc LIMIT 2;受影响的行: 0时间: 0.278s 复制代码可以看到经过优化性能提高了将近20倍。 优化原理:子查询是在索引上完成的，而普通的查询时在数据文件上完成的，通常来说，索引文件要比数据文件小得多，所以操作起来也会更有效率。因为要取出所有字段内容，第一种需要跨越大量数据块并取出，而第二种基本通过直接根据索引字段定位后，才取出相应内容，效率自然大大提升。因此，对limit的优化，不是直接使用limit，而是首先获取到offset的id，然后直接使用limit size来获取数据。在实际项目使用，可以利用类似策略模式的方式去处理分页，例如，每页100条数据，判断如果是100页以内，就使用最基本的分页方式，大于100，则使用子查询的分页方式。 对uuid处理因为插入值会随机写到索引的不同位置,使得insert更慢,这会导致页分裂,磁盘随机访问,以及对聚集索引产生碎片,逻辑上相邻的行被分布在磁盘和内存的不同位置随机值导致缓存对所有类型的查询语句效果很差,使得缓存依赖以工作的访问局部性原理失效, 如果存储uuid应该移除 ‘-‘ 符号,或者更好的做法是,用UNHEX()函数转换uuid为16字节的数字,并且存储在一个binary(16)列中, 物化视图物化视图是预先计算并存储在磁盘上的表,通过各种策略刷新和更新,mysql不支持(flexviews) alter tablemysql 执行大部分 alter table操作 使用新的结构创建按一个空表, 从旧表中查出所有的数据插入新表,然后删除旧表. 当内存不足而且表又很大时,而且有很多索引的情况下尤其如此,]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-mvc]]></title>
    <url>%2F2020%2F10%2F07%2Fspring-mvc%2F</url>
    <content type="text"><![CDATA[SpringMVC 原理Spring 的模型-视图-控制器（MVC）框架是围绕一个 DispatcherServlet 来设计的，这个 Servlet会把请求分发给各个处理器，并支持可配置的处理器映射、视图渲染、本地化、时区与主题渲染等，甚至还能支持文件上传。 Http 请求到 DispatcherServlet(1) 客户端请求提交到 DispatcherServlet。HandlerMapping 寻找处理器(2) 由 DispatcherServlet 控制器查询一个或多个 HandlerMapping，找到处理请求的Controller。调用处理器 Controller(3) DispatcherServlet 将请求提交到 Controller。Controller 调用业务逻辑处理后，返回 ModelAndView(4)(5)调用业务处理和返回结果：Controller 调用业务逻辑处理后，返回 ModelAndView。DispatcherServlet 查询 ModelAndView(6)(7)处理视图映射并返回模型： DispatcherServlet 查询一个或多个 ViewResoler 视图解析器，找到 ModelAndView 指定的视图。ModelAndView 反馈浏览器 HTTP(8) Http 响应：视图负责将结果显示到客户端。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-nio]]></title>
    <url>%2F2020%2F10%2F07%2Fjava-nio%2F</url>
    <content type="text"><![CDATA[NIO 的非阻塞IO 的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 NIO 的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞 IO 的空闲时间用于在其它通道上执行 IO 操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 SelectorSelector 能够检测多个注册的通道上是否有事件发生,如果事件发生,便获取事件然后针对每个事件进行相应的相应处理这样一个单线程就可以处理多个通道,也就是管理多个连接,]]></content>
  </entry>
  <entry>
    <title><![CDATA[jvm-内存回收]]></title>
    <url>%2F2020%2F10%2F06%2Fjvm-%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[对空间基本结构eden s0 s1 tentired新生代GC(MinorGC)老年代GC(MajorGC) 判断对象是否会被回收引用技术(循环引用)GC Root 引用类型 强引用 软引用 内存不足会被回收,如果软引用对象被回收虚拟机会把这个软引用加入到与之相关联的队列中 弱引用 虚引用]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-collection]]></title>
    <url>%2F2020%2F10%2F06%2Fjava-collection%2F</url>
    <content type="text"><![CDATA[集合RandomAccess 空接口数组是天然支持随机访问的, 时间复杂度为O(1) 所以称为快速随机访问,]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-exception]]></title>
    <url>%2F2020%2F10%2F06%2Fjava-exception%2F</url>
    <content type="text"><![CDATA[Throwable Error VirtualMachineeError AWTError Exception IOException RunntimeException 异常处理 try: 用于捕获异常catch: 处理try捕获到的异常finally: 当try与finally 都有return语句时, 在方法返回之前, finally语句将被执行 并且finally语句的返回值会覆盖原始返回值]]></content>
  </entry>
  <entry>
    <title><![CDATA[文件编码]]></title>
    <url>%2F2020%2F07%2F24%2F%E6%96%87%E4%BB%B6%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[android-xposed]]></title>
    <url>%2F2020%2F07%2F24%2Fandroid-xposed%2F</url>
    <content type="text"><![CDATA[[link]https://bbs.pediy.com/thread-257844.htm]]></content>
      <tags>
        <tag>xposed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android-magisk]]></title>
    <url>%2F2020%2F07%2F23%2Fandroid-magisk%2F</url>
    <content type="text"><![CDATA[magisk 获取root流程 解锁bootloader或者通过特殊方式绕过AVB校验。 解锁bootloader方法 进入开发者模式，选择允许OEM解锁 重启进入fastboot，执行fastboot flashing unlock ,解锁后在启动过程如果出现镜像校验失败也会继续启动，因此就可以刷入非官方镜像 从官方的update包中提取boot.img, 通过magisk工具对boot.img打补丁，在ramdisk中植入新的init程序，使用改init引导系统启动。 将boot.img刷入手机中，即完成root，可以通过magisk APK管理root授权]]></content>
      <tags>
        <tag>magisk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ida-调试]]></title>
    <url>%2F2020%2F07%2F22%2Fida-%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[### adb 调试步骤 12345678adb shellsucd /data/local/tmp./android_server# adb forward tcp:23946 tcp:23946]]></content>
  </entry>
  <entry>
    <title><![CDATA[android-adb]]></title>
    <url>%2F2020%2F07%2F09%2Fandroid-adb%2F</url>
    <content type="text"><![CDATA[adb install [-l] [-r] [-s] &lt;file&gt; EN push this package file to the device and install it CHS 给设备安装软件 (-l means forward-lock the app) #锁定该程序 (-r means reinstall the app, keeping its data) #重新安装该程序，保存数据 (-s means install on SD card instead of internal storage) #安装在SD卡内，而不是设备内部存储 adb uninstall [-k] &lt;package&gt; EN remove this app package from the device CHS 从设备删除程序包 (-k means keep the data and cache directories) #不删除程序运行所产生的数据和缓存目录(如软件的数据库文件) 清除adb logcat 缓存1adb logcat -c adb 输出指定包的日志 并重定向1adb -d logcat com.xx.mm:I *:* | Tee-Object login0713-03.log dx.batcd MyApplication\app\build\intermediates\javac\debug\classesbuild-tools\29.0.3\dx.bat –dex –output=./classes.dex .\com\example\myapplication\ExampleFunction.class ### 链接指定设备 adb connect [ip]adb disconnect [ip]adb -s shell 设备id[ip:port]]]></content>
      <tags>
        <tag>adb</tag>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python2.7 pip]]></title>
    <url>%2F2020%2F07%2F09%2Fpython2-7-pip%2F</url>
    <content type="text"><![CDATA[python2 pip install1234cd .\Scripts\.\easy_install.exe pipcd .. .\python.exe -m pip install six]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-gradle 编译问题记录]]></title>
    <url>%2F2020%2F07%2F05%2Fjava-gradle-%E7%BC%96%E8%AF%91%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[gradle tookchain =&gt; jdk14gradle config targetCompatibility=1.8gradle config sourceCompatibility=1.8 ByteBufferNoSuchMethodError: java.nio.ByteBuffer.limit(I)Ljava/nio/ByteBuffer link[https://github.com/eclipse/jetty.project/issues/3244] link[https://stackoverflow.com/questions/61267495/exception-in-thread-main-java-lang-nosuchmethoderror-java-nio-bytebuffer-flip] 构建工具 运行环境 配置jdk版本要一致啊。]]></content>
      <tags>
        <tag>java</tag>
        <tag>gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-交叉编译配置]]></title>
    <url>%2F2020%2F06%2F27%2Fgo-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[输入密码，查看文章 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]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android-Art虚拟机启动流程]]></title>
    <url>%2F2020%2F06%2F19%2Fandroid-Art%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[主要大致流程①Linux init进程解析配置脚本-&gt;②app_process（zygote进程对应的程序）-&gt;③ZygoteInit ① 解析配置脚本 service zygote:它告诉init进程,现在我们要配置一个名为zygote的服务. /system/bin/app_process: 声明zygote进程对应的文件路径.init创建服务的处理逻辑很简单,就是启动(fork)一个子进程来运行指定的程序.对zygote服务而言这个程序就是/system/bin/app_process. -Xzygote/system/bin--zygote--start-system-server:传递给app_process的启动参数. ②app_process 创建frameworks\base\cmds\app_process.cpp-&gt;main函数 frameworks\base\core\jni\AndroidRuntime.cpp-&gt;start函数 核心函数为: init,startVm三个函数主要功能: JNI_GetDefaultJavaVMInitArgs – 获取虚拟机的默认初始化参数 JNI_CreateJavaVM – 在进程中创建虚拟机实例 JNI_GetCreatedJavaVMs – 获取进程中创建的虚拟机实例 ART像Dalvik一样,都实现Java虚拟机接口,这三个接口也是ART虚拟机核心接口. startVm函数很复杂牵扯逻辑也很多,不逐一描述了. ③ZygoteInit 继续查看 frameworks\base\core\jni\AndroidRuntime.cpp-&gt;start函数 参数className的值等于”com.android.internal.os.ZygoteInit“,本地变量env是从调用另外一个成员函数startVm创建的ART虚拟机获得的JNI接口.函数的目标就是要找到一个名称为com.android.internal.os.ZygoteInit的类,以及它的静态成员函数main,然后就以这个函数为入口,开始运行ART虚拟机.为此,函数执行了以下步骤: ① 调用JNI接口FindClass加载com.android.internal.os.ZygoteInit类. ② 调用JNI接口 GetStaticMethodID 找到com.android.internal.os.ZygoteInit 类的静态成员函数main. ③ 调用JNI接口CallStaticVoidMethod开始执行com.android.internal.os.ZygoteInit类的静态成员函数main. 下面看看 Xposed 是如何做拦截的 打开 Xposed 项目 大于21编译走的是 app_main2.cpp 看看 具体改动了哪些经过查阅,被修改的 main 函数,一共有两个地方. 其一,红框的地方 是判断是否是 Xposed版本的虚拟机 在解析开启启动 init 脚本的时候 添加了 --xposedversion 版本号的命令 这块启动的已经是自定义的虚拟机了 handleOptions函数 第二个地方在 start 函数这块,先看看 原函数. 也是在这个地方 进行的初始化 判断是否初始化成功 . initialize 函数返回的是否加载成功的 一个全局变量 isXposedLoaded 初始化完毕以后开始调用真正的 start 函数下面看 runtimeStart 函数 这块很有趣 在 libart.so 里面根据符号表信息尝试拿到 Android::start 函数上面这些只要有一步失败了,在刷入的时候就可能变砖.如果获取到了,则可以直接通过函数指针调用,主要是针对一些特殊的安卓版本号. 如果都没有找到 可以看到 Log会打印 . “app_process: could not locate AndroidRuntime::start() method.” （这个地方有个小技巧,可以对so文件里面的全部函数名字进行逐一字符判断,比如可以对这个字符串 判断 是否含有 RuntimeStart 这几个字符,来绕过因为编译优化字符串不同问题） 这样一来完美替换了原虚拟机.在新的虚拟机里面会将 XposedBridge.jar 进行注入,这么一来,所有被 Xposed fork 的进程都具备了XposedBridge.jar 的代码 . 问题3:当我们 findAndHookMethod 一个函数以后 Xposed 是怎么处理的？ 打开 XposedBridge 项目 找到 findAndHookMethod link:https://bbs.pediy.com/thread-257844.htm]]></content>
      <tags>
        <tag>android</tag>
        <tag>xposed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android-设备标识]]></title>
    <url>%2F2020%2F06%2F18%2Fandroid-%E8%AE%BE%E5%A4%87%E6%A0%87%E8%AF%86%2F</url>
    <content type="text"><![CDATA[IMEI （手机的身份证号码）Android 10 获取不到IMEI(International Mobile Equipment Identity)是国际移动设备身份码的缩写,国际移动装备辨识码,是由15位数字组成的 电子串号 ,它与每台移动电话机一一对应,而且该码是全世界唯一的。每一只移动电话机在组装完成后都将被赋予一个全球唯一的一组号码,这个号码从生产到交付使用都将被制造生产的厂商所记录。 有些设备的IMEI有两个,可以在拨号键盘输入 *#06# 查看。普通APP获取需要申请权限(): 2.IMSI （SIM卡的身份证号码）IMSI 是区别移动用户的标志,储存在 SIM卡中,可用于区别移动用户的有效信息。其总长度不超过15位,同样使用0～9的数字,例如460010280100023。其中MCC是移动用户所属国家代号,占3位数字,中国的 MCC 规定为 460,MNC 是移动网号码,最多由两位数字组成,用于识别移动用户所归属的移动通信网,MSIN是移动用户识别码,用以识别某一移动通信网中的移动用户, 123456789101112131415161718192021222324android.os.Build.BRAND: 获取设备品牌android.os.Build.MODEL : 获取手机的型号 设备名称。android.os.Build.MANUFACTURER:获取设备制造商android.os.Build.BOARD: 获取设备基板名称android.os.Build.BOOTLOADER:获取设备引导程序版本号android.os.Build.CPU_ABI: 获取设备指令集名称（CPU的类型）android.os.Build.CPU_ABI2: 获取第二个指令集名称android.os.Build.DEVICE: 获取设备驱动名称android.os.Build.DISPLAY: 获取设备显示的版本包（在系统设置中显示为版本号）和ID一样android.os.Build.FINGERPRINT: 设备的唯一标识。由设备的多个信息拼接合成。android.os.Build.HARDWARE: 设备硬件名称,一般和基板名称一样（BOARD）android.os.Build.HOST: 设备主机地址android.os.Build.ID:设备版本号。android:os.Build.PRODUCT: 整个产品的名称android:os.Build.RADIO: 无线电固件版本号,通常是不可用的 显示unknownandroid.os.Build.TAGS: 设备标签。如release-keys 或测试的 test-keys android.os.Build.TIME: 时间android.os.Build.TYPE:设备版本类型 主要为&quot;user&quot; 或&quot;eng&quot;.android.os.Build.USER:设备用户名 基本上都为android-buildandroid.os.Build.VERSION.RELEASE: 获取系统版本字符串。如4.1.2 或2.2 或2.3等android.os.Build.VERSION.CODENAME: 设备当前的系统开发代号,一般使用REL代替android.os.Build.VERSION.INCREMENTAL: 系统源代码控制值,一个数字或者git hash值android.os.Build.VERSION.SDK: 系统的API级别 一般使用下面大的SDK_INT 来查看android.os.Build.VERSION.SDK_INT: 系统的API级别 数字表示]]></content>
      <tags>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS 的实现原理]]></title>
    <url>%2F2020%2F06%2F10%2FHTTPS-%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[HTTPS实现过程在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段. HTTPS 的整体过程分为证书验证和数据传输阶段，具体的交互过程如下： ① 证书验证阶段 浏览器发起 HTTPS 请求 服务端返回 HTTPS 证书 客户端验证证书是否合法，如果不合法则提示告警 ② 数据传输阶段 当证书验证合法后，在本地生成随机数 通过公钥加密随机数，并把加密后的随机数传输到服务端 服务端通过私钥对随机数进行解密 服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输 为什么需要 CA 认证机构颁发证书？HTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题. 首先我们假设不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的 “中间人攻击” 问题.“中间人攻击”的具体过程如下： 过程原理： 1,本地请求被劫持(如 DNS 劫持等)，所有请求均发送到中间人的服务器 2,中间人服务器返回中间人自己的证书 3,客户端创建随机数，通过中间人证书的公钥对随机数加密后传送给中间人，然后凭随机数构造对称加密对传输内容进行加密传输 4,中间人因为拥有客户端的随机数，可以通过对称加密算法进行内容解密 5,中间人以客户端的请求内容再向正规网站发起请求 6,因为中间人与服务器的通信过程是合法的，正规网站通过建立的安全通道返回加密后的数据 7,中间人凭借与正规网站建立的对称加密算法对内容进行解密 8,中间人通过与客户端建立的对称加密算法对正规内容返回的数据进行加密传输 9,客户端通过与中间人建立的对称加密算法对返回结果数据进行解密 由于缺少对证书的验证，所以客户端虽然发起的是 HTTPS 请求，但客户端完全不知道自己的网络已被拦截，传输内容被中间人全部窃取. 浏览器是如何确保 CA 证书的合法性？ 证书包含什么信息？ 颁发机构信息 公钥 公司信息 域名 有效期 指纹…… 证书的合法性依据是什么？首先，权威机构是要有认证的，不是随便一个机构都有资格颁发证书，不然也不叫做权威机构.另外，证书的可信性基于信任制，权威机构需要对其颁发的证书进行信用背书，只要是权威机构生成的证书，我们就认为是合法的.所以权威机构会对申请者的信息进行审核，不同等级的权威机构对审核的要求也不一样，于是证书也分为免费的、便宜的和贵的. 浏览器如何验证证书的合法性？浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证： 1, 验证域名、有效期等信息是否正确.证书上都有包含这些信息，比较容易完成验证； 2, 判断证书来源是否合法.每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证； 3, 判断证书是否被篡改.需要与 CA 服务器进行校验； 4, 判断证书是否已吊销.通过CRL(Certificate Revocation List 证书注销列表) 和 OCSP (Online Certificate Status Protocol 在线证书状态协议)实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率 1234这里插一个我想了很久的但其实答案很简单的问题： 既然证书是公开的，如果要发起中间人攻击，我在官网上下载一份证书作为我的服务器证书，那客户端肯定会认同这个证书是合法的，如何避免这种证书冒用的情况？其实这就是非加密对称中公私钥的用处，虽然中间人可以得到证书，但私钥是无法获取的，一份公钥是不可能推算出其对应的私钥，中间人即使拿到证书也无法伪装成合法服务端，因为无法对客户端传入的加密数据进行解密. 用了 HTTPS 会被抓包吗？HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看. 但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求.因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理.通常HTTPS抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环. 既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？HTTPS 可以防止用户在不知情的情况下通信链路被监听，对于主动授信的抓包操作是不提供防护的，因为这个场景用户是已经对风险知情.要防止被抓包，需要采用应用级的安全防护，例如采用私有的对称加密，同时做好移动端的防反编译加固，防止本地算法被破解.4 以下用简短的Q&amp;A形式进行全文总结： Q: HTTPS 为什么安全？A: 因为 HTTPS 保证了传输安全，防止传输过程被监听、防止数据被窃取，可以确认网站的真实性. Q: HTTPS 的传输过程是怎样的？A: 客户端发起 HTTPS请求，服务端返回证书，客户端对证书进行验证，验证通过后本地生成用于改造对称加密算法的随机数，通过证书中的公钥对随机数进行加密传输到服务端，服务端接收后通过私钥解密得到随机数，之后的数据交互通过对称加密算法进行加解密. Q: 为什么需要证书？A: 防止”中间人”攻击，同时可以为网站提供身份证明. Q: 使用 HTTPS 会被抓包吗？A: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建”中间人”网络，代理软件可以对传输内容进行解密. 分享一张详细的过程图： link :https://mp.weixin.qq.com/s/IoEHv13YyWEhpZJYAA0juw]]></content>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 环境变量作用及其加载顺序]]></title>
    <url>%2F2020%2F06%2F08%2FLinux-%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%8F%8A%E5%85%B6%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[最近在在 Linux 服务器上总是遇到已经配置了 JAVA 环境变量, 却还是提示 command not found, 于是特意查了下 Linux 配置环境变量的几个文件的加载顺序. 1/etc/profile -&gt; (~/.bash_profile | ~/.bash_login | ~/.profile) -&gt; ~/.bashrc -&gt; /etc/bashrc -&gt; ~/.bash_logout 其中, 几个文件的作用如下： 系统级：1./etc/environment:是系统在登录时读取的第一个文件, 该文件设置的是整个系统的环境, 只要启动系统就会读取该文件, 用于为所有进程设置环境变量.系统使用此文件时并不是执行此文件中的命令, 而是根据而是根据KEY=VALUE模式的代码, 对KEY赋值以VALUE, 因此文件中如果要定义PATH环境变量, 只需加入一行形如 PATH=$PATH:/xxx/bin的代码即可. 2./etc/profile:此文件是系统登录时执行的第二个文件. 为系统的每个用户设置环境信息, 当用户第一次登录时, 该文件被执行.并从 /etc/profile.d 目录的配置文件中搜集 shell 的设置.( /etc/profile可以用于设定针对全系统所有用户的环境变量, 环境变量周期是永久性). 3./etc/bashrc:是针对所有用户的bash初始化文件, 在此中设定的环境变量将应用于所有用户的 shell 中, 此文件会在用户每次打开 shell时执行一次.(即每次新开一个终端, 都会执行 /etc/bashrc). 用户级(这些文件处于家目录下)：1.~/.profile:对应当前登录用户的 profile 文件, 用于定制当前用户的个人工作环境(变量是永久性), 每个用户都可使用该文件输入专用于自己使用的 shell 信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的 .bashrc文件.这里是推荐放置个人设置的地方. ~/.bashrc:该文件包含专用于你的bash shell的 bash信息, 当登录时以及每次打开新的shell时, 该文件被读取.(~/.bashrc只针对当前用户, 变量的生命周期是永久的).不推荐放到这儿, 因为每开一个shell, 这个文件会读取一次, 效率肯定有影响. ~/.bash_profile or ~./bash_login:~/.bash_profile是交互式login 方式进入 bash 运行的, ~/.bashrc 是交互式 non-login 方式进入 bash 运行的通常二者设置大致相同, 所以通常前者会调用后者.每个用户都可使用该文件输入专用于自己使用的shell信息, 当用户登录时, 该文件仅仅执行一次.默认情况下, 他设置一些环境变量, 执行用户的 .bashrc 文件.(如果~/目录下没有 .bash_profile则新建立一个)这里是推荐放置个人设置的地方.当一个shell关闭时, 在 bash_profile 中定义的系统变量则会失效.因此, 每打开一个新的shell时都要运行一次 source bash_profile. 而且针对当前用户. 4.~/.pam_environment:用户级的环境变量设置文件, 没有做测试, 不知道管不管用. 5.~/.bash_logout:当每次退出系统(退出bash shell)时, 执行该文件.另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承 /etc/profile中的变量,他们是”父子”关系]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Diffie-Hellman 算法]]></title>
    <url>%2F2020%2F06%2F05%2FDiffie-Hellman-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[D-H算法的数学基础是离散对数的数学难题，其加密过程如下： （1）客户端与服务端确定两个大素数 n和 g，这两个数不用保密（2）客户端选择另一个大随机数x，并计算A如下：A = g^x mod n（3）客户端将 A 发给服务端（4）服务端选择另一个大随机数y，并计算B如下：B = g^y mod n（5）服务端将B发给客户端（7）计算秘密密钥K1如下：K1=B^2 mod n ， 计算秘密密钥K2如下：K2=A^y mod n , K1=K2，因此服务端和客户端可以用其进行加解密 How QUIC works? 一、基于TCP+TLS的HTTP2建连 出于HTTP的明文和无法验证服务器的真实性，在TCP的基础上引入了TLS协议，目前广泛使用的HTTPS是基于TCP+TLS协议，HTTP2也被主流浏览器默认支持TLS。 但对于建立连接的耗时而言，TCP本身就需要握手时延，而TLS协议为了使得客户端和服务器端在不安全的网络通信中协商出后续安全通信所需的加密私钥，更是要经过额外2次RTT(RoundTrip Time往返时间) 图3 TCP+TLS建连过程 除了TCP建立连接过程，TLS握手过程要经过如下步骤： 1、客户端提供加密套件（算法）列表，版本等信息 2、服务器端提供自己的证书，选择的加密套件，非对称加密公钥（自己保留私钥）等 3、客户端提供自己的证书，用服务器公钥和加密套件加密的自己的私钥 4、服务端用保留的私钥解密客户端传来的加密私钥，得到的私钥即为后续加密传输使用的对称密钥，最后完成握手 此时，双方协商出了对称密钥。基于TCP+TLS的HTTP2建连过程结束，大约需要耗时200-300ms。 二、 QUIC建连 为了保证安全，QUIC也是加密传输数据的，所以在QUIC的建连过程中也需要双方协商出一个加密私钥。但与TLS不同，QUIC采用的加密算法仅需要一个RTT就能实现密钥交换，并且该算法也被用于目前正在草案阶段的TLS1.3协议。该就是Diffie-Hellman密钥交换算法。 图4 Diffie-Hellman算法 可以看到，客户端和服务端各自保留了自己的私钥a和b，通过交换各自的公钥B和A，以及基底G和很大的质数P，双方就能计算出相等的私钥S，这个S就是加密传输的对称密钥。 另外，根据离散对数的不可逆，即使拿到G,P,和质数B，也很难推导出私钥b（同理私钥a），也就保证了计算密钥的安全。 该过程对应到QUIC建连的过程中如下图。 图5 1RTT建连 1、客户端发起Inchoate client hello 2、服务器返回Rejection，包括密钥交换算法的公钥信息，算法信息，证书信息等被放到server config中传给客户端 3、客户端发起client hello，包括客户端公钥信息 此时，双方各自计算出了对称密钥。QUIC的1RTT建连过程结束，平均只耗时100ms以内。 后续发起连接的过程中，一旦客户端缓存或持久化了server config，就可以复用并结合本地生成的私钥进行加密数据传输了，不需要再次握手，从而实现0RTT建立连接。]]></content>
      <tags>
        <tag>加解密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux_variables]]></title>
    <url>%2F2020%2F06%2F02%2Flinux-variables%2F</url>
    <content type="text"><![CDATA[How do I add environment variables? To set variable only for current shell: VARNAME=&quot;my value&quot; To set it for current shell and all processes started from current shell: export VARNAME=&quot;my value&quot;shorter, less portable version To set it permanently for all future bash sessions add such line to your .bashrc file in your $HOME directory. To set it permanently, and system wide (all users, all processes) add set variable in /etc/environment: sudo -H gedit /etc/environment This file only accepts variable assignments like: VARNAME=&quot;my value&quot; Do not use the export keyword here. You need to logout from current user and login again so environment variables changes take place.]]></content>
  </entry>
  <entry>
    <title><![CDATA[gradle-task]]></title>
    <url>%2F2020%2F02%2F13%2Fgradle-task%2F</url>
    <content type="text"><![CDATA[概述Gradle 中的每一个 Project 都是由一个或者多个 Task 来构成的, 它是 gradle 构建脚本的最小运行单元, 一个 Task 代表一些更加细化的构建, 可能是编译一些 classes 、创建一个 Jar、生成 javadoc、或者生成某个目录的压缩文件.Task 有一些重要的功能: 任务动作(task action)和任务依赖(task dependency). task action定义了任务执行时最小的工作单元, 比如 doFirst 和doLast. task dependency定义了 task 之间的依赖关系, 例如在某个 task 运行之前要运行另外一个 task, 尤其是需要另一个 task 的输出作为输入的时候. Task 相关命令./gradlew tasks: 列出当前工程的所有 Task./gradlew [-q] &lt;task name&gt;: 单独执行某个 task, -q 代表 quite 模式, 它不会生成 Gradle 的日志信息 (log messages), 所以用户只能看到 tasks 的输出.创建任务默认情况下, 我们创建的每一个 Task 都是 org.gradle.api.DefaultTask 类型, 这是一个通用的 Task 类型. 另外 Gradle 还提供了具有一些特定功能的 Task, 比如 Copy 和 Delete 等, 我们需要时直接继承即可. 另外, 我们还可以创建自己的 Task 类型, 并且可以自行定义我们创建的 Task 类型.Task 的定义和构造方式也是多种多样的, Gradle 提供了多种方法来定义 Task. 另外 Task 既可以在 build.gradle 文件中直接创建, 也可以由不同的 Plugin 引入. Task 构造方法可以通过下面几个方法来构造 Task: task myTask: 用 task 关键字构造 task(): 用 project 的 task 方法构造 tasks.create: 用 TaskContainer 的 create 方法构造 myTask extends DefaultTask Task 示例代码示例123456789101112131415161718192021222324252627task hello &#123; doFirst &#123; println 'task hello doFirst' &#125; doLast &#123; println 'task hello doLast' &#125; println 'config task hello'&#125;task hello2 (type: Copy)&#123; from 'src/main/AndroidManifest.xml' into 'build/test'&#125;task hello3(group: "myTest", description:"This is test task paras", dependsOn: ["A", "B"])&#123; println 'task hello3'&#125;task("hello4", group: "myTest", description:"This is test task para name")&#123; println 'task hello4'&#125;tasks.create("hello5") &#123; group = 'Welcome' description = 'Produces a greeting' doLast &#123; println 'Hello, World' &#125;&#125; 执行 ./gradlew tasks --all, 上面的 Task 就会在列表中显示出来.123456789101112MyTest tasks------------hello3 - This is test task parashello4 - This is test task para nameOther tasks-----------...hellohello2Welcome tasks-------------hello5 - Produces a greeting 我们执行 ./gradlew -q hello, 会有下面的输出: 123config task hellotask hello doFirsttask hello doLast 实际上大部分时候 task 都应该是在执行状态下才真正执行的, 配置状态大部分时候用于声明执行时需要用到的变量等为执行服务的前置动作.hello2: Task 创建的时候可以通过 type: SomeType 指定Type, Type其实就是告诉Gradle, 这个新建的Task对象会从哪个基类Task派生. 比如, Gradle本身提供了一些通用的 Task, 最常见的有Copy、Delete、 Sync 、 Tar 等任务. Copy是Gradle中的一个类. 当我们: task myTask(type:Copy)的时候, 创建的Task就是一个Copy Task. 下面会详细介绍. Gradle 本身提供了很多已经定义好的这些 task 来供我们使用, 我们使用时直接继承就行了, 这个后面会有介绍. 当然我们也可以自定义一些这样的 Task 来供其他 Task 来继承. 下面也会详细介绍. Task 的一些构造方法Task 提供了下面几个构造方法: task myTask task myTask { configure closure } task myTask(type: SomeType) task myTask(type: SomeType) { configure closure } Project 提供了下面几个 task 方法来创建 task: task(name) task(name, configureClosure) task(name, configureAction) task(args, name) task(args, name, configureClosure) TaskContainer 提供了下面几个方法来创建 task, TaskContainer 可以通过 Project 的 tasks 属性或者 getTasks() 方法获取: create​(String name) create​(String name, Closure configureClosure) create​(String name, Class type) create​(String name, Class type, Object… constructorArgs) create​(String name, Class type, Action&lt;? super T&gt; configuration) create​(Map options) create​(Map options, Closure configureClosure) TaskContainer create 方法定义 Task来介绍一下 create 方法的用法:先来看一段代码: 12345tasks.create("hello") &#123; doLast &#123; println 'Hello, World!' &#125;&#125; 上面代码做了两件事情: 创建了一个叫 hello 的 task在 doLast action 中打印 Hello, World! 到终端运行 ./gradlew tasks --all 命令后, 会在 Other tasks 的组中发现 hello task.运行 ./gradlew hello 输出: Task :helloHello, World!下面我们为 task 添加组信息和task描述信息: 1234567tasks.create("hello") &#123; group = 'Welcome' description = 'Produces a greeting' doLast &#123; println 'Hello, World' &#125;&#125; 为该 task 生成组和描述信息: 123Welcome tasks-------------hello - Produces a greeting 实现 TaskAction, 定义公共 Task上面介绍的集中定义 task 的方法, 实际真正执行的时候什么都没有做, 或者是在 doFirst 和 doLast action 时做了一些事情.我们还可以通过继承 Task 的方式通过 @TaskAction 操作符也可以指定一个 Task 执行时要做的事情, 它区别于 doFirst 和 doLast. // 这个类可以放到当前 build.gradle 文件中, 也可以放到单独的 gradle 文件中, 也可以放到插件的 java 或者 groovy 文件中. 1234567891011121314151617class TestTask extends DefaultTask &#123; String source @TaskAction void testAction() &#123; println getName()+" ### testAction!" &#125; void testMothod(String para) &#123; println "Source is "+source+ " para = "+para &#125;&#125;task testTask (type: TestTask) &#123; source "MyApplication" testMothod "test" doLast &#123; println 'GoodBye' &#125;&#125; 代码中定义了一个名为 TestTask 的 Task, 它继承自 DefaultTask, 通过注解 TaskAction 标记了默认的 Action. @TaskAction表示该Task要执行的动作, 即在调用该Task时, 要执行的方法.另外演示了属性和方法的应用.我们可以定制一些这样的 Task 放到 plugin 中, 作为公共的 Task 供开发者调用和继承. 在项目中存在大量自定义的 Task 类型时, 我们是推荐使用这种方法的. 下面来介绍一下在 plugin 的 java 文件中定义 task:创建自定义 task: 123456789package com.android.hq.testplugin;import org.gradle.api.DefaultTask;import org.gradle.api.tasks.TaskAction;public class TestJavaTask extends DefaultTask &#123; @TaskAction public void testAction()&#123; System.out.println("### This is TestJavaTask"); &#125;&#125; 定义 task: 12345678910111213141516171819// 引用插件apply plugin: 'com.android.hq.testplugin.test'// 添加插件的依赖路径buildscript &#123; dependencies &#123; // 这里直接使用了上面插件工程中生成 jar 包的路径 classpath files("../../TestGradlePlugin/app/build/libs/app-1.0.0.jar") // NOTE: Do not place your application dependencies here; they belong // in the individual module build.gradle files &#125;&#125;task testCustom(type: com.android.hq.testplugin.TestJavaTask) &#123; doFirst &#123; println "Hello" &#125; doLast &#123; println "GoodBye" &#125;&#125; 执行 ./gradlew testCustom 后生成信息: Task :app:testCustomHello This is TestJavaTaskGoodBye 配置 Task 属性下面我们再来介绍如何进行灵活地进行定制 task 的一些属性, 比如下面例子中的要输出的信息.这里我们通过实现一个 Task 类来实现创建 task.在 build.gradle 中添加代码: 1234567891011121314151617181920class Greeting extends DefaultTask &#123; String message String recipient @TaskAction void sayGreeting() &#123; println "$&#123;message&#125;, $&#123;recipient&#125;!" &#125;&#125;tasks.create("hello", Greeting) &#123; group = 'Welcome' description = 'Produces a world greeting' message = 'Hello' recipient = 'World'&#125;tasks.create("gutenTag", Greeting) &#123; group = 'Welcome' description = 'Produces a German greeting' message = 'Guten Tag' recipient = 'Welt'&#125; 运行 ./gradlew tasks 命令: 1234Welcome tasks-------------gutenTag - Produces a German greetinghello - Produces a world greeting 下面几点说明:虽然 Gradle API 的其他 Task 类我们也可以使用, 但继承 DefaultTask 是常见的扩展 Task 的方法.上面例子中添加了自定义的 message 和 recipient 实现任务类型的可配置, 分别实现了不同输出的 hello 和 gutenTag 两个任务.通过注解 TaskAction 标记了默认的 Action.@TaskAction表示该 Task 要执行的动作, 即在调用该Task时, 要执行的方法.通过引用 Greeting 实现了不同的任务类.Task 的状态这里会有个奇怪的现象, 我们在执行 ./gradlew -q hello 时有上面三条输出是容易理解的, 但是在执行 ./gradlew tasks 或者是执行其他 Task 比如 ./gradlew -q hello1 时, config task hello 这个打印也会输出, 为什么呢？这是因为 Task 有两种状态, 分别是: 配置状态（Configuration State）执行状态（Execution State）这其实对应了 Gradle 三个生命周期中的配置阶段和执行阶段, task 的配置块永远在 task 动作执行之前被执行.Gradle 会在进入执行之前, 配置所有 Task, 而 println ‘config task hello’ 这段代码就是在配置时进行执行. 所以哪怕没有显式调用 gradlew hello, 只是调用列出所有 task 的命令, hello task 仍然需要进入到配置状态, 也就仍然执行了一遍.很多时候我们并不需要配置代码, 我们想要我们的代码在执行 task 的时候才执行, 这个时候可以通过 doFirst、doLast或者实现 TaskAction 来完成. Task 参数我们用命令查看 Task 信息时一般是这样的: 1234567Build tasks-----------assemble - Assembles all variants of all applications and secondary packages.assembleAndroidTest - Assembles all the Test applications.assembleDdd - Assembles all Ddd builds.assembleDebug - Assembles all Debug builds.assembleRelease - Assembles all Release builds. 一般是由 group、name 和 description组成, 其实在上面的示例中大家应该看到 hello3、hello4和其他任务的不同了. 他们两个处在同一个 group 中并且 Task 名称后面有一些描述信息.Task的一般的属性有下面几种, 可以在创建 Task 的时候在闭包中声明: Property Propertyname task的名字type task的“父类”overwrite 是否替换已经存在的taskdependsOn task依赖的task的集合group task属于哪个组description task的描述 动态任务123454.times &#123; counter -&gt; task &quot;task$counter&quot; &lt;&lt; &#123; println &quot;I&apos;m task number $counter&quot; &#125;&#125; 这里动态的创建了 task0, task1, task2, task3.执行 ./gradlew -q task1, 输出: 1I&apos;m task number 1 调用任务短标记法可以使用短标记 $ 可以访问一个存在的任务: 123hello.doLast &#123; println "Greetings from the $hello.name task."&#125; 执行 gradle -q hello 输出: 12Hello world!Greetings from the hello task. 任务流程 任务依赖创建依赖在Gradle中, Task之间是可以存在依赖关系的. 这种关系可以通过 dependsOn 来实现.123456789101112131415task A &lt;&lt; &#123; println &apos;Hello from A&apos;&#125;task B &lt;&lt; &#123; println &apos;Hello from B&apos;&#125;B.dependsOn A或者task A &lt;&lt; &#123; println &apos;Hello from A&apos;&#125;task B (dependsOn: A) &lt;&lt; &#123; println &apos;Hello from B&apos;&#125; 执行 ./gradlew -q B 的同时会先去执行任务 A: Hello from AHello from B下面的代码同样能创建有依赖关系的任务: task A &lt;&lt; { println ‘Hello from A’}task B { dependsOn A doLast { println ‘Hello from B’ }}插入依赖也可以在已经存在的 task 依赖中插入我们的 task .比如前面的A和B已经存在了依赖关系, 我们想在中间插入任务B1, 可以通过下面的代码实现: B1.dependsOn AB.dependsOn B1输出结果: Hello from AHello from B1Hello from B任务排序mustRunAfter 与 shouldRunAfter在某些情况下, 我们希望能控制任务的的执行顺序, 这种控制并不是向上面那样去显示地加入依赖关系. 我们可以通过 mustRunAfter 和 shouldRunAfter 来实现.使用 mustRunAfter 意味着 taskB 必须总是在 taskA 之后运行, shouldRunAfter 和 mustRunAfter 很像, 只是没有这么严格.1234567891011task A () &#123; doLast&#123; println &apos;Hello from A&apos; &#125;&#125;task B() &#123; doLast&#123; println &apos;Hello from B&apos; &#125;&#125;A.mustRunAfter B 执行 ./gradlew -q A B, 结果:12Hello from BHello from A 如果换成 shouldRunAfter, 结果也是一样的.虽然我们将两个任务进行了排序, 但是他们仍然是可以单独执行的, 任务排序不影响任务执行. 排序规则只有当两个任务同时执行时才会被应用.比如执行 ./gradlew -q A 会输出: 1Hello from A 另外, shouldRunAfter 不影响任务之间的执行依赖. 但如果 mustRunAfter 和任务依赖之间发生了冲突, 那么执行时将会报错. finalizedBy假如出现了这样一种使用场景, 执行完任务 A 之后必须要执行一下任务 B, 那么上面的方法是无法解决这个问题的, 这时 finalizedBy 就派上用场了. task A &lt;&lt; { println ‘Hello from A’}task B &lt;&lt; { println ‘Hello from B’}A.finalizedBy B执行 ./gradlew -q A, 结果: Hello from AHello from BTask Type除了前面我们介绍的 task 定义方式以外, Gradle 本身还提供了一些已有的 task 供我们使用, 比如 Copy、Delete、Sync 等. 因此我们定义 task 的时候是可以继承已有的 task, 比如我们可以继承自系统的 Copy Task 来完成文件的拷贝操作. CopyCopy的API文档 task hello2 (type: Copy){ from ‘src/main/AndroidManifest.xml’ // 调用 from 方法 into ‘build/test’ // 调用 into 方法 // 调用 rename 方法 rename {String fileName -&gt; fileName = “AndroidManifestCopy.xml” }}Copy Task的其他属性和方法参考源码或者文档. 其他 Type 具体详见文档, 此处不详细解释. ExecExec Task用来执行命令行. 任务的执行条件使用判断条件可以使用 onlyIf() 方法来为一个任务加入判断条件. 就和 Java 里的 if 语句一样, 任务只有在条件判断为真时才会执行. 可以通过一个闭包来实现判断条件. task A &lt;&lt; { println ‘Hello from A’}A.onlyIf{!project.hasProperty(‘skipA’)}执行 “./gradlew A -PskipA”, 输出: :app:A SKIPPED可以看到 A 任务被跳过. 使用 StopExecutionException如果想要跳过一个任务的逻辑并不能被判断条件通过表达式表达出来, 那么可以使用 StopExecutionException. 如果这个异常是被一个任务要执行的动作抛出的, 这个动作之后的执行以及所有紧跟它的动作都会被跳过. 构建将会继续执行下一个任务. task hello { doFirst { println ‘task hello doFirst’ throw new StopExecutionException() } doLast { println ‘task hello doLast’ }}如果你直接使用 Gradle 提供的任务, 这项功能还是十分有用的. 它允许你为内建的任务加入条件来控制执行. 激活和注销任务每一个任务都有一个已经激活的标记(enabled flag), 这个标记一般默认为真. 将它设置为假, 那它的任何动作都不会被执行. task A &lt;&lt; { println ‘Hello from A’}A.enabled = false执行 “./gradlew A”, 输出: :app:A SKIPPED声明任务的输入和输出我们在执行 Gradle 任务的时候, 你可能会注意到 Gradle 会跳过一些任务, 这些任务后面会标注 up-to-date. 代表这个任务已经运行过了或者说是最新的状态, 不再需要产生一次相同的输出.Gradle 通过比较两次 build 之间输入和输出有没有变化来确定这个任务是否是最新的, 如果从上一个执行之后这个任务的输入和输出没有发生改变这个任务就标记为 up-to-date, 跳过这个任务.因此, 要想跳过 up-to-date 的任务, 我们必须为任务指定输入和输出.任务的输入属性是 TaskInputs 类型. 任务的输出属性是 TaskOutputs 类型.下面的例子中把上面的 Copy 示例中的输入和输出文件作为 hello task 的输入和输出文件: task hello { inputs.file (“src/main/AndroidManifest.xml”) outputs.file (“build/test/AndroidManifestCopy.xml”) doFirst { println ‘task hello doFirst’ }}第一次执行 ./gradlew hello输出: :app:hellotask hello doFirst可以看到任务正常的执行.然后进行第二次执行, 输出: :app:hello UP-TO-DATE跳过这个任务. 可以看到, Gradle 能够检测出任务是否是 up-to-date 状态.如果我们修改一下 src/main/AndroidManifest.xml 文件, 输入上面命令就会再次执行该任务. UP-TO-DATE 原理当一个任务是首次执行时, Gradle 会取一个输入的快照 (snapshot). 该快照包含组输入文件和每个文件的内容的散列. 然后当 Gradle 执行任务时, 如果任务成功完成, Gradle 会获得一个输出的快照. 该快照包含输出文件和每个文件的内容的散列. Gradle 会保留这两个快照用来在该任务的下一次执行时进行判断.之后, 每次在任务执行之前, Gradle 都会为输入和输出取一个新的快照, 如果这个快照和之前的快照一样, Gradle 就会假定这个任务已经是最新的 (up-to-date) 并且跳过任务, 反之亦然.需要注意的是, 如果一个任务有指定的输出目录, 自从该任务上次执行以来被加入到该目录的任务文件都会被忽略, 并且不会引起任务过时 (out of date). 这是因为不相关任务也许会共用同一个输出目录. 如果这并不是你所想要的情况, 可以考虑使用 TaskOutputs.upToDateWhen(). 参考文档http://wiki.jikexueyuan.com/project/GradleUserGuide-Wiki/https://docs.gradle.org/current/userguide/userguide.htmlhttp://www.jianshu.com/p/cd1a78dc8346]]></content>
  </entry>
  <entry>
    <title><![CDATA[Linux 磁盘占用]]></title>
    <url>%2F2020%2F01%2F16%2FLinux-%E7%A3%81%E7%9B%98%E5%8D%A0%E7%94%A8%2F</url>
    <content type="text"><![CDATA[查看文件夹文件大小1du -h --max-depth=1 /var/log 或者 12cd /du -sh * 当前目录的磁盘占用 1du -sh ./* 处理文件删除问题应该是删除了这些文件，但是空间没有释放，当然重启可以解决目的，但是会造成服务器上所有业务中断，可使用下面命令查看删除文件占用情况： 1[root@ziggle]# lsof |grep delete 磁盘扩容 @see https://help.aliyun.com/knowledge_detail/111738.html 运行fdisk -l命令查看现有云盘大小。以下示例返回云盘（/dev/vda）容量是100GiB。 1fdisk -l 运行df -h命令查看云盘分区大小。以下示例返回分区（/dev/vda1）容量是20GiB。 1df -h 运行growpart &lt;DeviceName&gt; &lt;PartionNumber&gt;命令调用growpart为需要扩容的云盘和对应的第几个分区扩容。示例命令表示为系统盘的第一个分区扩容。 若运行命令后报以下错误，您可以运行LANG=en_US.UTF-8切换ECS实例的字符编码类型。 1234growpart /dev/vda 1---# LANG=en_US.UTF-8 运行resize2fs 命令调用resize2fs扩容文件系统。示例命令表示为扩容系统盘的/dev/vda1分区文件系统。 1resize2fs /dev/vda1 1234df -h #整个磁盘空间du -h --max-depth=1 #看当前目录各子目录占用空间du -h -d 1 #同上，适用于mac系统]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlserver 配置]]></title>
    <url>%2F2020%2F01%2F10%2Fsqlserver-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1234use mastergo-- 读取配置 EXEC sp_configure @configname='remote query timeout'; 12name minimum maximum config_value run_valueremote query timeout (s) 0 2147483647 600 600 1234-- 配置EXEC sp_configure 'remote query timeout', 300 ; GO RECONFIGURE ; https://docs.microsoft.com/zh-cn/sql/relational-databases/system-stored-procedures/sp-configure-transact-sql?view=sql-server-ver15]]></content>
  </entry>
  <entry>
    <title><![CDATA[sql-join]]></title>
    <url>%2F2019%2F12%2F01%2Fsql-join%2F</url>
    <content type="text"><![CDATA[ref &gt;&gt; https://blog.csdn.net/yu849893679/article/details/86487628对于SQL的Join，在学习起来可能是比较乱的。我们知道，SQL的Join语法有很多inner的，有outer的，有left的，有时候，对于Select出来的结果集是什么样子有点不是很清楚。Coding Horror上有一篇文章,通过韦恩图(Venn diagram,可用来表示多个集合之间的逻辑关系)。解释了SQL的Join。我觉得清楚易懂，转过来。 假设我们有两张表。Table A 是左边的表。Table B 是右边的表。其各有四条记录，其中有两条记录name是相同的，如下所示：让我们看看不同JOIN的不同 A表id name1 Pirate2 Monkey3 Ninja4 SpaghettiB表id name1 Rutabaga2 Pirate3 Darth Vade4 Ninja1.INNER JOIN SELECT * FROM TableA INNER JOIN TableB ON TableA.name = TableB.name 结果集(TableA.) (TableB.)id name id name1 Pirate 2 Pirate3 Ninja 4 Ninja Inner join 产生的结果集中，是A和B的交集。 2.FULL [OUTER] JOIN (1) SELECT * FROM TableA FULL OUTER JOIN TableB ON TableA.name = TableB.name 结果集(TableA.) (TableB.)id name id name1 Pirate 2 Pirate2 Monkey null null3 Ninja 4 Ninja4 Spaghetti null nullnull null 1 Rutabaganull null 3 Darth VadeFull outer join 产生A和B的并集。但是需要注意的是，对于没有匹配的记录，则会以null做为值。 可以使用IFNULL判断。 (2) SELECT * FROM TableA FULL OUTER JOIN TableB ON TableA.name = TableB.nameWHERE TableA.id IS null OR TableB.id IS null 结果集(TableA.) (TableB.)id name id name2 Monkey null null4 Spaghetti null nullnull null 1 Rutabaganull null 3 Darth Vade产生A表和B表没有交集的数据集。 3.LEFT [OUTER] JOIN (1) SELECT * FROM TableA LEFT OUTER JOIN TableB ON TableA.name = TableB.name 结果集(TableA.) (TableB.)id name id name1 Pirate 2 Pirate2 Monkey null null3 Ninja 4 Ninja4 Spaghetti null nullLeft outer join 产生表A的完全集，而B表中匹配的则有值，没有匹配的则以null值取代。 (2) SELECT * FROM TableA LEFT OUTER JOIN TableB ON TableA.name = TableB.name WHERE TableB.id IS null 结果集(TableA.) (TableB.)id name id name2 Monkey null null4 Spaghetti null null产生在A表中有而在B表中没有的集合。 4.RIGHT [OUTER] JOIN RIGHT OUTER JOIN 是后面的表为基础，与LEFT OUTER JOIN用法类似。这里不介绍了。 5.UNION 与 UNION ALL UNION 操作符用于合并两个或多个 SELECT 语句的结果集。请注意，UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条 SELECT 语句中的列的顺序必须相同。UNION 只选取记录，而UNION ALL会列出所有记录。 (1)SELECT name FROM TableA UNION SELECT name FROM TableB 新结果集namePirateMonkeyNinjaSpaghettiRutabagaDarth Vade选取不同值 (2)SELECT name FROM TableA UNION ALL SELECT name FROM TableB 新结果集namePirateMonkeyNinjaSpaghettiRutabagaPirateDarth VadeNinja全部列出来 (3)注意: SELECT FROM TableA UNION SELECT FROM TableB 新结果集id name1 Pirate2 Monkey3 Ninja4 Spaghetti1 Rutabaga2 Pirate3 Darth Vade4 Ninja由于 id 1 Pirate 与 id 2 Pirate 并不相同，不合并 还需要注册的是我们还有一个是“交差集” cross join, 这种Join没有办法用文式图表示，因为其就是把表A和表B的数据进行一个NM的组合，即笛卡尔积。表达式如下：SELECT FROM TableA CROSS JOIN TableB 这个笛卡尔乘积会产生 4 x 4 = 16 条记录，一般来说，我们很少用到这个语法。但是我们得小心，如果不是使用嵌套的select语句，一般系统都会产生笛卡尔乘积然再做过滤。这是对于性能来说是非常危险的，尤其是表很大的时候。————————————————版权声明：本文为CSDN博主「蓝海丶丶」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/yu849893679/article/details/86487628]]></content>
  </entry>
  <entry>
    <title><![CDATA[spring-security]]></title>
    <url>%2F2019%2F11%2F26%2Fspring-security%2F</url>
    <content type="text"><![CDATA[WebSecurityConfigurerAdapterorg.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter 1234567891011121314151617181920212223242526 * spring security filter chain * * (1)// UsernamePasswordAuthenticationFilter * * @link &#123;org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter&#125; * 登陆请求 检查参数 * (2) // BasicAuthenticationFilter * @link &#123;org.springframework.security.web.authentication.www.BasicAuthenticationFilter&#125; * 检查请求头Basic * ... .... * (3) org.springframework.security.web.access.ExceptionTranslationFilter * 异常转换 (必定会有 位置在FilterSecurityInterceptor) * * (4) @link &#123;org.springframework.security.web.access.intercept.FilterSecurityInterceptor&#125; * // 最终决定是否通过 */ @Overrideprotected void configure(HttpSecurity http) throws Exception &#123; // http.formLogin() http.httpBasic() .and() .authorizeRequests() .anyRequest() .authenticated();&#125; 要想实现短信验证码登录流程，我们可以借鉴已有的用户名密码登录流程，分析有哪些组件是需要我们自己来实现的： 首先我们需要一个SmsAuthenticationFilter拦截短信登录请求进行认证，期间它会将登录信息封装成一个Authentication请求AuthenticationManager 进行认证AuthenticationManager会遍历所有的AuthenticationProvider找到其中支持认证该Authentication 并调用authenticate进行实际的认证， 因此我们需要实现自己的Authentication(SmsAuthenticationToken)和认证该Authentication的AuthenticationProvider（SmsAuthenticationProvider），并将SmsAuthenticationProvider添加到SpringSecurty的AuthenticationProvider集合中，以使AuthenticationManager 遍历该集合时能找到我们自定义的SmsAuthenticationProvider``SmsAuthenticationProvider在进行认证时，需要调用UserDetailsService根据手机号查询存储的用户信息loadUserByUsername， 因此我们还需要自定义的SmsUserDetailsService下面我们来一一实现下(其实就是依葫芦画瓢，把对应用户名密码登录流程对应组件的代码COPY过来改一改) UsernamePasswordAuthenticationFilter 从请求中拿到 username password生成UsernamePasswordAuthenticationToken (未认证) AuthenticationManager 用token从众多 DaoAuthenticationProvider 选择合适的 org.springframework.security.authentication.AuthenticationProvider (根据 org.springframework.security.authentication.AuthenticationProvider#supports )]]></content>
  </entry>
  <entry>
    <title><![CDATA[sqlserver-index]]></title>
    <url>%2F2019%2F10%2F30%2Fsqlserver-index%2F</url>
    <content type="text"><![CDATA[获取数据库表所占空间大小(包含索引)12345678910111213-- Get Sizes of All Tables and Indexes in a Database Size of each Table (Including Indexes) SELECT t.[Name] AS TableName, p.[rows] AS [RowCount], SUM(a.total_pages) * 8 AS TotalSpaceKB, SUM(a.used_pages) * 8 AS UsedSpaceKBFROM sys.tables tINNER JOIN sys.indexes i ON t.OBJECT_ID = i.object_idINNER JOIN sys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_idINNER JOIN sys.allocation_units a ON p.partition_id = a.container_idWHERE t.is_ms_shipped = 0 AND i.OBJECT_ID &gt; 255GROUP BY t.[Name], p.[Rows]ORDER BY UsedSpaceKB desc 获取数据库索引所占空间大小 方法1 1234567891011--Size of each Index SELECT i.[name] AS IndexName, t.[name] AS TableName, SUM(s.[used_page_count]) * 8 AS IndexSizeKBFROM sys.dm_db_partition_stats AS sINNER JOIN sys.indexes AS i ON s.[object_id] = i.[object_id] AND s.[index_id] = i.[index_id]INNER JOIN sys.tables t ON t.OBJECT_ID = i.object_idGROUP BY i.[name], t.[name]ORDER BY IndexSizeKB desc , i.[name], t.[name] 方法2 1234567891011SELECTOBJECT_SCHEMA_NAME(i.OBJECT_ID) AS SchemaName,OBJECT_NAME(i.OBJECT_ID) AS TableName,i.name AS IndexName,i.index_id AS IndexID,8 * SUM(a.used_pages) AS 'Indexsize(KB)'FROM sys.indexes AS iJOIN sys.partitions AS p ON p.OBJECT_ID = i.OBJECT_ID AND p.index_id = i.index_idJOIN sys.allocation_units AS a ON a.container_id = p.partition_idGROUP BY i.OBJECT_ID,i.index_id,i.nameORDER BY OBJECT_NAME(i.OBJECT_ID),i.index_id 索引碎片检查 查找 AdventureWorks2016 数据库中 HumanResources.Employee 表内所有索引的平均碎片百分比123456789101112SELECT a.object_id, object_name(a.object_id) AS TableName, a.index_id, name AS IndedxName, avg_fragmentation_in_percentFROM sys.dm_db_index_physical_stats (DB_ID (N'AdventureWorks2016_EXT') , OBJECT_ID(N'HumanResources.Employee') , NULL , NULL , NULL) AS aINNER JOIN sys.indexes AS b ON a.object_id = b.object_id AND a.index_id = b.index_id;GO 未使用的索引123456789101112131415161718192021222324252627SELECT TOP 25o.name AS ObjectName, i.name AS IndexName, i.index_id AS IndexID, dm_ius.user_seeks AS UserSeek, dm_ius.user_scans AS UserScans, dm_ius.user_lookups AS UserLookups, dm_ius.user_updates AS UserUpdates, p.TableRows, 'DROP INDEX ' + QUOTENAME(i.name)+ ' ON ' + QUOTENAME(s.name) + '.'+ QUOTENAME(OBJECT_NAME(dm_ius.OBJECT_ID)) AS 'drop statement'FROM sys.dm_db_index_usage_stats dm_iusINNER JOIN sys.indexes i ON i.index_id = dm_ius.index_id AND dm_ius.OBJECT_ID = i.OBJECT_IDINNER JOIN sys.objects o ON dm_ius.OBJECT_ID = o.OBJECT_IDINNER JOIN sys.schemas s ON o.schema_id = s.schema_idINNER JOIN (SELECT SUM(p.rows) TableRows, p.index_id, p.OBJECT_IDFROM sys.partitions p GROUP BY p.index_id, p.OBJECT_ID) pON p.index_id = dm_ius.index_id AND dm_ius.OBJECT_ID = p.OBJECT_IDWHERE OBJECTPROPERTY(dm_ius.OBJECT_ID,'IsUserTable') = 1AND dm_ius.database_id = DB_ID()AND i.type_desc = 'nonclustered'AND i.is_primary_key = 0AND i.is_unique_constraint = 0ORDER BY (dm_ius.user_seeks + dm_ius.user_scans + dm_ius.user_lookups) ASCGO 索引空间占用12345678910111213141516171819202122232425SELECT COUNT (1) * 8 / 1024 AS MBUsed, OBJECT_SCHEMA_NAME(object_id) SchemaName, name AS TableName, index_id FROM sys.dm_os_buffer_descriptors AS bd INNER JOIN ( SELECT object_name(object_id) AS name ,index_id ,allocation_unit_id, object_id FROM sys.allocation_units AS au INNER JOIN sys.partitions AS p ON au.container_id = p.hobt_id AND (au.type = 1 OR au.type = 3) UNION ALL SELECT object_name(object_id) AS name ,index_id, allocation_unit_id, object_id FROM sys.allocation_units AS au INNER JOIN sys.partitions AS p ON au.container_id = p.partition_id AND au.type = 2 ) AS obj ON bd.allocation_unit_id = obj.allocation_unit_id WHERE database_id = DB_ID() GROUP BY OBJECT_SCHEMA_NAME(object_id), name, index_id ORDER BY COUNT (*) * 8 / 1024 DESCGO 列出所有会话exec sp_who 执行时间长最消耗资源session1234567891011121314151617SELECT TOP(50) qs.execution_count AS [Execution Count],(qs.total_logical_reads)*8/1024.0 AS [Total Logical Reads (MB)],(qs.total_logical_reads/qs.execution_count)*8/1024.0 AS [Avg Logical Reads (MB)],(qs.total_worker_time)/1000.0 AS [Total Worker Time (ms)],(qs.total_worker_time/qs.execution_count)/1000.0 AS [Avg Worker Time (ms)],(qs.total_elapsed_time)/1000.0 AS [Total Elapsed Time (ms)],(qs.total_elapsed_time/qs.execution_count)/1000.0 AS [Avg Elapsed Time (ms)],qs.creation_time AS [Creation Time],t.text AS [Complete Query Text], qp.query_plan AS [Query Plan]FROM sys.dm_exec_query_stats AS qs WITH (NOLOCK)CROSS APPLY sys.dm_exec_sql_text(plan_handle) AS tCROSS APPLY sys.dm_exec_query_plan(plan_handle) AS qpWHERE t.dbid = DB_ID()ORDER BY qs.execution_count DESC OPTION (RECOMPILE);-- frequently ran query-- ORDER BY [Total Logical Reads (MB)] DESC OPTION (RECOMPILE);-- High Disk Reading query-- ORDER BY [Avg Worker Time (ms)] DESC OPTION (RECOMPILE);-- High CPU query-- ORDER BY [Avg Elapsed Time (ms)] DESC OPTION (RECOMPILE);-- Long Running query 加速索引重建 使用 SORT_IN_TEMPDB 加速重建索引,tempdb 与userdatabase 不在相同磁盘上有效123ALTER INDEX [NameOfTheIndex] ON [SchemaName].[TableName]REBUILD PARTITION = ALL WITH (SORT_IN_TEMPDB = ON)GO 优化索引方向谓词 == 筛选条件 SQL Server 查询优化器是基于成本的查询优化器。 也就是说，它选择估计处理成本最低的查询计划。 查询优化器基于以下两个主要因素来确定执行查询计划的开销： 查询计划每个级别上处理的总行数，称为该计划的基数。 由查询中所使用的运算符规定的算法的开销模式。 第一个因素（基数）用作第二个因素（开销模式）的输入参数。 因此，增大基数将减少估计开销，从而加快执行计划。SQL Server 中的基数估计 (CE)主 要派生自创建索引或统计信息时所创建的直方图（以手动或自动方式）。 有时，SQL Server 还使用查询的约束信息和逻辑重写来确定基数。在下列情况下，SQL Server 无法精确计算基数。 这会导致成本计算不准确，进而可能导致查询计划欠佳。 避免在查询中使用这些构造可以提高查询性能。 有时，使用查询表达式或其他措施也可以提高查询性能，如下所述： 带谓词的查询，这些查询在同一表的不同列之间使用比较运算符。 带谓词的查询，这些查询使用运算符且下列任何一种情况为 True： 运算符两侧所涉及的列中没有统计信息。 统计信息中值的分布不均匀，但查询将查找高选择性的值集。 特别是，当运算符是除相等 (=) 运算符以外的任何其他运算符时，这种情况可能为 True。 谓词使用不等于 (!=) 比较运算符或 NOT 逻辑运算符。 使用任意 SQL Server 内置函数或标量值用户定义函数（其参数不是常量值）的查询。 包含通过算术或字符串串联运算符联接的列的查询。 比较在编译或优化查询时其值未知的变量的查询。]]></content>
  </entry>
  <entry>
    <title><![CDATA[spring-simplejdbccall]]></title>
    <url>%2F2019%2F10%2F23%2Fspring-simplejdbccall%2F</url>
    <content type="text"><![CDATA[怎样理解 spring simplejdbccall ?https://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/jdbc/core/simple/SimpleJdbcCall.html 1A SimpleJdbcCall is a `multi-threaded`, `reusable` object representing a call to a stored procedure or a stored function. It provides meta-data processing to simplify the code needed to access basic stored procedures/functions. All you need to provide is the name of the procedure/function and a Map containing the parameters when you execute the call. The names of the supplied parameters will be matched up with in and out parameters declared when the stored procedure was created. 回答 123456789So, is it a Spring&apos;s bug ?No, you&apos;re just using it incorrectly. The documentation for SimpleJdbcCall could perhaps be more explicit, but it does say:A SimpleJdbcCall is a multi-threaded, reusable object representing a call to a stored procedure or a stored function.In other words, each instance of SimpleJdbcCall is configured to invoke a specific stored procedure. Once configured, it shouldn&apos;t be changed.If you need to invoke multiple stored procedures, you need to have multiple SimpleJdbcCall objects. https://stackoverflow.com/questions/6592814/simplejdbccall-can-not-call-more-than-one-procedure]]></content>
  </entry>
  <entry>
    <title><![CDATA[不刷新加载第三方JS]]></title>
    <url>%2F2019%2F09%2F26%2F%E4%B8%8D%E5%88%B7%E6%96%B0%E5%8A%A0%E8%BD%BD%E7%AC%AC%E4%B8%89%E6%96%B9JS%2F</url>
    <content type="text"><![CDATA[异步加载 异步加载JS的方法很多，最常见的就是动态创建一个script标签，然后设置其src和async属性，再插入到页面中 1234567891011 &lt;script&gt; function loadScript(url) &#123; var scrs = document.getElementsByTagName('script'); var last = scrs[scrs.length - 1]; var scr = document.createElement('script'); scr.src = url; scr.async = true; last.parentNode.insertBefore(scr, last); &#125; loadScript('test.js');&lt;/script&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker 修改容器启动配置]]></title>
    <url>%2F2019%2F09%2F19%2Fdocker-%E4%BF%AE%E6%94%B9%E5%AE%B9%E5%99%A8%E5%90%AF%E5%8A%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[修改容器启动配置 1docker container update --restart=always &lt;containername&gt; 配置文件路径为 文档https://docs.docker.com/engine/reference/commandline/container_update/]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux server 计算PI]]></title>
    <url>%2F2019%2F09%2F18%2Flinux-server-%E8%AE%A1%E7%AE%97PI%2F</url>
    <content type="text"><![CDATA[测试一下单核CPU的计算能力1time echo "scale=10000; 4*a(1)" | bc -l]]></content>
  </entry>
  <entry>
    <title><![CDATA[CENTOS/RHEL 7 系统中设置SYSTEMD SERVICE的ULIMIT资源限制]]></title>
    <url>%2F2019%2F09%2F18%2FCENTOS-RHEL-7-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E8%AE%BE%E7%BD%AESYSTEMD-SERVICE%E7%9A%84ULIMIT%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[在bash中，有个ulimit命令，提供了对shell及该shell启动的进程的可用资源控制。主要包括打开文件描述符数量、用户的最大进程数量、coredump文件的大小等。 在centos 5/6 等版本中，资源限制的配置可以在/etc/security/limits.conf 设置，针对root/user等各个用户或者*代表所有用户来设置。 当然，/etc/security/limits.d/ 中可以配置，系统是先加载limits.conf然后按照英文字母顺序加载limits.d目录下的配置文件，后加载配置覆盖之前的配置。 一个配置示例如下： 12345678910111213[root@ziggle-linux yum.repos.d]# cat /etc/security/limits.conf # /etc/security/limits.conf##This file sets the resource limits for the users logged in via PAM.#It does not affect resource limits of the system services.##Also note that configuration files in /etc/security/limits.d directory,#which are read in alphabetical order, override the settings in this#file in case the domain is the same or more specific.#That means for example that setting a limit for wildcard domain here#can be overriden with a wildcard setting in a config file in the#subdirectory, but a user specific setting here can be overriden only#with a user specific setting in the subdirectory. 123456* soft nofile 100000* hard nofile 100000* soft nproc 100000* hard nproc 100000* soft core 100000* hard core 100000 不过，在CentOS 7 / RHEL 7的系统中，使用Systemd替代了之前的SysV，因此 /etc/security/limits.conf 文件的配置作用域缩小了一些。limits.conf这里的配置，只适用于通过PAM认证登录用户的资源限制，它对systemd的service的资源限制不生效。登录用户的限制，与上面讲的一样，通过 /etc/security/limits.conf 和 limits.d 来配置即可。对于systemd service的资源限制，如何配置呢？ 全局的配置，放在文件 /etc/systemd/system.conf 和 /etc/systemd/user.conf。 同时，也会加载两个对应的目录中的所有.conf文件 /etc/systemd/system.conf.d/*.conf 和 /etc/systemd/user.conf.d/*.conf其中，system.conf 是系统实例使用的，user.conf用户实例使用的。一般的sevice，使用system.conf中的配置即可。systemd.conf.d/*.conf中配置会覆盖system.conf。 123DefaultLimitCORE=infinityDefaultLimitNOFILE=100000DefaultLimitNPROC=100000 注意：修改了system.conf后，需要重启系统才会生效。 针对单个Service，也可以设置，以nginx为例。编辑 /usr/lib/systemd/system/nginx.service 文件，或者 /usr/lib/systemd/system/nginx.service.d/my-limit.conf 文件，做如下配置： 1234[Service]LimitCORE=infinityLimitNOFILE=100000LimitNPROC=100000 然后运行如下命令，才能生效。 12sudo systemctl daemon-reloadsudo systemctl restart nginx.service 查看一个进程的limit设置：cat /proc/YOUR-PID/limits例如我的一个nginx service的配置效果： 123456789101112131415161718$cat /proc/$(cat /var/run/nginx.pid)/limitsLimit Soft Limit Hard Limit UnitsMax cpu time unlimited unlimited secondsMax file size unlimited unlimited bytesMax data size unlimited unlimited bytesMax stack size 8388608 unlimited bytesMax core file size unlimited unlimited bytesMax resident set unlimited unlimited bytesMax processes 100000 100000 processesMax open files 100000 100000 filesMax locked memory 65536 65536 bytesMax address space unlimited unlimited bytesMax file locks unlimited unlimited locksMax pending signals 1030606 1030606 signalsMax msgqueue size 819200 819200 bytesMax nice priority 0 0Max realtime priority 0 0Max realtime timeout unlimited unlimited us 顺便提一下，我还被CentOS7自带的/etc/security/limits.d/20-nproc.conf文件坑过，里面默认设置了非root用户的最大进程数为4096，难怪我上次在limits.conf中设置了没啥效果，原来被limit.d目录中的配置覆盖了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[netty-handler]]></title>
    <url>%2F2019%2F08%2F28%2Fnetty-handler%2F</url>
    <content type="text"><![CDATA[channel的生命状态周期 状态 说明 channelUnregistered 说明 channelUnregistered channel创建之后，还未注册到EventLoop channelRegistered channel注册到了对应的EventLoop channelActive channel处于活跃状态，活跃状态表示已经连接到了远程服务器，现在可以接收和发送数据 channelInactive channel 未连接到远程服务器 一个Channel正常的生命周期如下channelRegistered -&gt; channelActice -&gt; channelInactive -&gt; channelUnregistered 零拷贝（DIRECT BUFFERS 使用堆外直接内存） Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行 Socket 读写，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。 Netty 提供了组合 Buffer 对象，可以聚合多个 ByteBuffer 对象，用户可以像操作一个 Buffer 那样方便的对组合 Buffer 进行操作，避免了传统通过内存拷贝的方式将几个小 Buffer 合并成一个大的Buffer。 Netty的文件传输采用了transferTo方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环 write 方式导致的内存拷贝问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker-compose]]></title>
    <url>%2F2019%2F08%2F25%2Fdocker-compose%2F</url>
    <content type="text"><![CDATA[安装docker-compose1curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose &amp;&amp; sudo chmod +x /usr/local/bin/docker-compose docker-compose volumes 卷标1docker volume ls mysql-配置12345678910111213141516171819202122232425262728293031323334353637383940version: '3.3'services: redis: # 指定镜像 image: redis:4 ports: # 端口映射 - 6379:6379 volumes: # 目录映射 - "redis-db:/usr/local/etc/redis" - "redis-db:/data" command: # 执行的命令 redis-server mysql: image: mysql:5.7 restart: always environment: MYSQL_DATABASE: 'db' # So you don't have to use root, but you can if you like MYSQL_USER: 'root' # You can use whatever password you like MYSQL_PASSWORD: '123456' # Password for root access MYSQL_ROOT_PASSWORD: '123456' ports: # &lt;Port exposed&gt; : &lt; MySQL Port running inside container&gt; - '3306:3306' expose: # Opens port 3306 on the container - '3306' # Where our data will be persisted volumes: - my-db:/var/lib/mysql - my-db:/etc/my.cnf"# Names our volumevolumes: my-db: redis-db:]]></content>
  </entry>
  <entry>
    <title><![CDATA[regex]]></title>
    <url>%2F2019%2F08%2F07%2Fregex%2F</url>
    <content type="text"><![CDATA[Translations: English Español Français Português do Brasil 中文版 日本語 한국어 Turkish Greek Magyar Polish Русский 什么是正则表达式? 正则表达式是一组由字母和符号组成的特殊文本, 它可以用来从文本中找出满足你想要的格式的句子. 一个正则表达式是在一个主体字符串中从左到右匹配字符串时的一种样式.例如”Regular expression”是一个完整的句子, 但我们常使用缩写的术语”regex”或”regexp”.正则表达式可以用来替换文本中的字符串,验证形式,提取字符串等等. 想象你正在写一个应用, 然后你想设定一个用户命名的规则, 让用户名包含字符,数字,下划线和连字符,以及限制字符的个数,好让名字看起来没那么丑.我们使用以下正则表达式来验证一个用户名: 以上的正则表达式可以接受 john_doe, jo-hn_doe, john12_as.但不匹配Jo, 因为它包含了大写的字母而且太短了. 目录 1. 基本匹配 2. 元字符 2.1 点运算符 . 2.2 字符集 2.2.1 否定字符集 2.3 重复次数 2.3.1 * 号 2.3.2 号 2.3.3 ? 号 2.4 {} 号 2.5 (…) 特征标群 2.6 | 或运算符 2.7 转码特殊字符 2.8 锚点 2.8.1 ^ 号 2.8.2 $ 号 3. 简写字符集 4. 前后关联约束(前后预查) 4.1 ?=… 前置约束(存在) 4.2 ?!… 前置约束-排除 4.3 ?&lt;= … 后置约束-存在 4.4 ?&lt;!… 后置约束-排除 5. 标志 5.1 忽略大小写 (Case Insensitive) 5.2 全局搜索 (Global search) 5.3 多行修饰符 (Multiline) 额外补充 贡献 许可证 1. 基本匹配正则表达式其实就是在执行搜索时的格式, 它由一些字母和数字组合而成.例如: 一个正则表达式 the, 它表示一个规则: 由字母t开始,接着是h,再接着是e. "the" => The fat cat sat on the mat. 在线练习 正则表达式123匹配字符串123. 它逐个字符的与输入的正则表达式做比较. 正则表达式是大小写敏感的, 所以The不会匹配the. "The" => The fat cat sat on the mat. 在线练习 2. 元字符正则表达式主要依赖于元字符.元字符不代表他们本身的字面意思, 他们都有特殊的含义. 一些元字符写在方括号中的时候有一些特殊的意思. 以下是一些元字符的介绍: 元字符 描述 . 句号匹配任意单个字符除了换行符. [ ] 字符种类. 匹配方括号内的任意字符. [^ ] 否定的字符种类. 匹配除了方括号里的任意字符 * 匹配&gt;=0个重复的在*号之前的字符. + 匹配&gt;1个重复的+号前的字符. ? 标记?之前的字符为可选. {n,m} 匹配num个中括号之前的字符 (n &lt;= num &lt;= m). (xyz) 字符集, 匹配与 xyz 完全相等的字符串. &#124; 或运算符,匹配符号前或后的字符. &#92; 转义字符,用于匹配一些保留的字符 [ ] ( ) { } . * + ? ^ $ \ &#124; ^ 从开始行开始匹配. $ 从末端开始匹配. 2.1 点运算符 ..是元字符中最简单的例子..匹配任意单个字符, 但不匹配换行符.例如, 表达式.ar匹配一个任意字符后面跟着是a和r的字符串. ".ar" => The car parked in the garage. 在线练习 2.2 字符集字符集也叫做字符类.方括号用来指定一个字符集.在方括号中使用连字符来指定字符集的范围.在方括号中的字符集不关心顺序.例如, 表达式[Tt]he 匹配 the 和 The. "[Tt]he" => The car parked in the garage. 在线练习 方括号的句号就表示句号.表达式 ar[.] 匹配 ar.字符串 "ar[.]" => A garage is a good place to park a car. 在线练习 2.2.1 否定字符集一般来说 ^ 表示一个字符串的开头, 但它用在一个方括号的开头的时候, 它表示这个字符集是否定的.例如, 表达式[^c]ar 匹配一个后面跟着ar的除了c的任意字符. "[^c]ar" => The car parked in the garage. 在线练习 2.3 重复次数后面跟着元字符 +, * or ? 的, 用来指定匹配子模式的次数.这些元字符在不同的情况下有着不同的意思. 2.3.1 * 号*号匹配 在*之前的字符出现大于等于0次.例如, 表达式 a* 匹配以0或更多个a开头的字符, 因为有0个这个条件, 其实也就匹配了所有的字符. 表达式[a-z]* 匹配一个行中所有以小写字母开头的字符串. "[a-z]*" => The car parked in the garage #21. 在线练习 *字符和.字符搭配可以匹配所有的字符.*.*和表示匹配空格的符号\s连起来用, 如表达式\s*cat\s*匹配0或更多个空格开头和0或更多个空格结尾的cat字符串. "\s*cat\s*" => The fat cat sat on the concatenation. 在线练习 2.3.2 + 号+号匹配+号之前的字符出现 &gt;=1 次个字符.例如表达式c.+t 匹配以首字母c开头以t结尾,中间跟着任意个字符的字符串. "c.+t" => The fat cat sat on the mat. 在线练习 2.3.3 ? 号在正则表达式中元字符 ? 标记在符号前面的字符为可选, 即出现 0 或 1 次.例如, 表达式 [T]?he 匹配字符串 he 和 The. "[T]he" => The car is parked in the garage. 在线练习 "[T]?he" => The car is parked in the garage. 在线练习 2.4 {} 号在正则表达式中 {} 是一个量词, 常用来一个或一组字符可以重复出现的次数.例如, 表达式 [0-9]{2,3} 匹配 2~3 位 0~9 的数字. "[0-9]{2,3}" => The number was 9.9997 but we rounded it off to 10.0. 在线练习 我们可以省略第二个参数.例如, [0-9]{2,} 匹配至少两位 0~9 的数字. 如果逗号也省略掉则表示重复固定的次数.例如, [0-9]{3} 匹配3位数字 "[0-9]{2,}" => The number was 9.9997 but we rounded it off to 10.0. 在线练习 "[0-9]{3}" => The number was 9.9997 but we rounded it off to 10.0. 在线练习 2.5 (...) 特征标群特征标群是一组写在 (...) 中的子模式. 例如之前说的 {} 是用来表示前面一个字符出现指定次数. 但如果在 {} 前加入特征标群则表示整个标群内的字符重复 N 次. 例如, 表达式 (ab)* 匹配连续出现 0 或更多个 ab. 我们还可以在 () 中用或字符 | 表示或. 例如, (c|g|p)ar 匹配 car 或 gar 或 par. "(c|g|p)ar" => The car is parked in the garage. 在线练习 2.6 | 或运算符或运算符就表示或, 用作判断条件. 例如 (T|t)he|car 匹配 (T|t)he 或 car. "(T|t)he|car" => The car is parked in the garage. 在线练习 2.7 转码特殊字符反斜线 \ 在表达式中用于转码紧跟其后的字符. 用于指定 { } [ ] / \ + * . $ ^ | ? 这些特殊字符. 如果想要匹配这些特殊字符则要在其前面加上反斜线 \. 例如 . 是用来匹配除换行符外的所有字符的. 如果想要匹配句子中的 . 则要写成 \.. "(f|c|m)at\.?" => The fat cat sat on the mat. 在线练习 2.8 锚点在正则表达式中, 想要匹配指定开头或结尾的字符串就要使用到锚点. ^ 指定开头, $ 指定结尾. 2.8.1 ^ 号^ 用来检查匹配的字符串是否在所匹配字符串的开头. 例如, 在 abc 中使用表达式 ^a 会得到结果 a. 但如果使用 ^b 将匹配不到任何结果. 应为在字符串 abc 中并不是以 b 开头. 例如, ^(T|t)he 匹配以 The 或 the 开头的字符串. "(T|t)he" => The car is parked in the garage. 在线练习 "^(T|t)he" => The car is parked in the garage. 在线练习 2.8.2 $ 号同理于 ^ 号, $ 号用来匹配字符是否是最后一个. 例如, (at\.)$ 匹配以 at. 结尾的字符串. "(at\.)" => The fat cat. sat. on the mat. 在线练习 "(at\.)$" => The fat cat. sat. on the mat. 在线练习 3. 简写字符集正则表达式提供一些常用的字符集简写. 如下: 简写 描述 . 除换行符外的所有字符 \w 匹配所有字母数字, 等同于 [a-zA-Z0-9_] \W 匹配所有非字母数字, 即符号, 等同于: [^\w] \d 匹配数字: [0-9] \D 匹配非数字: [^\d] \s 匹配所有空格字符, 等同于: [\t\n\f\r\p{Z}] \S 匹配所有非空格字符: [^\s] 4. 前后关联约束(前后预查)前置约束和后置约束都属于非捕获簇(用于匹配不在匹配列表中的格式).前置约束用于判断所匹配的格式是否在另一个确定的格式之后. 例如, 我们想要获得所有跟在 $ 符号后的数字, 我们可以使用正向向后约束 (?&lt;=\$)[0-9\.]*.这个表达式匹配 $ 开头, 之后跟着 0,1,2,3,4,5,6,7,8,9,. 这些字符可以出现大于等于 0 次. 前后关联约束如下: 符号 描述 ?= 前置约束-存在 ?! 前置约束-排除 ?&lt;= 后置约束-存在 ?&lt;! 后置约束-排除 4.1 ?=... 前置约束(存在)?=... 前置约束(存在), 表示第一部分表达式必须跟在 ?=...定义的表达式之后. 返回结果只瞒住第一部分表达式.定义一个前置约束(存在)要使用 (). 在括号内部使用一个问号和等号: (?=...). 前置约束的内容写在括号中的等号后面.例如, 表达式 [T|t]he(?=\sfat) 匹配 The 和 the, 在括号中我们又定义了前置约束(存在) (?=\sfat) ,即 The 和 the 后面紧跟着 (空格)fat. "[T|t]he(?=\sfat)" => The fat cat sat on the mat. 在线练习 4.2 ?!... 前置约束-排除前置约束-排除 ?! 用于筛选所有匹配结果, 筛选条件为 其后不跟随着定义的格式前置约束-排除 定义和 前置约束(存在) 一样, 区别就是 = 替换成 ! 也就是 (?!...). 表达式 [T|t]he(?!\sfat) 匹配 The 和 the, 且其后不跟着 (空格)fat. "[T|t]he(?!\sfat)" => The fat cat sat on the mat. 在线练习 4.3 ?&lt;= ... 后置约束-存在后置约束-存在 记作(?&lt;=...) 用于筛选所有匹配结果, 筛选条件为 其前跟随着定义的格式.例如, 表达式 (?&lt;=[T|t]he\s)(fat|mat) 匹配 fat 和 mat, 且其前跟着 The 或 the. "(? 在线练习 4.4 ?&lt;!... 后置约束-排除后置约束-排除 记作 (?&lt;!...) 用于筛选所有匹配结果, 筛选条件为 其前不跟着定义的格式.例如, 表达式 (?&lt;!(T|t)he\s)(cat) 匹配 cat, 且其前不跟着 The 或 the. "(?&lt;![T|t]he\s)(cat)" => The cat sat on cat. 在线练习 5. 标志标志也叫修饰语, 因为它可以用来修改表达式的搜索结果.这些标志可以任意的组合使用, 它也是整个正则表达式的一部分. 标志 描述 i 忽略大小写. g 全局搜索. m 多行的: 锚点元字符 ^ $ 工作范围在每行的起始. 5.1 忽略大小写 (Case Insensitive)修饰语 i 用于忽略大小写.例如, 表达式 /The/gi 表示在全局搜索 The, 在后面的 i 将其条件修改为忽略大小写, 则变成搜索 the 和 The, g 表示全局搜索. "The" => The fat cat sat on the mat. 在线练习 "/The/gi" => The fat cat sat on the mat. 在线练习 5.2 全局搜索 (Global search)修饰符 g 常用于执行一个全局搜索匹配, 即(不仅仅返回第一个匹配的, 而是返回全部).例如, 表达式 /.(at)/g 表示搜索 任意字符(除了换行) + at, 并返回全部结果. "/.(at)/" => The fat cat sat on the mat. 在线练习 "/.(at)/g" => The fat cat sat on the mat. 在线练习 5.3 多行修饰符 (Multiline)多行修饰符 m 常用于执行一个多行匹配. 像之前介绍的 (^,$) 用于检查格式是否是在待检测字符串的开头或结尾. 但我们如果想要它在每行的开头和结尾生效, 我们需要用到多行修饰符 m. 例如, 表达式 /at(.)?$/gm 表示在待检测字符串每行的末尾搜索 at后跟一个或多个 . 的字符串, 并返回全部结果. "/.at(.)?$/" => The fat cat sat on the mat. 在线练习 "/.at(.)?$/gm" => The fat cat sat on the mat. 在线练习 额外补充 正整数: ^\d+$ 负整数: ^-\d+$ 手机国家号: ^+?[\d\s]{3,}$ 手机号: ^+?[\d\s]+(?[\d\s]{10,}$ 整数: ^-?\d+$ 用户名: ^[\w\d_.]{4,16}$ 数字和英文字母: ^[a-zA-Z0-9]*$ 数字和应为字母和空格: ^[a-zA-Z0-9 ]*$ 密码: ^(?=^.{6,}$)((?=.*[A-Za-z0-9])(?=.*[A-Z])(?=.*[a-z]))^.*$ 邮箱: ^([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,4})*$ IP4 地址: ^((?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))*$ 纯小写字母: ^([a-z])*$ 纯大写字母: ^([A-Z])*$ URL: ^(((http|https|ftp):\/\/)?([[a-zA-Z0-9]\-\.])+(\.)([[a-zA-Z0-9]]){2,4}([[a-zA-Z0-9]\/+=%&amp;_\.~?\-]*))*$ VISA 信用卡号: ^(4[0-9]{12}(?:[0-9]{3})?)*$ 日期 (MM/DD/YYYY): ^(0?[1-9]|1[012])[- /.](0?[1-9]|[12][0-9]|3[01])[- /.](19|20)?[0-9]{2}$ 日期 (YYYY/MM/DD): ^(19|20)?[0-9]{2}[- /.](0?[1-9]|1[012])[- /.](0?[1-9]|[12][0-9]|3[01])$ MasterCard 信用卡号: ^(5[1-5][0-9]{14})*$ 贡献 报告问题 开放合并请求 传播此文档 直接和我联系 ziishaned@gmail.com 或 许可证MIT &copy; Zeeshan Ahmad]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux-awk]]></title>
    <url>%2F2019%2F08%2F06%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[打印某一列1awk '&#123;print $1&#125;' file 1234awk -F &quot;,&quot; &apos;&#123;print $1,$2&#125;&apos; file |参数 | |操作|awk -F &quot;,&quot; &apos;/^a/ &#123;print $3&#125;&apos; file]]></content>
  </entry>
  <entry>
    <title><![CDATA[spring自动配置]]></title>
    <url>%2F2019%2F08%2F06%2Fspring%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[自动配置是如何实现1、Spring Boot 的自动配置是如何实现的？Spring Boot 项目的启动注解是：@SpringBootApplication，其实它就是由下面三个注解组成的： @Configuration @ComponentScan @EnableAutoConfiguration 其中 @EnableAutoConfiguration 是实现自动配置的入口，该注解又通过 @Import 注解导入了AutoConfigurationImportSelector，在该类中加载 META-INF/spring.factories 的配置信息。然后筛选出以 EnableAutoConfiguration 为 key 的数据，加载到 IOC 容器中，实现自动配置功能！]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-lock]]></title>
    <url>%2F2019%2F08%2F05%2Fjava-lock%2F</url>
    <content type="text"><![CDATA[synchronized使用场景Synchronized修饰普通同步方法：锁对象当前实例对象；Synchronized修饰静态同步方法：锁对象是当前的类Class对象；Synchronized修饰同步代码块：锁对象是Synchronized后面括号里配置的对象，这个对象可以是某个对象（xlock），也可以是某个类（Xlock.class）； 注意 使用synchronized修饰非静态方法或者使用synchronized修饰代码块时制定的为实例对象时，同一个类的不同对象拥有自己的锁，因此不会相互阻塞 使用synchronized修饰类和对象时，由于类对象和实例对象分别拥有自己的监视器锁，因此不会相互阻塞。 使用使用synchronized修饰实例对象时，如果一个线程正在访问实例对象的一个synchronized方法时，其它线程不仅不能访问该synchronized方法，该对象的其它synchronized方法也不能访问，因为一个对象只有一个监视器锁对象，但是其它线程可以访问该对象的非synchronized方法。 线程A访问实例对象的非static synchronized方法时，线程B也可以同时访问实例对象的static synchronized方法，因为前者获取的是实例对象的监视器锁，而后者获取的是类对象的监视器锁，两者不存在互斥关系 java 对象头Hotspot 虚拟机的对象头主要包含2部分数据, Mark Word(标记字段) Klass Pointer(类型指针). 其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据，它是实现轻量级锁和偏向锁的关键 Mark Word。Mark Word用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。Java对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit），但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。下图是Java对象头的存储结构（32位虚拟机） 对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，变化状态如下（32位虚拟机） 实现原理java 对象头对象在队中的三个部分 , 对象头, 实例变量, 填充字节 对象头的zhuyao是由MarkWord和Klass Point(类型指针)组成，其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据。如果对象是数组对象，那么对象头占用3个字宽（Word），如果对象是非数组对象，那么对象头占用2个字宽。（1word = 2 Byte = 16 bit） 实例变量存储的是对象的属性信息，包括父类的属性信息，按照4字节对齐 填充字符，因为虚拟机要求对象字节必须是8字节的整数倍，填充字符就是用于凑齐这个整数倍的 synchronized 都是通过持有修饰对象的锁来实现.那么Synchronized锁对象是存在哪里的呢？答案是存在锁对象的对象头的MarkWord中。那么MarkWord在对象头中到底长什么样，也就是它到底存储了什么呢? 重量级锁对应的锁标志位是10，存储了指向重量级监视器锁的指针，在Hotspot中，对象的监视器（monitor）锁对象由ObjectMonitor对象实现（C++），其跟同步相关的数据结构如下： 123456789ObjectMonitor() &#123; _count = 0; //用来记录该对象被线程获取锁的次数 _waiters = 0; _recursions = 0; //锁的重入次数 _owner = NULL; //指向持有ObjectMonitor对象的线程 _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表 &#125; 偏向锁的升级当线程1访问代码块并获取锁对象时，会在java对象头和栈帧中记录偏向的锁的threadID，因为偏向锁不会主动释放锁，因此以后线程1再次获取锁的时候，需要比较当前线程的threadID和Java对象头中的threadID是否一致，如果一致（还是线程1获取锁对象），则无需使用CAS来加锁、解锁；如果不一致（其他线程，如线程2要竞争锁对象，而偏向锁不会主动释放因此还是存储的线程1的threadID），那么需要查看Java对象头中记录的线程1是否存活，如果没有存活，那么锁对象被重置为无锁状态，其它线程（线程2）可以竞争将其设置为偏向锁；如果存活，那么立刻查找该线程（线程1）的栈帧信息，如果还是需要继续持有这个锁对象，那么暂停当前线程1，撤销偏向锁，升级为轻量级锁，如果线程1 不再使用该锁对象，那么将锁对象状态设为无锁状态，重新偏向新的线程。 轻量级锁为什么要引入轻量级锁？ 轻量级锁考虑的是竞争锁对象的线程不多，而且线程持有锁的时间也不长的情景。因为阻塞线程需要CPU从用户态转到内核态，代价较大，如果刚刚阻塞不久这个锁就被释放了，那这个代价就有点得不偿失了，因此这个时候就干脆不阻塞这个线程，让它自旋这等待锁释放 轻量级锁什么时候升级为重量级锁？ 线程1获取轻量级锁时会先把锁对象的对象头MarkWord复制一份到线程1的栈帧中创建的用于存储锁记录的空间（称为DisplacedMarkWord），然后使用CAS把对象头中的内容替换为线程1存储的锁记录（DisplacedMarkWord）的地址；如果在线程1复制对象头的同时（在线程1CAS之前），线程2也准备获取锁，复制了对象头到线程2的锁记录空间中，但是在线程2CAS的时候，发现线程1已经把对象头换了，线程2的CAS失败，那么线程2就尝试使用自旋锁来等待线程1释放锁。 但是如果自旋的时间太长也不行，因为自旋是要消耗CPU的，因此自旋的次数是有限制的，比如10次或者100次，如果自旋次数到了线程1还没有释放锁，或者线程1还在执行，线程2还在自旋等待，这时又有一个线程3过来竞争这个锁对象，那么这个时候轻量级锁就会膨胀为重量级锁。重量级锁把除了拥有锁的线程都阻塞，防止CPU空转。 注意：为了避免无用的自旋，轻量级锁一旦膨胀为重量级锁就不会再降级为轻量级锁了；偏向锁升级为轻量级锁也不能再降级为偏向锁。一句话就是锁可以升级不可以降级，但是偏向锁状态可以被重置为无锁状态。 锁粗化按理来说，同步块的作用范围应该尽可能小，仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作数量尽可能缩小，缩短阻塞时间，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。但是加锁解锁也需要消耗资源，如果存在一系列的连续加锁解锁操作，可能会导致不必要的性能损耗。锁粗化就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁，避免频繁的加锁解锁操作。 锁消除Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，经过逃逸分析，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-lib]]></title>
    <url>%2F2019%2F07%2F10%2Fjava-lib%2F</url>
    <content type="text"><![CDATA[PLACEHOLDER]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-guava]]></title>
    <url>%2F2019%2F07%2F10%2Fjava-guava%2F</url>
    <content type="text"><![CDATA[RateLimiter基本使用1234567891011121314public class RateLimiterTest &#123; // 1秒钟产生0.5张令牌 private final static RateLimiter limiter = RateLimiter.create(0.5); public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(5); IntStream.range(0, 5).forEach(i -&gt; service.submit(RateLimiterTest::testLimiter)); service.shutdown(); &#125; private static void testLimiter() &#123; System.out.println(Thread.currentThread() + " waiting " + limiter.acquire()); &#125;&#125; 设置超时时间1234567public class RateLimiterTest &#123; public static void main(String[] args) &#123; RateLimiter limiter = RateLimiter.create(1); System.out.println(limiter.acquire(3)); System.out.println(limiter.tryAcquire(1, 2, TimeUnit.SECONDS)); &#125;&#125; 上面例子limiter.tryAcquire设置了超时时间为2秒，由于第一次请求一次性获取了3张令牌，所以这里需要等待大约3秒钟，超出了2秒的超时时间，所以limiter.tryAcquire不会等待3秒，而是直接返回false。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-thread]]></title>
    <url>%2F2019%2F07%2F10%2Fjava-thread%2F</url>
    <content type="text"><![CDATA[终止线程的方法使用退出标志退出线程Interrupt 方法结束线程123456789101112 public class ThreadSafe extends Thread &#123; public void run() &#123; while (!isInterrupted())&#123; //非阻塞过程中通过判断中断标志来退出 try&#123; Thread.sleep(5*1000);//阻塞过程捕获中断异常来退出 &#125;catch(InterruptedException e)&#123; e.printStackTrace(); break;//捕获到异常之后，执行 break 跳出循环 &#125; &#125; &#125;&#125; sleep 与 wait 区别 对于 sleep()方法，我们首先要知道该方法是属于 Thread 类中的。而 wait()方法，则是属于Object 类中的。 sleep()方法导致了程序暂停执行指定的时间，让出 cpu 该其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态。 在调用 sleep()方法的过程中，线程不会释放对象锁。 而当调用 wait()方法的时候，线程会放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象调用 notify()方法后本线程才进入对象锁定池准备获取对象锁进入运行状态 ref = https://mrbird.cc/JUC-CyclicBarrier.html JUC之CyclicBarrierCyclicBarrier的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续运行。CyclicBarrier默认的构造方法是CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达了屏障，然后当前线程被阻塞。 CyclicBarrier的构造函数支持传入一个回调方法：123CyclicBarrier barrier = new CyclicBarrier(n, () -&gt; &#123; System.out.println("当所有线程到达屏障时，执行该回调");&#125;); 设置超时时间await的重载方法：await(long timeout, TimeUnit unit)可以设置最大等待时长，超出这个时间屏障还没有开启的话则抛出TimeoutException： BrokenBarrierException抛出BrokenBarrierException异常时表示屏障破损，此时标志位broken=true。抛出BrokenBarrierException异常的情况主要有： 其他等待的线程被中断，则当前线程抛出BrokenBarrierException异常； 其他等待的线程超时，则当前线程抛出BrokenBarrierException异常； 当前线程在等待时，其他线程调用CyclicBarrier.reset()方法，则当前线程抛出BrokenBarrierException异常。 和CountDownLatch区别 CountDownLatch：一个线程(或者多个)，等待另外N个线程完成某个事情之后才能执行；CyclicBarrier：N个线程相互等待，任何一个线程完成之前，所有的线程都必须等待。 CountDownLatch：一次性的；CyclicBarrier：可以重复使用。 JUC之CountDownLatchCountDownLatch允许一个或多个线程等待其他线程完成操作。定义CountDownLatch的时候，需要传入一个正数来初始化计数器（虽然传入0也可以，但这样的话CountDownLatch没什么实际意义）。其countDown方法用于递减计数器，await方法会使当前线程阻塞，直到计数器递减为0。所以CountDownLatch常用于多个线程之间的协调工作。 threadpool 关闭方法123456789101112131415threadPool.shutdown(); // Disable new tasks from being submitted // 设定最大重试次数 try &#123; // 等待 60 s if (!threadPool.awaitTermination(60, TimeUnit.SECONDS)) &#123; // 调用 shutdownNow 取消正在执行的任务 threadPool.shutdownNow(); // 再次等待 60 s，如果还未结束，可以再次尝试，或则直接放弃 if (!threadPool.awaitTermination(60, TimeUnit.SECONDS)) System.err.println("线程池任务未正常执行结束"); &#125; &#125; catch (InterruptedException ie) &#123; // 重新调用 shutdownNow threadPool.shutdownNow(); &#125; 线程池调用 Executor 的 shutdown() 方法会等待线程都执行完毕之后再关闭，但是如果调用的是 shutdownNow() 方法，则相当于调用每个线程的 interrupt() 方法。以下使用 Lambda 创建线程，相当于创建了一个匿名内部线程。 12345678910111213public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -&gt; &#123; try &#123; Thread.sleep(2000); System.out.println("Thread run"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); executorService.shutdownNow(); System.out.println("Main run");&#125; 12345678Main runjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at ExecutorInterruptExample.lambda$main$0(ExecutorInterruptExample.java:9) at ExecutorInterruptExample$$Lambda$1/1160460865.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 如果只想中断 Executor 中的一个线程，可以通过使用 submit() 方法来提交一个线程，它会返回一个 Future&lt;?&gt; 对象，通过调用该对象的 cancel(true) 方法就可以中断线程。 1234Future&lt;?&gt; future = executorService.submit(() -&gt; &#123; // ..&#125;);future.cancel(true); Java并发之线程中断线程在不同状态下对于中断所产生的反应 线程一共6种状态，分别是NEW，RUNNABLE，BLOCKED，WAITING，TIMED_WAITING，TERMINATED（Thread类中有一个State枚举类型列举了线程的所有状态）。下面我们就将把线程分别置于上述的不同种状态，然后看看我们的中断操作对它们的影响。 1、NEW和TERMINATED 线程的new状态表示还未调用start方法，还未真正启动。线程的terminated状态表示线程已经运行终止。这两个状态下调用中断方法来中断线程的时候，Java认为毫无意义，所以并不会设置线程的中断标识位，什么事也不会发生。例如：123456789public static void main(String[] args) throws InterruptedException &#123; Thread thread = new MyThread(); System.out.println(thread.getState()); thread.interrupt(); System.out.println(thread.isInterrupted());&#125; 什么是上下文切换多线程编程中线程数一般大于cpu的个数, 而一个cpu在任意时刻只能被一个线程使用,为了让这些线程都可以有效执行, cpu采用的是为每个线程分配时间片并轮转的形式, 当一个线程的时间片用完的时候就会重新处于就绪状态让其他线程使用,这个过程就是一次上下文切换 当cpu切换到另一个任务之前会先保存自己的状态, 以便于切换回这个任务, 任务从保存到在加载的过程就是一次上下文切换 synchronized使用方式1 修饰实例方法,12345678910111213141516171819public class Singleton&#123; // 禁止指令重排 private volatile static Singleton instance; private Singletion()&#123;&#125; public static Signleton getInstance()&#123; // 没有初始化才进行下面逻辑 if(instance !=null )&#123; synchronized(Singleton.class)&#123; if(instance == null)&#123; instance = new Singleton &#125; &#125; &#125; return instance; &#125; &#125; volatail比 sychronized 更轻量级的同步锁在访问 volatile 变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此 volatile 变量是一种比 sychronized 关键字更轻量级的同步机制。volatile 适合这种场景：一个变量被多个线程共享，线程直接给这个变量赋值。 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到 CPU 缓存中。如果计算机有多个 CPU，每个线程可能在不同的 CPU 上被处理，这意味着每个线程可以拷贝到不同的 CPUcache 中。而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache这一步。 字节码使用monitorenter monitorexit 指令, synchronized /ReentrantLock 都是可重入锁, 自己可以再次获取自己的内部锁, 比如一个线程获取某个对象的锁, 这个对象的锁还没有释放, 当再次获取这个锁的时候还是可以获取的 sync.. 依赖于jvm ReentrantLock java实现(lock(),unlock() ,try/finally 实现) ReentrantLock 添加了一些高级功能, 等待可中断, 可以实现公平锁, 可以实现选择性通知 ReentrantLock 可以指定是公平锁还是非公平锁 默认非公平锁, synchronized 只能是非公平锁 性能已经不是主要选择项 ThreadPoolExecutor 饱和策略定义AbortPolicyCallerRunsPolicyDiscardPolicyDiscardOldPolicy Java 锁乐观锁乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作（比较跟上一次的版本号，如果一样则更新），如果失败则要重复读-比较-写的操作。java 中的乐观锁基本都是通过 CAS 操作实现的，CAS 是一种更新的原子操作，比较当前值跟传入值是否一样，一样则更新，否则失败。 悲观锁悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会 block 直到拿到锁。java中的悲观锁就是Synchronized,AQS框架下的锁则是先尝试cas乐观锁去获取锁，获取不到，才会转换为悲观锁，如 RetreenLock 自旋锁自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。线程自旋是需要消耗 cup 的，说白了就是让 cup 在做无用功，如果一直获取不到锁，那线程也不能一直占用 cup 自旋做无用功，所以需要设定一个自旋等待的最大时间。如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。 自旋锁的优缺点自旋锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起再唤醒的操作的消耗，这些操作会导致线程发生两次上下文切换！但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适合使用自旋锁了，因为自旋锁在获取锁前一直都是占用 cpu 做无用功，占着 XX 不 XX，同时有大量线程在竞争一个锁，会导致获取锁的时间很长，线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要 cup 的线程又不能获取到 cpu，造成 cpu 的浪费。所以这种情况下我们要关闭自旋锁]]></content>
      <tags>
        <tag>thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alg]]></title>
    <url>%2F2019%2F07%2F05%2Falg%2F</url>
    <content type="text"><![CDATA[二叉查找树二叉查找树有哪些特性呢？1， 左子树上所有的节点的值均小于或等于他的根节点的值2， 右子数上所有的节点的值均大于或等于他的根节点的值3， 左右子树也一定分别为二叉排序树 红黑树红黑树就是一种平衡的二叉查找树，说他平衡的意思是他不会变成“瘸子”，左腿特别长或者右腿特别长。除了符合二叉查找树的特性之外，还具体下列的特性： 节点是红色或者黑色 根节点是黑色 每个叶子的节点都是黑色的空节点（NULL） 每个红色节点的两个子节点都是黑色的。 从任意节点到其每个叶子的所有路径都包含相同的黑色节点。 红黑树平衡调整 变色 旋转]]></content>
  </entry>
  <entry>
    <title><![CDATA[css]]></title>
    <url>%2F2019%2F07%2F05%2Fcss%2F</url>
    <content type="text"><![CDATA[margin/paddingpadding和margin属性详解先看两个单词的释义：margin 边缘padding 衬垫，填充然后应该就能区分出这两个属性了，一个是边缘（外边距），指该控件距离父控件或其他控件的边距；另一个是填充（内边距），指该控件内部内容，如文本/图片距离该控件的边距。 Flexbox1234.div&#123; display:block; &#125; display: flex; flex-direction: column / row; 可以调换主轴：flex-direction: column 并不意味着将子元素在交叉轴上排列。而是将主轴从横向变为纵向。flex-direction 有另外两个值可以设置： row-reverse 和 column-reversejustify-content 用于控制子元素在主轴上如何对齐。其共有五个可供设置的值： flex-startflex-endcenterspace-betweenspace-around 12345.container &#123; display: flex; flex-direction: row; justify-content: flex-start;&#125; 块级元素(block)特性：总是独占一行，表现为另起一行开始，而且其后的元素也必须另起一行显示;宽度(width)、高度(height)、内边距(padding)和外边距(margin)都可控制; 内联元素(inline)特性：和相邻的内联元素在同一行;宽度(width)、高度(height)、内边距的top/bottom(padding-top/padding-bottom)和外边距的top/bottom(margin-top/margin-bottom)都不可改变，就是里面文字或图片的大小; 块级元素主要有： address , blockquote , center , dir , div , dl , fieldset , form , h1 , h2 , h3 , h4 , h5 , h6 , hr , isindex , menu , noframes , noscript , ol , p , pre , table , ul , li 内联元素主要有： a , abbr , acronym , b , bdo , big , br , cite , code , dfn , em , font , i , img , input , kbd , label , q , s , samp , select , small , span , strike , strong , sub , sup ,textarea , tt , u , var 可变元素(根据上下文关系确定该元素是块元素还是内联元素)： applet ,button ,del ,iframe , ins ,map ,object , script CSS中块级、内联元素的应用： 利用CSS我们可以摆脱上面表格里HTML标签归类的限制，自由地在不同标签/元素上应用我们需要的属性。 主要用的CSS样式有以下三个： display:block – 显示为块级元素 display:inline – 显示为内联元素 display:inline-block – 显示为内联块元素，表现为同行显示并可修改宽高内外边距等属性我们常将元素加上display:inline-block样式，原本垂直的列表就可以水平显示了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[vs-keymap]]></title>
    <url>%2F2019%2F07%2F05%2Fvs-keymap%2F</url>
    <content type="text"><![CDATA[link https://docs.microsoft.com/zh-cn/visualstudio/ide/default-keyboard-shortcuts-for-frequently-used-commands-in-visual-studio?view=vs-2019 编辑.完成单词 Alt+向右键 [文本编辑器、工作流设计器] 或 Ctrl+空格键 [文本编辑器、工作流设计器] 或 Ctrl+K、W [工作流设计器] 或 Ctrl+K、Ctrl+W [工作流设计器] 编辑.查找所有引用 Shift+F12 [全局]编辑.查找下一个 F3 [全局]编辑.查找下一个选定项 Ctrl+F3 [全局] 编辑.编排文档格式 Ctrl+K、Ctrl+D [文本编辑器]编辑.格式化选定内容 Ctrl+K、Ctrl+F [文本编辑器]]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux-shell]]></title>
    <url>%2F2019%2F07%2F03%2Flinux-shell%2F</url>
    <content type="text"><![CDATA[更改字段分隔符造成这个问题的原因是特殊的环境变量IFS，叫作内部字段分隔符（internal field separator）。IFS环境变量定义了bash shell用作字段分隔符的一系列字符。默认情况下，bash shell会将下列字符当作字段分隔符： 空格 制表符 换行符 1234IFS.OLD=$IFS IFS=$&apos;\n&apos; &lt;在代码中使用新的IFS值&gt; IFS=$IFS.OLD 1trap "echo 'Sorry ! I have trapped Ctrl-C ' " SIGINT Linux的alternatives命令替换选择软件的版本1alternatives ### 根据内存占用排序进程 ps aux --sort -rss]]></content>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-arthas]]></title>
    <url>%2F2019%2F07%2F03%2Fjava-arthas%2F</url>
    <content type="text"><![CDATA[原文连接 https://alibaba.github.io/arthas/arthas-tutorials?language=cn&amp;id=arthas-advanced 使用arthas 搜索jvm加载的lei 1sc -d `全类名` watch 12watch com.oucloud.data.service.impl.TaskServiceImpl getTaskStringFromCache &quot;&#123;params,returnObj&#125;&quot; -x 2 -bwatch com.oucloud.data.component.JobsOps schedule &quot;&#123;params,returnObj&#125;&quot; -x 2 -b]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux-故障排查]]></title>
    <url>%2F2019%2F06%2F27%2Flinux-%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[找到占用CPU最高的线程 通常的做法是：➊ 在命令行输入top，然后shift+p查看占用CPU最高的进程，记下进程号➋ 在命令行输入top -Hp 进程号，查看占用CPU最高的线程➌ 使用printf 0x%x 线程号，得到其16进制线程号➍ 使用jstack 进程号得到java执行栈，然后grep16进制找到相应的信息 这一行Shell的意思是，找到使用CPU最高的进程之使用CPU最高的线程的16进制号。 1ps -eo %cpu,pid |sort -n -k1 -r | head -n 1 | awk &apos;&#123;print $2&#125;&apos; |xargs top -b -n1 -Hp | grep COMMAND -A1 | tail -n 1 | awk &apos;&#123;print $1&#125;&apos; | xargs printf 0x%x 先记住这些判断准则，我们在示例中再聊：➊ 如果load超过了cpu核数，则负载过高➋ 如果wa过高，可初步判断I/O有问题➌ sy,si,hi,st，任何一个超过5%，都有问题➍ 进程状态长时处于D、Z、T状态，提高注意度➎ cpu不均衡，判断亲和性和优先级问题 CPU过高是表象。除了系统确实负载已经到了极限，其他的，都是由其他原因引起的，比如I/O；比如设备。这些我们放在其他章节进行讨论。 GC引起的CPU过高接着我们最开始的例子来。通过查看jstack找到相应的16进制进程，结果发现是GC线程。 “VM Thread” prio=10 tid=0x00007f06d8089000 nid=0x58c7 runnable “GC task thread#0 (ParallelGC)” prio=10 tid=0x00007f06d801b800 nid=0x58d7 runnable这种情况，一般都是JVM内存不够用了，疯狂GC，可能是socket/线程忘了关闭了，也可能是大对象没有回收。这种情况只能通过重启来解决了，记得重启之前，使用jmap dump一下堆栈哦。当然，你可能会得到jdk版本的问题。 st%占比过高st过高一般是物理CPU资源不足所致，也就是只发生在虚拟机上。如果你买的虚拟机st一直很高，那你的服务提供商可能在超卖，挤占你的资源。不信双11的时候看下你的虚拟机？ 网卡导致单cpu过高业务方几台kafka，cpu使用处于正常水平，才10%左右，但有一核cpu，负载特别的高，si奇高。 mpstat -I SUM -P ALL 查看cpu使用情况，cpu0的中断确实比较多。 20:15:18 CPU intr/s20:15:23 all 34234.2020:15:23 0 9566.2020:15:23 1 0.00网卡需要cpu服务时，都会抛出一个中断，中断告诉cpu发生了什么事情，cpu就要停止目前的工作来处理这个中断。其实，默认所有的中断处理都集中在cpu0 上，导致服务器负载过高。cpu0 成了瓶颈，而其他cpu却还闲着。➊ 解决方式1：使用CPU亲和性功能，kafka略过网卡所使用的CPU➋ 解决方式2: 更换网卡➌ 通常修改的方式还是有些复杂了，比如，修改 /proc/irq/{seq}/smp_affinity 我们可以直接安装irqbalance，然后执行就可以了。 yum install irqbalance -yservice irqbalance startcpu使用率低，但负载高cpu id%高，也就是空闲，比如90%。但 load average非常高，比如4核达到10。 分析：load average高，说明其任务已经排队，许多任务正在等待。出现此种情况，可能存在大量不可中断的进程。 使用top或者ps可以看到进程相应的状态。 https://mp.weixin.qq.com/s/WTva_bvkIn7uTCxv0m2RiA which woman love man 排查内存的一些命令内存分两部分，物理内存和swap。物理内存问题主要是内存泄漏，而swap的问题主要是用了swap～，我们先上一点命令。 物理内存 根据使用量排序查看REStop -&gt; shift + m 查看进程使用的物理内存ps -p 75 -o rss,vsz 显示内存的使用情况free -h 使用sar查看内存信息sar -r 显示内存每个区的详情cat /proc/meminfo 查看slab区使用情况slabtop通常，通过查看物理内存的占用，你发现不了多少问题，顶多发现那个进程占用内存高（比如vim等旁路应用）。meminfo和slabtop对系统的全局判断帮助很大，但掌握这两点坡度陡峭。 swap 查看si,so是否异常vmstat 1 使用sar查看swapsar -W 禁用swapswapoff 查询swap优先级sysctl -q vm.swappiness 设置swap优先级sysctl vm.swappiness=10建议关注非0 swap的所有问题，即使你用了ssd。swap用的多，通常伴随着I/O升高，服务卡顿。swap一点都不好玩，不信搜一下《swap罪与罚》这篇文章看下，千万不要更晕哦。 JVM 查看系统级别的故障和问题dmesg 统计实例最多的类前十位jmap -histo pid | sort -n -r -k 2 | head -10 统计容量前十的类jmap -histo pid | sort -n -r -k 3 | head -10以上命令是看堆内的，能够找到一些滥用集合的问题。堆外内存，依然推荐《Java堆外内存排查小结》 其他 释放内存echo 3 &gt; /proc/sys/vm/drop_caches 查看进程物理内存分布pmap -x 75 | sort -n -k3 dump内存内容gdb –batch –pid 75 -ex “dump memory a.dump 0x7f2bceda1000 0x7f2bcef2b000” 内存模型二王的问题表象都是CPU问题，CPU都间歇性的增高，那是因为Linux的内存管理机制引起的。你去监控Linux的内存使用率，大概率是没什么用的。因为经过一段时间，剩余的内存都会被各种缓存迅速占满。一个比较典型的例子是ElasticSearch，分一半内存给JVM，剩下的一半会迅速被Lucene索引占满。 如果你的App进程启动后，经过两层缓冲后还不能落地，迎接它的，将会是oom killer。 接下来的知识有些烧脑，但有些名词，可能是你已经听过多次的了。 操作系统视角 我们来解释一下上图，第一部分是逻辑内存和物理内存的关系；第二部分是top命令展示的一个结果，详细的列出了每一个进程的内存使用情况；第三部分是free命令展示的结果，它的关系比较乱，所以给加上了箭头来作说明。 学过计算机组成结构的都知道，程序编译后的地址是逻辑内存，需要经过翻译才能映射到物理内存。这个管翻译的硬件，就叫MMU；TLB就是存放这些映射的小缓存。内存特别大的时候，会涉及到hugepage，在某些时候，是进行性能优化的杀手锏，比如优化redis (THP，注意理解透彻前不要妄动) 物理内存的可用空间是有限的，所以逻辑内存映射一部分地址到硬盘上，以便获取更大的物理内存地址，这就是swap分区。swap是很多性能场景的万恶之源，建议禁用 像top展示的字段，RES才是真正的物理内存占用（不包括swap，ps命令里叫RSS)。在java中，代表了堆内+堆外内存的总和。而VIRT、SHR等，几乎没有判断价值(某些场景除外) 系统的可用内存，包括：free + buffers + cached，因为后两者可以自动释放。但不要迷信，有很大一部分，你是释放不了的 slab区，是内核的缓存文件句柄等信息等的特殊区域，slabtop命令可以看到具体使用 更详细的，从/proc/meminfo文件中可以看到具体的逻辑内存块的大小。有多达40项的内存信息，这些信息都可以通过/proc一些文件的遍历获取，本文只挑重点说明。 [xjj@localhost ~]$ cat /proc/meminfoMemTotal: 3881692 kBMemFree: 249248 kBMemAvailable: 1510048 kBBuffers: 92384 kBCached: 1340716 kB40+ more …oom-killer以下问题已经不止一个小伙伴问了：我的java进程没了，什么都没留下，就像个屁一样蒸发不见了why？是因为对象太多了么？执行dmesg命令，大概率会看到你的进程崩溃信息躺尸在那里。 为了能看到发生的时间，我们习惯性加上参数T dmesg -T由于linux系统采用的是虚拟内存，进程的代码，库，堆和栈的使用都会消耗内存，但是申请出来的内存，只要没真正access过，是不算的，因为没有真正为之分配物理页面。 第一层防护墙就是swap；当swap也用的差不多了，会尝试释放cache；当这两者资源都耗尽，杀手就出现了。oom killer会在系统内存耗尽的情况下跳出来，选择性的干掉一些进程以求释放一点内存。2.4内核杀新进程；2.6杀用的最多的那个。所以，买内存吧。 这个oom和jvm的oom可不是一个概念。顺便，瞧一下我们的JVM堆在什么位置。 例子jvm内存溢出排查应用程序发布后，jvm持续增长。使用jstat命令，可以看到old区一直在增长。 jstat -gcutil 28266 1000在jvm参数中，加入-XX:+HeapDumpOnOutOfMemoryError，在jvm oom的时候，生成hprof快照。然后，使用Jprofile、VisualVM、Mat等打开dump文件进行分析。 你要是个急性子，可以使用jmap立马dump一份 jmap -heap:format=b pid最终发现，有一个全局的Cache对象，不是guava的，也不是commons包的，是一个简单的ConcurrentHashMap，结果越积累越多，最终导致溢出。 溢出的情况也有多种区别，这里总结如下： 关键字 原因Java.lang.OutOfMemoryError: Java heap space 堆内存不够了，或者存在内存溢出java.lang.OutOfMemoryError: PermGen space Perm区不够了，可能使用了大量动态加载的类，比如cglibjava.lang.OutOfMemoryError: Direct buffer memory 堆外内存、操作系统没内存了，比较严重的情况java.lang.StackOverflowError 调用或者递归层次太深，修正即可java.lang.OutOfMemoryError: unable to create new native thread 无法创建线程，操作系统内存没有了，一定要预留一部分给操作系统，不要都给jvmjava.lang.OutOfMemoryError: Out of swap space 同样没有内存资源了，swap都用光了jvm程序内存问题，除了真正的内存泄漏，大多数都是由于太贪心引起的。一个4GB的内存，有同学就把jvm设置成了3840M，只给操作系统256M，不死才怪。 另外一个问题就是swap了，当你的应用真正的高并发了，swap绝对能让你体验到它魔鬼性的一面：进程倒是死不了了，但GC时间长的无法忍受。 我的ES性能低业务方的ES集群宿主机是32GB的内存，随着数据量和访问量增加，决定对其进行扩容=&gt;内存改成了64GB。 内存升级后，发现ES的性能没什么变化，某些时候，反而更低了。 通过查看配置，发现有两个问题引起。一、64GB的机器分配给jvm的有60G，预留给文件缓存的只有4GB，造成了文件缓存和硬盘的频繁交换，比较低效。二、JVM大小超过了32GB，内存对象的指针无法启用压缩，造成了大量的内存浪费。由于ES的对象特别多，所以留给真正缓存对象内容的内存反而减少了。 解决方式：给jvm的内存30GB即可。 其他基本上了解了内存模型，上手几次内存溢出排查，内存问题就算掌握了。但还有更多，这条知识系统可以深挖下去。 JMM还是拿java来说。java中有一个经典的内存模型，一般面试到volitile关键字的时候，都会问到。其根本原因，就是由于线程引起的。当两个线程同时访问一个变量的时候，就需要加所谓的锁了。由于锁有读写，所以java的同步方式非常多样。wait,notify、lock、cas、volitile、synchronized等，我们仅放上volitile的读可见性图作下示例。 线程对共享变量会拷贝一份到工作区。线程1修改了变量以后，其他线程读这个变量的时候，都从主存里刷新一份，此所谓读可见。 JMM问题是纯粹的内存问题，也是高级java必备的知识点。 CacheLine &amp; False Sharing是的，内存的工艺制造还是跟不上CPU的速度，于是聪明的硬件工程师们，就又给加了一个缓存（哦不，是多个）。而Cache Line为CPU Cache中的最小缓存单位。 这个缓存是每个核的，而且大小固定。如果存在这样的场景，有多个线程操作不同的成员变量，但是相同的缓存行，这个时候会发生什么？。没错，伪共享（False Sharing）问题就发生了！ 伪共享也是高级java的必备技能（虽然几乎用不到），赶紧去探索吧。 HugePage回头看我们最长的那副图，上面有一个TLB，这个东西速度虽然高，但容量也是有限的。当访问频繁的时候，它会成为瓶颈。TLB是存放Virtual Address和Physical Address的映射的。如图，把映射阔上一些，甚至阔上几百上千倍，TLB就能容纳更多地址了。像这种将Page Size加大的技术就是Huge Page。 HugePage有一些副作用，比如竞争加剧（比如redis： https://redis.io/topics/latency ）。但在大内存的现代，开启后会一定程度上增加性能（比如oracle： https://docs.oracle.com/cd/E11882_01/server.112/e10839/appi_vlm.htm )。 Numa本来想将Numa放在cpu篇，结果发现numa改的其实是内存控制器。这个东西，将内存分段，分别”绑定”在不同的CPU上。也就是说，你的某核CPU，访问一部分内存速度贼快，但访问另外一些内存，就慢一些。 所以，Linux识别到NUMA架构后，默认的内存分配方案就是：优先尝试在请求线程当前所处的CPU的内存上分配空间。如果绑定的内存不足，先去释放绑定的内存。 以下命令可以看到当前是否是NUMA架构的硬件。 numactl –hardwareNUMA也是由于内存速度跟不上给加的折衷方案。Swap一些难搞的问题，大多是由于NUMA引起的。 分析工具 cpu| 工具 | 描述 || ——– | —————————— || uptime/w | 查看服务器运行时间、平均负载 || top | 监控每个进程的CPU用量分解 || vmstat | 系统的CPU平均负载情况 || mpstat | 查看多核CPU信息 || sar -u | 查看CPU过去或未来时点CPU利用率 || pidstat | 查看每个进程的用量分解 | 内存| 工具 | 描述 || ——- | —————————— || free | 查看内存的使用情况 || top | 监控每个进程的内存使用情况 || vmstat | 虚拟内存统计信息 || sar -r | 查看内存 || sar | 查看CPU过去或未来时点CPU利用率 || pidstat | 查看每个进程的内存使用情况 | 磁盘| 工具 | 描述 || ——- | —————————- || iostat | 磁盘详细统计信息 || iotop | 按进程查看磁盘IO统计信息 || pidstat | 查看每个进程的磁盘IO使用情况 | 网络| ——– | —————————- || ping | 测试网络的连通性 || netstat | 检验本机各端口的网络连接情况 || hostname | 查看主机和域名 | 统计机器中网络连接各个状态个数 netstat -an | ``awk &#39;/^tcp/ {++S[$NF]} END {for (a in S) print a,S[a]} &#39;]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-apps]]></title>
    <url>%2F2019%2F06%2F26%2Fdocker-apps%2F</url>
    <content type="text"><![CDATA[docker 安装 nuxus12$ mkdir -p /lib/nexus/nexus-data &amp;&amp; chown -R 200 /lib/nexus/nexus-data$ docker run -d -p 8081:8081 --name nexus -v /lib/nexus/nexus-data:/nexus-data sonatype/nexus3 docker-mysql step1docker pull mysql/mysql-server:latest step2docker run –name foo-mysql -e MYSQL_ROOT_PASSWORD=passwd -e MYSQL_ROOT_HOST=% -v /etc/mysql/my.conf:/etc/mysql/my.conf -p 3306:3306 -d mysql:latest step3 12docker exec -it foo-mysql mysql -u root -p ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;&lt;password&gt;&apos;; docker rabbitmq1docker run -d --hostname some-rabbit --name some-rabbit --network some-network -e RABBITMQ_ERLANG_COOKIE='123456' rabbitmq:3]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux-tcp]]></title>
    <url>%2F2019%2F06%2F25%2Flinux-tcp%2F</url>
    <content type="text"><![CDATA[linux tcp 调优端口范围 0-1024 端口受保护 当前主机可以使用的端口范围12[root@localhost logstash]# cat /proc/sys/net/ipv4/ip_local_port_range 32768 60999 大量并发请求时 大量请求等待建立连接 如果TIME_WAIT 很多的话 设置更小的timeout 快速释放请求12[root@localhost logstash]# cat /proc/sys/net/ipv4/tcp_fin_timeout 60 设置time_wait连接重用1echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_reuse 设置time_wait 快速回收1echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_recycle 以上设置都是临时性的123cat /proc/net/netstat # tcp 统计信息cat /proc/net/snmp # 系统连接情况netstat -s # 网络统计信息 linux net 相关 netstat属于net-tools工具集，而ss属于iproute。其命令对应如下，是时候和net-tools说Bye了。 用途 net-tools iproute统计 ifconfig ss地址 netstat ip addr路由 route ip route邻居 arp ip neighVPN iptunnel ip tunnelVLAN vconfig ip link组播 ipmaddr ip maddr 查看系统正在监听的tcp连接 ss -atrss -atn #仅ip 查看系统中所有连接 ss -alt 查看监听444端口的进程pid ss -ltp | grep 444 查看进程555占用了哪些端口 ss -ltp | grep 555 显示所有udp连接 ss -u -a查看TCP sockets，使用-ta选项查看UDP sockets，使用-ua选项查看RAW sockets，使用-wa选项查看UNIX sockets，使用-xa选项 显示所有的http连接ss dport = :http 查看连接本机最多的前10个ip地址netstat -antp | awk ‘{print $4}’ | cut -d ‘:’ -f1 | sort | uniq -c | sort -n -k1 -r | head -n 10 sysctl命令可以设置这些参数，如果想要重启生效的话，加入/etc/sysctl.conf文件中。 12345678# 修改阈值net.ipv4.tcp_max_tw_buckets = 50000 # 表示开启TCP连接中TIME-WAIT sockets的快速回收net.ipv4.tcp_tw_reuse = 1#启用timewait 快速回收。这个一定要开启，默认是关闭的。net.ipv4.tcp_tw_recycle= 1 # 修改系統默认的TIMEOUT时间,默认是60snet.ipv4.tcp_fin_timeout = 10 tcp 状态TCP(Transmission Control Protocol)传输控制协议 TCP是主机对主机层的传输控制协议，提供可靠的连接服务，采用三次握手确认建立一个连接： 位码即tcp标志位，有6种标示：SYN(synchronous建立联机) ACK(acknowledgement 确认) PSH(push传送) FIN(finish结束) RST(reset重置) URG(urgent紧急)Sequence number(顺序号码) Acknowledge number(确认号码) 第一次握手：主机A发送位码为syn＝1，随机产生seq number=1234567的数据包到服务器，主机B由SYN=1知道，A要求建立联机； 第二次握手：主机B收到请求后要确认联机信息，向A发送ack number=(主机A的seq+1)，syn=1，ack=1，随机产生seq=7654321的包； 第三次握手：主机A收到后检查ack number是否正确，即第一次发送的seq number+1，以及位码ack是否为1，若正确，主机A会再发送ack number=(主机B的seq+1)，ack=1，主机B收到后确认seq值与ack=1则连接建立成功。 FTP协议及时基于此协议 三次握手四次挥手 状态解释1234567891011LISTEN：侦听来自远方的TCP端口的连接请求SYN-SENT：再发送连接请求后等待匹配的连接请求（如果有大量这样的状态包，检查是否中招了）SYN-RECEIVED：再收到和发送一个连接请求后等待对方对连接请求的确认（如有大量此状态估计被flood攻击了）ESTABLISHED：代表一个打开的连接FIN-WAIT-1：等待远程TCP连接中断请求，或先前的连接中断请求的确认FIN-WAIT-2：从远程TCP等待连接中断请求CLOSE-WAIT：等待从本地用户发来的连接中断请求CLOSING：等待远程TCP对连接中断的确认LAST-ACK：等待原来的发向远程TCP的连接中断请求的确认（不是什么好东西，此项出现，检查是否被攻击）TIME-WAIT：等待足够的时间以确保远程TCP接收到连接中断请求的确认CLOSED：没有任何连接状态]]></content>
  </entry>
  <entry>
    <title><![CDATA[java-stream]]></title>
    <url>%2F2019%2F06%2F19%2Fjava-stream%2F</url>
    <content type="text"><![CDATA[Java Streamhttps://github.com/CarpenterLee/JavaLambdaInternals Java 8https://github.com/winterbe/java8-tutorial]]></content>
  </entry>
  <entry>
    <title><![CDATA[cargo]]></title>
    <url>%2F2019%2F05%2F27%2Fcargo%2F</url>
    <content type="text"><![CDATA[Cargo 代理配置conf12345[source.crates-io]registry = &quot;https://github.com/rust-lang/crates.io-index&quot;replace-with = &apos;ustc&apos;[source.ustc]registry = &quot;git://mirrors.ustc.edu.cn/crates.io-index&quot;]]></content>
  </entry>
  <entry>
    <title><![CDATA[nuget]]></title>
    <url>%2F2019%2F05%2F27%2Fnuget%2F</url>
    <content type="text"><![CDATA[本地仓库指定路径%AppData%\NuGet\nuget.config1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;configuration&gt; &lt;packageSources&gt; &lt;add key="nuget.org" value="https://api.nuget.org/v3/index.json" protocolVersion="3" /&gt; &lt;!-- &lt;add key="repo" value="http://192.168.2.85/repository/nuget.org-proxy/" /&gt; --&gt; &lt;!-- &lt;add key="cps" value="D:\vcpkg\scripts\buildsystems" /&gt; --&gt; &lt;add key="Microsoft Visual Studio Offline Packages" value="C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\" /&gt; &lt;/packageSources&gt; &lt;disabledPackageSources&gt; &lt;add key="repo" value="true" /&gt; &lt;/disabledPackageSources&gt; &lt;packageRestore&gt; &lt;add key="enabled" value="True" /&gt; &lt;add key="automatic" value="True" /&gt; &lt;/packageRestore&gt; &lt;bindingRedirects&gt; &lt;add key="skip" value="False" /&gt; &lt;/bindingRedirects&gt; &lt;packageManagement&gt; &lt;add key="format" value="0" /&gt; &lt;add key="disabled" value="False" /&gt; &lt;/packageManagement&gt; &lt;config&gt; &lt;add key="repositoryPath" value="E:\nugetpackage\.nuget\packages" /&gt; &lt;add key="globalPackagesFolder" value="E:\nugetpackage\.nuget\packages" /&gt; &lt;/config&gt;&lt;/configuration&gt; 添加1&lt;add key="globalPackagesFolder" value="E:\nugetpackage\.nuget\packages" /&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[pgsql]]></title>
    <url>%2F2019%2F05%2F23%2Fpgsql%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[sql2elasticsearch]]></title>
    <url>%2F2019%2F04%2F26%2Fsql2elasticsearch%2F</url>
    <content type="text"><![CDATA[导出数据elasticsearchlogstash config 12345678910111213141516171819202122232425[root@localhost ~]# cat /srv/docker-elk/logstash/pipeline/sqlserver.conf input &#123; jdbc &#123; jdbc_driver_library =&gt; &quot;/var/mssql-jdbc-7.2.2.jre8.jar&quot; jdbc_driver_class =&gt; &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot; jdbc_connection_string =&gt; &quot;jdbc:sqlserver://host:1433;databaseName=db&quot; jdbc_user =&gt; &quot;a&quot; jdbc_password =&gt; &quot;a&quot; schedule =&gt; &quot;* * * * *&quot; jdbc_default_timezone =&gt; &quot;Asia/Shanghai&quot; statement =&gt; &quot;SELECT * FROM table WHERE id &gt; :sql_last_value&quot; use_column_value =&gt; true tracking_column =&gt; &quot;id&quot; type =&gt; &quot;task_table&quot; &#125;&#125;output &#123; if [type] == &quot;task_table&quot; &#123; elasticsearch &#123; index =&gt; &quot;tasks&quot; hosts =&gt; [&quot;elasticsearch:9200&quot;] &#125; &#125;&#125; 其中 sql_last_value 计算查询条件的值]]></content>
  </entry>
  <entry>
    <title><![CDATA[how-to-publish-jar-lib]]></title>
    <url>%2F2019%2F01%2F21%2Fhow-to-publish-jar-lib%2F</url>
    <content type="text"><![CDATA[使用gradle 发布 java 类库 使用 maven 插件 1apply plugin: &apos;maven&apos; 上传nexus地址 123456789101112uploadArchives &#123; repositories &#123; mavenDeployer &#123; repository(url: "http://&#123;ip:port&#125;/repository/maven-snapshots/") &#123; authentication(userName: "user", password: "password") &#125; pom.version = version pom.artifactId = project.name pom.groupId = 'com.ziggle.test' &#125; &#125;&#125; 运行 gradle uploadArchives 注意jar包的命名影响上传的仓库]]></content>
  </entry>
  <entry>
    <title><![CDATA[how-to-enable-spring-security]]></title>
    <url>%2F2019%2F01%2F21%2Fhow-to-enable-spring-security%2F</url>
    <content type="text"><![CDATA[启用spring security 需要一个AuthenticationProvider123456789101112131415161718192021222324252627282930313233343536373839@Servicepublic class CustomerAuthenticationManager implements AuthenticationProvider &#123; private final SysUserDetailService sysUserDetailService; public CustomerAuthenticationManager(SysUserDetailService sysUserDetailService) &#123; this.sysUserDetailService = sysUserDetailService; &#125; @Override public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; // 获取认证的用户名 &amp; 密码 String name = authentication.getName(); String password = authentication.getCredentials().toString(); // 认证逻辑 SysUserDetail userDetails = (SysUserDetail) sysUserDetailService.loadUserByUsername(name); String pwd = DigestUtils.md5DigestAsHex(password.getBytes()); if (userDetails != null) &#123; // 密码MD5 加密 or BCryptPasswordEncoder &lt;--- use this one// if (encoder.matches(password, userDetails.getPassword())) &#123; if (pwd.equalsIgnoreCase(userDetails.getPassword())) &#123; // 这里设置权限和角色 Collection&lt;? extends GrantedAuthority&gt; authorities = userDetails.getAuthorities(); // 生成令牌 这里令牌里面存入了:name,password,authorities, 当然你也可以放其他内容 Authentication auth = new UsernamePasswordAuthenticationToken(name, password, authorities); return auth; &#125; else &#123; throw new BadCredentialsException("密码错误"); &#125; &#125; else &#123; throw new UsernameNotFoundException("用户不存在"); &#125; &#125; @Override public boolean supports(Class&lt;?&gt; auth) &#123; return auth.equals(UsernamePasswordAuthenticationToken.class); &#125;&#125; 需要获取用户信息的service 示例方法从数据库中获取用户信息 123456789101112131415161718192021222324252627282930313233@Servicepublic class SysUserDetailService implements UserDetailsService &#123; private SysUserMapper sysUserMapper; private SysUserRoleMapper sysUserRoleMapper; public SysUserDetailService(SysUserMapper sysUserMapper, SysUserRoleMapper sysUserRoleMapper) &#123; this.sysUserMapper = sysUserMapper; this.sysUserRoleMapper = sysUserRoleMapper; &#125; @Override public UserDetails loadUserByUsername(String s) throws UsernameNotFoundException &#123; SysUserDetail sysUserDetail = sysUserMapper.getSysUserDetail(s); if (sysUserDetail == null) &#123; throw new AuthenticationCredentialsNotFoundException("账号不存在"); &#125; List&lt;Role&gt; userRole = sysUserRoleMapper.getUserRole(sysUserDetail.getId()); List&lt;SimpleGrantedAuthority&gt; authorities = getAuthorities(userRole); sysUserDetail.setAuthorities(authorities); return sysUserDetail; &#125; private List&lt;SimpleGrantedAuthority&gt; getAuthorities(List&lt;Role&gt; roles) &#123; return roles .stream() .map(role -&gt; new SimpleGrantedAuthority(role.getRole())) .collect(Collectors.toList()); &#125;&#125; 一些帮助类 SysUserDetail 这个类继承自 org.springframework.security.core.userdetails.UserDetails 12345678910111213141516171819202122public class SysUserDetail implements UserDetails &#123; private static final long serialVersionUID = -2184425668041155384L; private Long id; private String username; @JsonIgnore private String password; private boolean disabled; private String token; private Collection&lt;? extends GrantedAuthority&gt; authorities; public SysUserDetail() &#123; &#125; public SysUserDetail(Long id, String username, String password, boolean disabled, Collection&lt;? extends GrantedAuthority&gt; authorities) &#123; this.id = id; this.username = username; this.password = password; this.disabled = disabled; this.authorities = authorities; &#125; ...... ignore getter setter hashcode. 基于jwt的帮助类 12345678910111213public interface IJwtTokenDecoder &#123; SysUserDetail getCurrentUserFromToken(String var1);&#125;/** * 生成token */String generateToken(SysUserDetail sysUserDetail);/** * 解析token */SysUserDetail getCurrentUserFromToken(String token) 添加过滤器对请求进行拦截1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class SecurityConfiguration &#123; private static final String[] AUTH_WHITE_LIST = &#123; // -- swagger ui "/swagger-resources/**", "/v2/api-docs", "/swagger-ui.html", "/**/*.css", "/**/*.js", "/**/*.png", "/**/*.jpg", "/webjars/**", "/druid/**", &#125;; @Bean @ConditionalOnClass(EnableWebSecurity.class) public WebSecurityConfigurerAdapter webSecurityConfigurerAdapter(AuthenticationProvider authenticationProvider, ZiggleStatProperties statProperties, IJwtTokenDecoder decoder ) &#123; return new WebSecurityConfigurerAdapter() &#123; @Bean(BeanIds.AUTHENTICATION_MANAGER) @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; //自定义身份验证组件 auth.authenticationProvider(authenticationProvider); super.configure(auth); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .cors() .and().csrf().disable() .sessionManagement() .sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() .authorizeRequests() .antMatchers(HttpMethod.POST, statProperties.getRoutingPrefixV1() + "/login").permitAll() .antMatchers(HttpMethod.POST, statProperties.getRoutingPrefixV1() + "/user/").permitAll() .antMatchers(AUTH_WHITE_LIST).permitAll() .anyRequest().authenticated() .and() .addFilterBefore(new JwtExceptionHandlerFilter(), ChannelProcessingFilter.class) .addFilterBefore(new JwtAuthenticationFilter(authenticationManager(), decoder), UsernamePasswordAuthenticationFilter.class) ; &#125; &#125;; &#125;&#125; 添加登陆授权接口1// ignore]]></content>
  </entry>
  <entry>
    <title><![CDATA[how-to-create-spring-boot-starter]]></title>
    <url>%2F2019%2F01%2F21%2Fhow-to-create-spring-boot-starter%2F</url>
    <content type="text"><![CDATA[创建spring.factoriesresources/META-INF/spring.factories12org.springframework.boot.autoconfigure.EnableAutoConfiguration=\com.oucloud.web.starter.ZiggleWebAutoConfigure 自动配置入口123456@Configuration@ConditionalOnClass(EnableStarterConfig.class)@EnableConfigurationProperties(&#123;ZiggleStatProperties.class&#125;)@Import(&#123;JacksonConfiguration.class, SecurityConfiguration.class&#125;)public class ZiggleWebAutoConfigure &#123;&#125; @Import 导入配置类 有EnableStartConfig.class 条件导入 自动配置注解1234567891011@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@EnableWebSecurity@AutoConfigureBefore(SecurityAutoConfiguration.class)@EnableGlobalMethodSecurity(prePostEnabled = true)@AutoConfigurationPackage@Import(&#123;ZiggleConfigurationSelector.class&#125;)public @interface EnableStarterConfig &#123;&#125; 使用这个注解后会启用EnableWebSecurity 最重要的是 @Import({ZiggleConfigurationSelector.class}) 导入配置类 配置类selector12345678public class ZiggleConfigurationSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; return new String[]&#123; JacksonConfiguration.class.getName(), SecurityConfiguration.class.getName()&#125;; &#125;&#125; 具体配置类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class SecurityConfiguration &#123; private static final String[] AUTH_WHITE_LIST = &#123; // -- swagger ui "/swagger-resources/**", "/v2/api-docs", "/swagger-ui.html", "/**/*.css", "/**/*.js", "/**/*.png", "/**/*.jpg", "/webjars/**", "/druid/**" &#125;; @Bean @ConditionalOnClass(EnableWebSecurity.class) public WebSecurityConfigurerAdapter webSecurityConfigurerAdapter(AuthenticationProvider authenticationProvider, ZiggleStatProperties statProperties, IJwtTokenDecoder decoder ) &#123; return new WebSecurityConfigurerAdapter() &#123; @Bean(BeanIds.AUTHENTICATION_MANAGER) @Override public AuthenticationManager authenticationManagerBean() throws Exception &#123; return super.authenticationManagerBean(); &#125; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; //自定义身份验证组件 auth.authenticationProvider(authenticationProvider); super.configure(auth); &#125; @Override protected void configure(HttpSecurity http) throws Exception &#123; http .cors() .and().csrf().disable() .sessionManagement() .sessionCreationPolicy(SessionCreationPolicy.STATELESS) .and() .authorizeRequests() .antMatchers(HttpMethod.POST, statProperties.getRoutingPrefixV1() + "/login").permitAll() .antMatchers(HttpMethod.POST, statProperties.getRoutingPrefixV1() + "/user/").permitAll() .antMatchers(AUTH_WHITE_LIST).permitAll() .anyRequest().authenticated() .and() .addFilterBefore(new JwtExceptionHandlerFilter(), ChannelProcessingFilter.class) .addFilterBefore(new JwtAuthenticationFilter(authenticationManager(), decoder), UsernamePasswordAuthenticationFilter.class) ; &#125; &#125;; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[sqlserver]]></title>
    <url>%2F2018%2F12%2F14%2Fsqlserver%2F</url>
    <content type="text"><![CDATA[coursor 使用123456789101112131415161718192021222324252627282930PRINT OBJECT_ID('[dbo].[PROC_checkPlan]');DECLARE @name VARCHAR(50) -- database name DECLARE @path VARCHAR(256) -- path for backup files DECLARE @fileName VARCHAR(256) -- filename for backup DECLARE @fileDate VARCHAR(20) -- used for file name SET @path = 'C:\sqlbackup\' SELECT @fileDate = CONVERT(VARCHAR(20),GETDATE(),112) DECLARE db_cursor CURSOR FOR SELECT name FROM MASTER.dbo.sysdatabases WHERE name NOT IN ('master','model','msdb','tempdb','backup') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN SET @fileName = @path + @name + '_' + @fileDate + '.BAK' BACKUP DATABASE @name TO DISK = @fileName FETCH NEXT FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor sqlserver 添加执行权限到用户/ CREATE A NEW ROLE /CREATE ROLE db_executor / GRANT EXECUTE TO THE ROLE /GRANT EXECUTE TO db_executor 修改表结构 / 添加默认值123456789update data_terminal set message_center_password = '' where message_center_password is null alter table data_terminalalter column [message_center_password] varchar(255) not null ALTER TABLE data_terminalADD CONSTRAINT default_terminal DEFAULT '' FOR [message_center_password]; Deleting all duplicate rows but keeping one123456WITH cte AS ( SELECT[foo], [bar], row_number() OVER(PARTITION BY foo, bar ORDER BY baz) AS [rn] FROM TABLE)DELETE cte WHERE [rn] &gt; 1 查看sqlserver 索引碎片情况12345678910SELECT OBJECT_NAME(ind.OBJECT_ID) AS TableName, ind.name AS IndexName, indexstats.index_type_desc AS IndexType, indexstats.avg_fragmentation_in_percent ,'alter index ' + ind.name + ' on ' + OBJECT_NAME(ind.OBJECT_ID)+ ' rebuild' as sqlFROM sys.dm_db_index_physical_stats(DB_ID(), NULL, NULL, NULL, NULL) indexstats INNER JOIN sys.indexes ind ON ind.object_id = indexstats.object_id AND ind.index_id = indexstats.index_id WHERE indexstats.avg_fragmentation_in_percent &gt; 50 ORDER BY indexstats.avg_fragmentation_in_percent DESC 重建索引的方法31ALTER INDEX [idx_name] ON [dbo].[table_name] REBUILD dbccDBCC(database console commands)如果表的当前标识值小于列中存储的最大标识值，则使用标识列中的最大值对其进行重置。1DBCC CHECKIDENT('[dbo].[data_module]', NORESEED) 重置identity种子1234567891011121314151617create table #table_temp (item varchar(111))insert into #table_temp (item)select name from sysobjects where type = 'u'declare @count int select @count= count(1) from #table_tempdeclare @i varchar(111)while @count&gt;0begin select @count= count(1) from #table_temp select top(1)@i= item from #table_temp DBCC CHECKIDENT( @i, RESEED) -- DBCC CHECKIDENT ( table_name, RESEED, new_reseed_value ) -- DBCC CHECKIDENT ( table_name, RESEED ) -- DBCC CHECKIDENT ( table_name, NORESEED) delete from #table_temp where item = @iend 统计数据库表行数1234567SELECT t.name, s.row_count from sys.tables tJOIN sys.dm_db_partition_stats sON t.object_id = s.object_idAND t.type_desc = 'USER_TABLE'AND t.name not like '%dss%'AND s.index_id IN (0,1)order by row_count desc 批量插入数据时 获取新主键列表1234567891011121314151617181920CREATE TABLE MyTable( MyPK INT IDENTITY(1,1) NOT NULL, MyColumn NVARCHAR(1000))DECLARE @myNewPKTable TABLE (myNewPK INT,myColumn varchar(1111))INSERT INTO MyTable( MyColumn)OUTPUT INSERTED.MyPK ,inserted.MyColumn INTO @myNewPKTable(myNewPK ,myColumn)SELECT sysobjects.nameFROM sysobjectsSELECT * FROM @myNewPKTable 批量插入数据 并获取插入后表数据12345678910111213begin transaction; -- 新增联系人 insert wx_contact( big_head , * ) output inserted.wx_contact_id, inserted.wx_username into @TVP(contactId ,wxusername) select t.bighead,* from @TVP t left join wx_contact c on t.wxusername =c.wx_username where c.wx_username is null and t.userState = 0commit; sqlserver 监控视图 dmdb*：数据库和数据库对象 dmexec*：执行用户代码和关联的连接 dmos*：内存、锁定和时间安排 dmtran*：事务和隔离 dmio*：网络和磁盘的输入/输出 显示缓冲计划所占用的cpu使用率 12345678910111213141516171819SELECT total_cpu_time, total_execution_count, number_of_statements, s2.textFROM (SELECT TOP 50 SUM(qs.total_worker_time) AS total_cpu_time, SUM(qs.execution_count) AS total_execution_count, COUNT(*) AS number_of_statements, qs.sql_handle --, --MIN(statement_start_offset) AS statement_start_offset, --MAX(statement_end_offset) AS statement_end_offset FROM sys.dm_exec_query_stats AS qs GROUP BY qs.sql_handle ORDER BY SUM(qs.total_worker_time) DESC) AS stats CROSS APPLY sys.dm_exec_sql_text(stats.sql_handle) AS s2 order by total_execution_count desc sql join/where 执行顺序https://docs.microsoft.com/zh-cn/sql/t-sql/queries/select-transact-sql?view=sql-server-2017 12345678910111. FROM2. ON3. JOIN4. WHERE5. GROUP BY6. WITH CUBE or WITH ROLLUP7. HAVING8. SELECT9. DISTINCT10. ORDER BY11. TOP 创建链接服务器12345678910111213/* 创建链接服务器 */exec sp_addlinkedserver 'srv_lnk','','sqloledb','条码数据库IP地址'exec sp_addlinkedsrvlogin 'srv_lnk','false',null,'用户名','密码'go/* 查询示例 */SELECT A.ListCodeFROM srv_lnk.条码数据库名.dbo.ME_ListCode A, IM_BarLend BWHERE A.ListCode=B.ListCodego/* 删除链接服务器 */exec sp_dropserver 'srv_lnk','droplogins' sys.sysprocesses 说明12345678910111213141516171819202122232425262728SELECT spid, -- sql session idkpid, -- windows thread idblocked, -- 值大于0表示阻塞, 值为本身进程ID表示io操作loginame,cmd, -- 当前正在执行的命令。open_tran, -- 进程的打开事务数status,program_name,waittime, -- (ms)db_name(dbid),uid,cpu,physical_io,memusage,login_time,last_batch,ecid,hostname,hostprocess,lastwaittype,waitresource,net_address,net_library,stmt_start,stmt_end,request_idFROM sys.sysprocesses WHERE spid &gt; 50 sqlserver schema123456789101112131415161718192021-- 完全限定的对象名称现在包含四部分：server.database.schema.objectuse test godrop schema myschemagocreate schema myschemagoalter schema myschema transfer dbo.Mytablecreate table myschema.TestTb( id bigint not null identity(1,1) , customer_name varchar(255) not null default(''), created_time datetime not null default getdate())-- 数据库锁定/block情况Select * From master.sys.sysprocesses Where Blocked &lt;&gt; 0 以下示例为 SQL Server 实例中的每个数据库返回后台队列中活动异步作业的数量。12345SELECT DB_NAME(database_id) AS [Database], COUNT(*) AS [Active Async Jobs] FROM sys.dm_exec_background_job_queue WHERE in_progress = 1 GROUP BY database_id; GO 收集查询自有连接有关信息的典型查询。12345678910SELECT c.session_id, c.net_transport, c.encrypt_option, c.auth_scheme, s.host_name, s.program_name, s.client_interface_name, s.login_name, s.nt_domain, s.nt_user_name, s.original_login_name, c.connect_time, s.login_time FROM sys.dm_exec_connections AS c JOIN sys.dm_exec_sessions AS s ON c.session_id = s.session_id WHERE c.session_id = @@SPID;]]></content>
  </entry>
  <entry>
    <title><![CDATA[jvm]]></title>
    <url>%2F2018%2F12%2F12%2Fjvm-memory-model%2F</url>
    <content type="text"><![CDATA[JVM 内存模型内存模型 多个线程同时对同一个共享变量进行读写的时候会产生线程安全问题。那为什么CPU不直接操作内存，而要在CPU和内存间加上各种缓存和寄存器等缓冲区呢？因为CPU的运算速度要比内存的读写速度快得多，如果CPU直接操作内存的话势必会花费很长时间等待数据到来，所以缓存的出现主要是为了解决CPU运算速度与内存读写速度不匹配的矛盾。 内存间交互协议JMM规定了主内存和工作内存间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，这主要包含了下面8个步骤： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 这8个步骤必须符合下述规则： 不允许read和load，store和write操作之一单独出现。 不允许一个线程丢弃它最近的assign操作。即变量在工作内存中改变了账号必须把变化同步回主内存 一个新的变量只允许在主内存中诞生，不允许工作内存直接使用未初始化的变量。 一个变量同一时刻只允许一条线程进行lock操作，但同一线程可以lock多次，lock多次之后必须执行同样次数的unlock操作 如果对一个变量进行lock操作，那么将会清空工作内存中此变量的值。 不允许对未lock的变量进行unlock操作，也不允许unlock一个被其它线程lock的变量 如果一个变量执行unlock操作，必须先把此变量同步回主内存中。 指令重排在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。从Java源代码到最终实际执行的指令序列，会分别经历下面3种重排序： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-LevelParallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 如果两个操作访问同一个变量，其中一个为写操作，此时这两个操作之间存在数据依赖性。 编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序，即不会重排序。不管怎么重排序，单线程下的执行结果不能被改变，编译器、runtime和处理器都必须遵守as-if-serial语义。 内存屏障通过插入内存屏障（Memory Barrier）可以阻止特定类型的指令重排。JMM将内存屏障划分为四种：| :——- | ——- |——–: ||屏障类型| 示例| 描述||LoadLoad Barriers | Load1-LoadLoad-Load2 |Load1数据装载过程要先于Load2及所有后续的数据装载过程||StoreStore Barriers | Store1-StoreStore-Store2| Store1刷新数据到内存的过程要先于Strore2及后续所有刷新数据到内存的过程||LoadStore Barriers |Load1-LoadStore-Store2| Load1数据装载要先于Strore2及后续所有刷新数据到内存的过程||StoreLoad Barriers |Store1-StoreLoad-Load2 |Store1刷新数据到内存的过程要先于Load2及所有后续的数据装载过程|Java中volatile关键字的实现就是通过内存屏障来完成的。 内存回收Eden 内存分配为了方便垃圾回收 ,jvm 将对内存分为新生代,老生带新生代分为 Eden ,from Survivor, to Survivor 区 其中Eden 和Survivor 区比例默认是 8:1:1 ,参数调整配置 -XX:SurvivorRatio=8当在Eden区分配内存不足时,会发生minorGC 由于多数对象生命周期很短,minorGC发生频繁 当发生minorGC 时jvm 会根据复制算法 将存活的对象拷贝到另一个未使用的Survivor区如果Survovor 区内存不足会使用分配担保策略将对象移动到老年代 谈到minorGC 相对的fullGC(majorGC) 是指发生在老年代的GC,不论是效率还是速度都比minorGC慢得多回收时会发生stop the world 是程序发生停顿 jvm confighttps://docs.oracle.com/cd/E40972_01/doc.70/e40973/cnf_jvmgc.htm#autoId2 1-server -Xms24G -Xmx24G -XX:PermSize=512m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=20 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70 javac 词法分析 语法分析 语义分析 字节码生成器 从源代码找出规范化的token流判断是否复合Java语言规范 形成抽象语法树将复杂语法转换为简单语法 语法糖处理生成字节码 jvm 运行时数据区域方法区虚拟机栈本地方法栈程序计数器 – 没有oom堆 运行时常量区是方法区的一部分存放编译时生成的字面量和符号引用 对象的创建 虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一 个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没 有，那必须先执行相应的类加载过程 对象的内存布局 示例数据 对齐填充 对象头 虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找 到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对 象头(Object Header)之中。根据虚拟机当前的运行状态的不同，如是否启用偏向锁等，对 象头会有不同的设置方式 对象的访问定位引用 java 1.2 如果reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块 内存代表着一个引用 回收方法区 判断是否是一个无用的类 所有类的实例都已经被回收 , java堆中不存在类的任何实例 加载该类的ClassLoader已经被回收 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该 类的方法 CMS 收集器 初始标记(CMS initial mark) - Stop The World 并发标记(CMS concurrent mark) - 重新标记(CMS remark) - Stop The World 并发清除(CMS concurrent sweep) - G1 初始标记(Initial Marking) 并发标记(Concurrent Marking) 最终标记(Final Marking) 筛选回收(Live Data Counting and Evacuation) 内存分配与回收策略 对象主要分配在新生代的Eden区上 ,少数情况下也可能会直接分配在老年代中 其细节取决于当前使用的是哪一种垃圾收集器组合，还有虚拟 机中与内存相关的参数的设置 对象优先在Eden分配 当Eden区没有足够空间进行分配时，虚拟 机将发起一次Minor GC]]></content>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[systemd]]></title>
    <url>%2F2018%2F12%2F06%2Fsystemd%2F</url>
    <content type="text"><![CDATA[Created symlink /etc/systemd/system/redis.service → /lib/systemd/system/redis-server.service the environment file1234567APP_ROOT=/srv/app/apiBINARY=&quot;api.jar&quot;PORT=&quot;1998&quot;USER=&quot;user&quot;JAVA_OPTS=&quot;-Xmx128m&quot;CONFIG_SERVER=&quot;-Dspring.cloud.config.uri=http://api:10000&quot;LOGGING=&quot;-Dlogging.config=/etc/logback-spring.xml -Dlogging.path=/opt/chuck/chuck-api/logs&quot; the systemd unit file1234567891011121314151617[Unit]Description=chuck-apiAfter=syslog.target[Service]EnvironmentFile=-/etc/default/chuck-apiWorkingDirectory=/opt/chuck/chuck-api/currentUser=chuckExecStart=/usr/bin/java -Duser.timezone=UTC $LOGGING $CONFIG_SERVER $JAVA_OPTS -Dserver.port=$&#123;PORT&#125; -jar $BINARYStandardOutput=journalStandardError=journalSyslogIdentifier=chuck-apiSuccessExitStatus=143[Install]WantedBy=multi-user.target 其中: - `WorkingDirectory` : app运行目录 - `SyslogIdentifier` : syslog 前缀 - `SuccessExitStatus` : JVM 成功退出码 `143` - `WorkingDirectory` and `User` 不能用环境变量 systemd fd limit系统 ulimit -n num systemd fd 默认4096 *.server1234[Service]LimitCORE=infinityLimitNOFILE=100000LimitNPROC=100000]]></content>
  </entry>
  <entry>
    <title><![CDATA[spring]]></title>
    <url>%2F2018%2F11%2F21%2Fspring%2F</url>
    <content type="text"><![CDATA[spring 实现拦截的五种姿势使用 Filter 接口Filter 接口由 J2EE 定义，在Servlet执行之前由容器进行调用。而SpringBoot中声明 Filter 又有两种方式： 注册 FilterRegistrationBean声明一个FilterRegistrationBean 实例，对Filter 做一系列定义，如下：1234567891011121314151617@Beanpublic FilterRegistrationBean customerFilter() &#123; FilterRegistrationBean registration = new FilterRegistrationBean(); // 设置过滤器 registration.setFilter(new CustomerFilter()); // 拦截路由规则 registration.addUrlPatterns("/intercept/*"); // 设置初始化参数 registration.addInitParameter("name", "customFilter"); registration.setName("CustomerFilter"); registration.setOrder(1); return registration;&#125; 其中 CustomerFilter 实现了Filter接口，如下： 12345678910111213141516171819public class CustomerFilter implements Filter &#123; private static final Logger logger = LoggerFactory.getLogger(CustomerFilter.class); private String name; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; name = filterConfig.getInitParameter("name"); &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; logger.info("Filter &#123;&#125; handle before", name); chain.doFilter(request, response); logger.info("Filter &#123;&#125; handle after", name); &#125;&#125; @WebFilter 注解为Filter的实现类添加 @WebFilter注解，由SpringBoot 框架扫描后注入 @WebFilter的启用需要配合@ServletComponentScan才能生效 1234567891011121314151617181920@Component@ServletComponentScan@WebFilter(urlPatterns = "/intercept/*", filterName = "annotateFilter")public class AnnotateFilter implements Filter &#123; private static final Logger logger = LoggerFactory.getLogger(AnnotateFilter.class); private final String name = "annotateFilter"; @Override public void init(FilterConfig filterConfig) throws ServletException &#123; &#125; @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; logger.info("Filter &#123;&#125; handle before", name); chain.doFilter(request, response); logger.info("Filter &#123;&#125; handle after", name); &#125;&#125; 使用注解是最简单的，但其缺点是仍然无法支持 order属性(用于控制Filter的排序)。而通常的@Order注解只能用于定义Bean的加载顺序，却真正无法控制Filter排序。这是一个已知问题，参考这里 HanlderInterceptorHandlerInterceptor 用于拦截 Controller 方法的执行，其声明了几个方法： 方法 说明preHandle Controller方法执行前调用preHandle Controller方法后，视图渲染前调用afterCompletion 整个方法执行后(包括异常抛出捕获)基于 HandlerInterceptor接口 实现的样例： 1234567891011121314151617181920212223242526272829303132333435363738394041public class CustomHandlerInterceptor implements HandlerInterceptor &#123; private static final Logger logger = LoggerFactory.getLogger(CustomHandlerInterceptor.class); /* * Controller方法调用前，返回true表示继续处理 */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HandlerMethod method = (HandlerMethod) handler; logger.info("CustomerHandlerInterceptor preHandle, &#123;&#125;", method.getMethod().getName()); return true; &#125; /* * Controller方法调用后，视图渲染前 */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; HandlerMethod method = (HandlerMethod) handler; logger.info("CustomerHandlerInterceptor postHandle, &#123;&#125;", method.getMethod().getName()); response.getOutputStream().write("append content".getBytes()); &#125; /* * 整个请求处理完，视图已渲染。如果存在异常则Exception不为空 */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; HandlerMethod method = (HandlerMethod) handler; logger.info("CustomerHandlerInterceptor afterCompletion, &#123;&#125;", method.getMethod().getName()); &#125;&#125; 除了上面的代码实现，还不要忘了将 Interceptor 实现进行注册：12345678910@Configurationpublic class InterceptConfig extends WebMvcConfigurerAdapter &#123; // 注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new CustomHandlerInterceptor()).addPathPatterns("/intercept/**"); super.addInterceptors(registry); &#125; HandlerInterceptor 来自SpringMVC框架，基本可代替 Filter 接口使用；除了可以方便的进行异常处理之外，通过接口参数能获得Controller方法实例，还可以实现更灵活的定制。 @ExceptionHandler 注解@ExceptionHandler 的用途是捕获方法执行时抛出的异常，通常可用于捕获全局异常，并输出自定义的结果。 如下面的实例：12345678910111213141516171819202122@ControllerAdvice(assignableTypes = InterceptController.class)public class CustomInterceptAdvice &#123; private static final Logger logger = LoggerFactory.getLogger(CustomInterceptAdvice.class); /** * 拦截异常 * * @param e * @param m * @return */ @ExceptionHandler(value = &#123; Exception.class &#125;) @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) @ResponseBody public String handle(Exception e, HandlerMethod m) &#123; logger.info("CustomInterceptAdvice handle exception &#123;&#125;, method: &#123;&#125;", e.getMessage(), m.getMethod().getName()); return e.getMessage(); &#125;&#125; 需要注意的是，@ExceptionHandler 需要与 @ControllerAdvice配合使用其中 @ControllerAdvice的 assignableTypes 属性指定了所拦截类的名称。除此之外，该注解还支持指定包扫描范围、注解范围等等。 RequestBodyAdvice/ResponseBodyAdviceRequestBodyAdvice、ResponseBodyAdvice 相对于读者可能比较陌生，而这俩接口也是 Spring 4.x 才开始出现的。 RequestBodyAdvice 的用法我们都知道，SpringBoot 中可以利用@RequestBody这样的注解完成请求内容体与对象的转换。而RequestBodyAdvice 则可用于在请求内容对象转换的前后时刻进行拦截处理，其定义了几个方法： 方法 说明supports 判断是否支持handleEmptyBody 当请求体为空时调用beforeBodyRead 在请求体未读取(转换)时调用afterBodyRead 在请求体完成读取后调用实现代码如下：1234567891011121314151617181920212223242526272829303132333435363738@ControllerAdvice(assignableTypes = InterceptController.class)public class CustomRequestAdvice extends RequestBodyAdviceAdapter &#123; private static final Logger logger = LoggerFactory.getLogger(CustomRequestAdvice.class); @Override public boolean supports(MethodParameter methodParameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123; // 返回true，表示启动拦截 return MsgBody.class.getTypeName().equals(targetType.getTypeName()); &#125; @Override public Object handleEmptyBody(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123; logger.info("CustomRequestAdvice handleEmptyBody"); // 对于空请求体，返回对象 return body; &#125; @Override public HttpInputMessage beforeBodyRead(HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) throws IOException &#123; logger.info("CustomRequestAdvice beforeBodyRead"); // 可定制消息序列化 return new BodyInputMessage(inputMessage); &#125; @Override public Object afterBodyRead(Object body, HttpInputMessage inputMessage, MethodParameter parameter, Type targetType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123; logger.info("CustomRequestAdvice afterBodyRead"); // 可针对读取后的对象做转换，此处不做处理 return body; &#125; 上述代码实现中，针对前面提到的 MsgBody对象类型进行了拦截处理。在beforeBodyRead 中，返回一个BodyInputMessage对象，而这个对象便负责源数据流解析转换 12345678910111213141516171819202122232425public static class BodyInputMessage implements HttpInputMessage &#123; private HttpHeaders headers; private InputStream body; public BodyInputMessage(HttpInputMessage inputMessage) throws IOException &#123; this.headers = inputMessage.getHeaders(); // 读取原字符串 String content = IOUtils.toString(inputMessage.getBody(), "UTF-8"); MsgBody msg = new MsgBody(); msg.setContent(content); this.body = new ByteArrayInputStream(JsonUtil.toJson(msg).getBytes()); &#125; @Override public InputStream getBody() throws IOException &#123; return body; &#125; @Override public HttpHeaders getHeaders() &#123; return headers; &#125; &#125; 代码说明完成数据流的转换，包括以下步骤： 获取请求内容字符串；构建 MsgBody 对象，将内容字符串作为其 content 字段；将 MsgBody 对象 Json 序列化，再次转成字节流供后续环节使用。ResponseBodyAdvice 用法ResponseBodyAdvice 的用途在于对返回内容做拦截处理，如下面的示例：123456789101112131415161718192021222324@ControllerAdvice(assignableTypes = InterceptController.class) public static class CustomResponseAdvice implements ResponseBodyAdvice&lt;String&gt; &#123; private static final Logger logger = LoggerFactory.getLogger(CustomRequestAdvice.class); @Override public boolean supports(MethodParameter returnType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; converterType) &#123; // 返回true，表示启动拦截 return true; &#125; @Override public String beforeBodyWrite(String body, MethodParameter returnType, MediaType selectedContentType, Class&lt;? extends HttpMessageConverter&lt;?&gt;&gt; selectedConverterType, ServerHttpRequest request, ServerHttpResponse response) &#123; logger.info("CustomResponseAdvice beforeBodyWrite"); // 添加前缀 String raw = String.valueOf(body); return "PREFIX:" + raw; &#125; &#125; 一般在需要对输入输出流进行特殊处理(比如加解密)的场景下使用。 @Aspect 注解这是目前最灵活的做法，直接利用注解可实现任意对象、方法的拦截。在某个Bean的类上面 @Aspect 注解便可以将一个Bean 声明为具有AOP能力的对象。1234567891011121314151617181920212223@Aspect@Componentpublic class InterceptControllerAspect &#123; private static final Logger logger = LoggerFactory.getLogger(InterceptControllerAspect.class); @Pointcut("target(org.zales.dmo.boot.controllers.InterceptController)") public void interceptController() &#123; &#125; @Around("interceptController()") public Object handle(ProceedingJoinPoint joinPoint) throws Throwable &#123; logger.info("aspect before."); try &#123; return joinPoint.proceed(); &#125; finally &#123; logger.info("aspect after."); &#125; &#125;&#125; 简单说明 @Pointcut 用于定义切面点，而使用target关键字可以定位到具体的类。@Around 定义了一个切面处理方法，通过注入ProceedingJoinPoint对象达到控制的目的。 一些常用的切面注解： 注解 说明@Before 方法执行之前@After 方法执行之后@Around 方法执行前后@AfterThrowing 抛出异常后@AfterReturing 正常返回后深入一点aop的能力来自于spring-boot-starter-aop，进一步依赖于aspectjweaver组件。有兴趣可以进一步了解。 推荐指数5颗星，aspectj 与 SpringBoot 可以无缝集成，这是一个经典的AOP框架，可以实现任何你想要的功能，笔者之前曾在多个项目中使用，效果是十分不错的。注解的支持及自动包扫描大大简化了开发，然而，你仍然需要先对 Pointcut 的定义有充分的了解。 123456789101112- Filter customFilter handle before- Filter annotateFilter handle before- CustomerHandlerInterceptor preHandle, body- CustomRequestAdvice beforeBodyRead- CustomRequestAdvice afterBodyRead- aspect before.- aspect after.- CustomResponseAdvice beforeBodyWrite- CustomerHandlerInterceptor postHandle, body- CustomerHandlerInterceptor afterCompletion, body- Filter annotateFilter handle after- Filter customFilter handle after]]></content>
  </entry>
  <entry>
    <title><![CDATA[windows-server]]></title>
    <url>%2F2018%2F11%2F02%2Fwindows-server%2F</url>
    <content type="text"><![CDATA[windows service1sc create consulserver binPath= "D:\consul\consul.exe agent -config-file D:\consul\conf\consul.json" DisplayName= "Consul Server" start= auto run with cmd and administrator vmware esxi 6 许可证 10U0QJ-FR1EP-KZQN9-J1C74-23P5R powershell 连接远程 windows123456$uname = &quot;Administrator&quot; #administrator为用户名$passwd = ConvertTo-SecureString &quot;abcdefg&quot; -AsPlainText -Force; #abcdefg为密码$cred = New-Object System.Management.Automation.PSCredential($uname, $passwd); #创建自动认证对象$pcname = &quot;127.0.0.1&quot;Enter-PSSession -ComputerName $pcname -Credential $cred #登录]]></content>
  </entry>
  <entry>
    <title><![CDATA[centos]]></title>
    <url>%2F2018%2F09%2F05%2Fcentos%2F</url>
    <content type="text"><![CDATA[CONFIG STATIC IPvim /etc/sysconfig/network-scripts/ifcfg-XXXX 1234567891011121314151617181920TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticIPADDR=192.168.2.81NETMASK=255.255.0.0GATEWAY=192.168.2.1DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens192UUID=965fd86f-b05a-4797-a851-2fd268affb03DEVICE=ens192ONBOOT=yesDNS1=192.168.2.214 yum install docker-ce123sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 123sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo 1sudo yum install docker-ce -y centos docker-compose1sudo curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose 1sudo chmod +x /usr/local/bin/docker-compose 加速器1234sudo tee /etc/docker/daemon.json &lt;&lt;- 'EOF'&#123; "registry-mirrors":["https://registry.docker-cn.com"]&#125; nginx 静态文件代理服务器 403 权限不够 selinux 需要关闭 临时 123getenforcesetenforce 0 永久 vim /etc/sysconfig/selinux 1SELINUX=disabled linux user useradd options username options -c comment -d 目录 -G 用户组指定用户所属的附加组 -g 永驻 指定用户所属的用户组 -s shell 文件 指定用户的登陆shell -u 用户号 用户名 指定新账号的登陆名 12345useradd -d /usr/ziggle -m ziggleuseradd -s /bin/sh -g group –G adm,root gem此命令新建了一个用户gem，该用户的登录Shell是 /bin/sh，它属于group用户组，同时又属于adm和root用户组，其中group用户组是其主组。 userdel options username123userdel -r ziggle此命令删除用户sam在系统文件中（主要是/etc/passwd, /etc/shadow, /etc/group等）的记录，同时删除用户的主目录。 linux usergroup group add options usergroup options -g GID 组标识 -o group del 1groupdel group1 与用户账号有关的系统文件1234567891011121314151617181920212223[root@frpserver ~]# cat /etc/passwdroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltmail:x:8:12:mail:/var/spool/mail:/sbin/nologinoperator:x:11:0:operator:/root:/sbin/nologingames:x:12:100:games:/usr/games:/sbin/nologinftp:x:14:50:FTP User:/var/ftp:/sbin/nologinnobody:x:99:99:Nobody:/:/sbin/nologinsystemd-network:x:192:192:systemd Network Management:/:/sbin/nologindbus:x:81:81:System message bus:/:/sbin/nologinpolkitd:x:999:997:User for polkitd:/:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinchrony:x:998:996::/var/lib/chrony:/sbin/nologinsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologintcpdump:x:72:72::/:/sbin/nologinnscd:x:28:28:NSCD Daemon:/:/sbin/nologin 从上面的例子我们可以看到，/etc/passwd中一行记录对应着一个用户，每行记录又被冒号(:)分隔为7个字段，其格式和具体含义如下： 用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell]]></content>
  </entry>
  <entry>
    <title><![CDATA[deploysh]]></title>
    <url>%2F2018%2F08%2F07%2Fdeploysh%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526#!/bin/sh# 结束当前运行jarkillJarApp()&#123; appPid=$(ps aux |grep java |grep -v grep |awk -e "&#123;print \$2&#125;") for id in $appPid do prefix='killing app .. pid : ' echo $prefix$id kill -9 $id done&#125;killJarApp# 运行最新jarrunLatestJarFile()&#123; latestJarFile=$(ls -lt |grep "data.*jar"| head -1|awk -e "&#123;print \$9&#125;") # echo -e 'latestJarFile to Run is : ' prefix='latestJarFile to Run is : ' echo $prefix$latestJarFile # nohup java -jar ./data-2018-08-07-06-04-22.jar --spring.profiles.active=prod --server.port=8081 &amp;&gt; 8081nohup.out&amp; nohup java -jar $latestJarFile --spring.profiles.active=prod --server.port=8081 &amp;&gt; 8081nohup.out&amp;&#125;runLatestJarFile]]></content>
  </entry>
  <entry>
    <title><![CDATA[wsl]]></title>
    <url>%2F2018%2F07%2F10%2Fwsl%2F</url>
    <content type="text"><![CDATA[wsl host config vim /etc/hosts 123456789101112# This file is automatically generated by WSL based on the Windows hosts file:# %WINDIR%\System32\drivers\etc\hosts. Modifications to this file will be overwritten.127.0.0.1 localhost# 127.0.1.1 ziggle-desk.localdomain ziggle-desk127.0.1.1 ziggle-desk# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters ssh remote to wsl sudo apt-get remove openssh-server sudo apt-get install openssh-server sudo vim /etc/ssh/sshd_config 修改配置/etc/ssh/sshd_config 1234Port 2222 #默认的是22，但是windows有自己的ssh服务，也是监听的22端口，所以这里要改一下UsePrivilegeSeparation noPasswordAuthentication yesAllowUsers youusername # 这里改成你登陆WSL用的 reboot ssh sudo service ssh –full-restart linux-brew install hisory version 12345brew uninstall &lt;package&gt; # Note 0brew tap-new &lt;yourname&gt;/&lt;package&gt; # Note 1brew extract --version &lt;version_you_want&gt; composer &lt;yourname&gt;/&lt;package&gt; # Note 2brew install &lt;package&gt;@&lt;version&gt;brew pin &lt;package&gt;@&lt;version&gt; # Note 3]]></content>
  </entry>
  <entry>
    <title><![CDATA[ssh-login]]></title>
    <url>%2F2018%2F06%2F08%2Fssh-login%2F</url>
    <content type="text"><![CDATA[配置ssh免密码登陆 本地生成ssh公钥1$ ssh-keygen 复制到服务器 123 scpgoogle app engine 添加元数据]]></content>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-find]]></title>
    <url>%2F2018%2F05%2F29%2Flinux-find%2F</url>
    <content type="text"><![CDATA[###123# find path -option [ -print] [ -exec -ok command] &#123;&#125; root@ziggle:~$ find / -name *.conf -type f -print 参数说明参数说明 : find 根据下列规则判断 path 和 expression，在命令列上第一个 - ( ) , ! 之前的部份为 path，之后的是 expression。如果 path 是空字串则使用目前路径，如果 expression 是空字串则使用 -print 为预设 expression。 expression 中可使用的选项有二三十个之多，在此只介绍最常用的部份。 -mount, -xdev : 只检查和指定目录在同一个文件系统下的文件，避免列出其它文件系统中的文件 -amin n : 在过去 n 分钟内被读取过 -anewer file : 比文件 file 更晚被读取过的文件 -atime n : 在过去 n 天过读取过的文件 -cmin n : 在过去 n 分钟内被修改过 -cnewer file :比文件 file 更新的文件 -ctime n : 在过去 n 天过修改过的文件 -empty : 空的文件-gid n or -group name : gid 是 n 或是 group 名称是 name -ipath p, -path p : 路径名称符合 p 的文件，ipath 会忽略大小写 -name name, -iname name : 文件名称符合 name 的文件。iname 会忽略大小写 -size n : 文件大小 是 n 单位，b 代表 512 位元组的区块，c 表示字元数，k 表示 kilo bytes，w 是二个位元组。-type c : 文件类型是 c 的文件。 d: 目录 c: 字型装置文件 b: 区块装置文件 p: 具名贮列 f: 一般文件 l: 符号连结 s: socket -pid n : process id 是 n 的文件 你可以使用 ( ) 将运算式分隔，并使用下列运算。 exp1 -and exp2 ! expr -not expr exp1 -or exp2 exp1, exp2-print :将查找到的文件输出到标准输出-exec command {} \; – 将查到的文件执行command操作,{} 和 \;之间有空格-ok 和-exec相同，只不过在操作前要询用户 实例 找到当前目录下及子目录拓展名为 c 的文件 1find . -name &quot;*.c&quot; 目录里找类型 1find . -type f 目录找最近天数修改的文件 1find . -ctime -20 查找前目录中文件属主具有读、写权限，并且文件所属组的用户和其他用户具有读权限的文件： 1234567891011121314find . -type f -perm 644 -exec ls -l &#123;&#125; \;find . -name *lin*，其中 . 代表在当前目录找，-name 表示匹配文件名 / 文件夹名，*lin* 用通配符搜索含有lin的文件或是文件夹find . -iname *lin*，其中 . 代表在当前目录找，-iname 表示匹配文件名 / 文件夹名（忽略大小写差异），*lin* 用通配符搜索含有lin的文件或是文件夹find / -name *.conf，其中 / 代表根目录查找，*.conf代表搜索后缀会.conf的文件find /opt -name .oh-my-zsh，其中 /opt 代表目录名，.oh-my-zsh 代表搜索的是隐藏文件 / 文件夹名字为 oh-my-zsh 的find /opt -type f -iname .oh-my-zsh，其中 /opt 代表目录名，-type f 代表只找文件，.oh-my-zsh 代表搜索的是隐藏文件名字为 oh-my-zsh 的find /opt -type d -iname .oh-my-zsh，其中 /opt 代表目录名，-type d 代表只找目录，.oh-my-zsh 代表搜索的是隐藏文件夹名字为 oh-my-zsh 的find . -name &quot;lin*&quot; -exec ls -l &#123;&#125; \;，当前目录搜索lin开头的文件，然后用其搜索后的结果集，再执行ls -l的命令（这个命令可变，其他命令也可以），其中 -exec 和 &#123;&#125; ; 都是固定格式find /opt -type f -size +800M -print0 | xargs -0 du -h | sort -nr，找出 /opt 目录下大于 800 M 的文件find / -name &quot;*tower*&quot; -exec rm &#123;&#125; \;，找到文件并删除find / -name &quot;*tower*&quot; -exec mv &#123;&#125; /opt \;，找到文件并移到 opt 目录find . -name &quot;*&quot; |xargs grep &quot;youmeek&quot;，递归查找当前文件夹下所有文件内容中包含 youmeek 的文件find . -size 0 | xargs rm -f &amp;，删除当前目录下文件大小为0的文件du -hm --max-depth=2 | sort -nr | head -12，找出系统中占用容量最大的前 12 个目录]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-io]]></title>
    <url>%2F2018%2F05%2F14%2Fjava-io%2F</url>
    <content type="text"><![CDATA[Java 的 I/O 大概可以分成以下几类： 磁盘操作：File 字节操作：InputStream 和 OutputStream 字符操作：Reader 和 Writer 对象操作：Serializable 网络操作：Socket 新的输入/输出：NIO 字节操作 java i/o 使用了装饰着模式实现,以 InputStream 为例，InputStream 是抽象组件，FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作。FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能，例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。 实例化一个具有缓存功能的字节流对象时，只需要在 FileInputStream 对象上再套一层 BufferedInputStream 对象即可。 1BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file)); 字符操作不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符，所以 I/O 操作的都是字节而不是字符。但是在程序中操作的通常是字符形式的数据，因此需要提供对字符进行操作的方法。 InputStreamReader 实现从文本文件的字节流解码成字符流；OutputStreamWriter 实现字符流编码成为文本文件的字节流。它们继承自 Reader 和 Writer。 编码就是把字符转换为字节，而解码是把字节重新组合成字符。 12byte[] bytes = str.getBytes(encoding); // 编码String str = new String(bytes, encoding)； // 解码 GBK 编码中，中文占 2 个字节，英文占 1 个字节；UTF-8 编码中，中文占 3 个字节，英文占 1 个字节；Java 使用双字节编码 UTF-16be，中文和英文都占 2 个字节。 如果编码和解码过程使用不同的编码方式那么就出现了乱码。 五、对象操作序列化就是将一个对象转换成字节序列，方便存储和传输。 序列化：ObjectOutputStream.writeObject() 反序列化：ObjectInputStream.readObject() 序列化的类需要实现 Serializable 接口，它只是一个标准，没有任何方法需要实现。 transient 关键字可以使一些属性不会被序列化。 ArrayList 序列化和反序列化的实现 ：ArrayList 中存储数据的数组是用 transient 修饰的，因为这个数组是动态扩展的，并不是所有的空间都被使用，因此就不需要所有的内容都被序列化。通过重写序列化和反序列化方法，使得可以只序列化数组中有内容的那部分数据。 1private transient Object[] elementData; 123456789101112131415161718192021public static void main(String[] args) throws Exception &#123; Pers p = new Pers("aa", "bb"); Ser(p); Pers ps = Deser(); System.out.println(ps);&#125;public static void Ser(Pers p) throws IOException &#123; try (ObjectOutputStream outputStream = new ObjectOutputStream(new FileOutputStream("./pers.bin"))) &#123; outputStream.writeObject(p); System.out.println("ser done"); &#125;&#125;public static Pers Deser() throws IOException, ClassNotFoundException &#123; ObjectInputStream ois = new ObjectInputStream(new FileInputStream(new File("./pers.bin"))); Pers p = (Pers) ois.readObject(); System.out.println("Person对象反序列化成功！"); return p;&#125; 网络操作NIO12345678910111213141516它用的是事件机制。它可以用一个线程把Accept，读写操作，请求处理的逻辑全干了。如果什么事都没得做，它也不会死循环，它会将线程休眠起来，直到下一个事件来了再继续干活，这样的一个线程称之为NIO线程。用伪代码表示：while true &#123; events = takeEvents(fds) // 获取事件，如果没有事件，线程就休眠 for event in events &#123; if event.isAcceptable &#123; doAccept() // 新链接来了 &#125; elif event.isReadable &#123; request = doRead() // 读消息 if request.isComplete() &#123; doProcess() &#125; &#125; elif event.isWriteable &#123; doWrite() // 写消息 &#125; &#125;&#125;]]></content>
      <tags>
        <tag>java-io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux group]]></title>
    <url>%2F2018%2F05%2F07%2Flinux-group%2F</url>
    <content type="text"><![CDATA[123456root@ziggle:~# cat /etc/passwdroot:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologin.... /etc/passwd 中各个字段表示的信息 用:分割 通过用户类型分组，我们可以把用户分为： 管理员 普通用户 通过用户组类型分组，我们可以把用户组分为： 管理员组 普通用户组 从用户的角度，我们可以把用户组分为： 基本组（默认组） 额外组（附加组） 对于普通用户，我们还可以分为: 系统用户 普通用户 因此，对于普通用户组，我们也可以分为： 系统用户组 普通用户组/etc/passwd 中各字段含义account : 用户名password :密码占位符 (x)uid : 用户IDgid : 用户组IDcommand : 注释信息home dir : 用户家目录shell : 用户默认shell 密码 /etc/shadow useradd useradd -u UID：指定 UID，这个 UID 必须是大于等于500，并没有其他用户占用的 UID useradd -g GID/GROUPNAME：指定默认组，可以是 GID 或者 GROUPNAME，同样也必须真实存在 useradd -G GROUPS：指定额外组 useradd -c COMMENT：指定用户的注释信息 useradd -d PATH：指定用户的家目录 useradd -s SHELL：指定用户的默认 shell，最好是在 /etc/shells 中存在的路径 useradd -s /sbin/nologin：该用户不能登录，还记得我们上面说到的系统用户不能登录吧？我们可以看到系统用户的 shell 字段也是 /sbin/nologin echo $SHELL ：查看当前用户的 shell 类型 useradd -M USERNAME：创建用户但不创建家目录 useradd -mk USERNAME：创建用户的同时创建家目录，并复制 /etc/skel 中的内容到家目录中。关于 /etc/skel 目录会在下一篇 Linux 权限管理中再次讲解。如果用户没有家目录，那么不能切换到该用户 userdel userdel [username] : 删除用户 userdel -r [username] : 删除用户同时删除用户家目录]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 问题解决]]></title>
    <url>%2F2018%2F05%2F07%2Fgit-%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[git unable pull from remote12345muyue@wp-desk MINGW64 /d/code/LearnOnlineAPI (dev)$ git pullerror: cannot lock ref 'refs/remotes/origin/feature/zbf': 'refs/remotes/origin/feature/zbf/Notice' exists; cannot create 'refs/remotes/origin/feature/zbf'From http://git.xueanquan.cc/dotnet/live/LearnOnlineAPI ! [new branch] feature/zbf -&gt; origin/feature/zbf (unable to update local ref) solution12$ git gc --prune=nowgit remote prune origin 全局配置~/.gitconfig1234567891011121314151617181920212223[filter "lfs"] clean = git-lfs clean -- %f smudge = git-lfs smudge -- %f process = git-lfs filter-process required = true[user] name = ziggle email = muyue1125@gmail.com# [https]# proxy = https://127.0.0.1:8087# [http]# proxy = http://127.0.0.1:8087[merge] tool = vimdiff[credential] helper = wincred[alias] tree = log --oneline --decorate --all --graph# [include]# path = 别名1git config --global alias.tree &quot;log --oneline --decorate --all --graph&quot; 代理###1$ git rm -r -n --cached "bin/" ，此命令是展示要删除的文件表预览 Unstaging a Staged File1git reset HEAD benchmarks.rb Unmodifying a Modified File1git checkout --benchmarks.rb 怎样在一个分支拉去另一个分支的远程代码12git fetch &lt;remote&gt; &lt;sourceBranch&gt;:&lt;destinationBranch&gt;git fetch origin master:master 找到一个被删除的文件并恢复 Use git log –diff-filter=D –summary to get all the commits which have deleted files and the files deleted; Use git checkout $commit~1 path/to/file.ext to restore the deleted file. linux /windows git status 显示不同 git config –global core.autocrlf true ###在默认设置下，中文文件名在工作区状态输出，查看历史更改概要，以及在补丁文件中，文件名的中文不能正确地显示，而是显示为八进制的字符编码，示例如下： $ git status -s ?? “\350\257\264\346\230\216.txt\n $ printf “\350\257\264\346\230\216.txt\n” 1git config --global core.quotepath false]]></content>
      <tags>
        <tag>-git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[powershell]]></title>
    <url>%2F2018%2F04%2F14%2Fpowershell%2F</url>
    <content type="text"><![CDATA[1&gt; taskkill /pid 12436 /f tee-object1scripts git:(master) ✗ frida -U com.tencent.mm -l .\ec\wechat.js | Tee-Object log.log]]></content>
      <tags>
        <tag>powershell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[stackoverflow-question]]></title>
    <url>%2F2018%2F04%2F12%2Fstackoverflow-question%2F</url>
    <content type="text"><![CDATA[一行初始化ArrayList1234567ArrayList&lt;String&gt; places = new ArrayList&lt;String&gt;()&#123;&#123; add("A"); add("B"); add("C");&#125;&#125;;List&lt;String&gt; places = Arrays.asList("Buenos Aires", "Córdoba", "La Plata"); 使用maven创建带依赖的可执行jar12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;fully.qualified.MainClass&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;fully.qualified.MainClass&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; and run mvn clean compile assembly:single 判断数组是否包含指定元素12int[] a = [1,2,3,4,5];boolean contains = IntStream.of(a).anyMatch(x-&gt;x==3); 什么时候使用静态方法One rule-of-thumb: ask yourself “does it make sense to call this method, even if no Obj has been constructed yet?” If so, it should definitely be static. So in a class Car you might have a method double convertMpgToKpl(double mpg) which would be static, because one might want to know what 35mpg converts to, even if nobody has ever built a Car. But void setMileage(double mpg) (which sets the efficiency of one particular Car) can’t be static since it’s inconceivable to call the method before any Car has been constructed. (Btw, the converse isn’t always true: you might sometimes have a method which involves two Car objects, and still want it to be static. E.g. Car theMoreEfficientOf( Car c1, Car c2 ). Although this could be converted to a non-static version, some would argue that since there isn’t a “privileged” choice of which Car is more important, you shouldn’t force a caller to choose one Car as the object you’ll invoke the method on. This situation accounts for a fairly small fraction of all static methods, though.) [not-just-yeti[source]] [https://stackoverflow.com/questions/2671496/java-when-to-use-static-methods] [source_link_title] 是不是final block 一定会被执行是的, final 一定会被执行 除了1 调用 System.exit();2 JVM 在执行final快前崩溃3 try 块 又死循环 等4 JVM 进行被强行终止5 系统崩溃 .. java bean 是什么JavaBean 是一个标准, 体格惯例1 所有的属性都是private2 有一个无参的构造函数3 实现Serializable StringBuffer 与 StringBuilder 的区别StringBuffer 是同步的 而 StringBuild不是]]></content>
      <tags>
        <tag>stackoverflow-questions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[differences_between_hashmap_and_hashtable]]></title>
    <url>%2F2018%2F04%2F08%2Fdifferences-between-hashmap-and-hashtable%2F</url>
    <content type="text"><![CDATA[线程hashtable 是同步的 相反 hashmap并不是,nullhashtable 不允许有nullkey 或value ,hashmap 运行有一个null key 和任意个null valueiteratorhashmap的子类linkedhashmap 迭代出的元素按插入顺序 可以有hashmap -&gt; linkedhashmap 获取有序遍历 为什么使用char[] 保存password 而不是 Stringstring 类型是不可变类型,当创建String 实例对象,如果另一个进程dump 内存,是没法清除创建的string (在gc介入之前),如果用char[] 可以显式wrap数据减少了password 被attack …. java concurrent 包实现线程间通信有四种方式A线程写volatile变量，随后B线程读这个volatile变量。A线程写volatile变量，随后B线程用CAS更新这个volatile变量。A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 volatile变量的读/写和CAS可以实现线程之间的通信 =&gt; java concurrent包的基石 分析HashMap的put方法①.判断键值对数组 table[i] 是否为空或为 null，否则执行 resize() 进行扩容； ②.根据键值 key 计算 hash 值得到插入的数组索引i，如果 table[i]==null，直接新建节点添加，转向 ⑥，如果table[i] 不为空，转向 ③； ③.判断 table[i] 的首个元素是否和 key 一样，如果相同直接覆盖 value，否则转向 ④，这里的相同指的是 hashCode 以及 equals； ④.判断table[i] 是否为 treeNode，即 table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向 ⑤； ⑤.遍历 table[i]，判断链表长度是否大于 8，大于 8 的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现 key 已经存在直接覆盖 value 即可； ⑥.插入成功后，判断实际存在的键值对数量 size 是否超多了最大容量 threshold，如果超过，进行扩容。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125;]]></content>
      <tags>
        <tag>hashmap hashtable 的不同</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[exceptionhandler]]></title>
    <url>%2F2018%2F04%2F04%2Fexceptionhandler%2F</url>
    <content type="text"><![CDATA[在task执行完处理异常 处理异常1123456789101112var task1 = Task.Run(() =&gt;&#123; Console.WriteLine(1223); // throw new CustomException(&quot;task1 faulted.&quot;);&#125;).ContinueWith(t =&gt; &#123; Console.WriteLine(&quot;&#123;0&#125;: &#123;1&#125;&quot;, t.Exception.InnerException.GetType().Name, t.Exception.InnerException.Message);&#125;, TaskContinuationOptions.OnlyOnFaulted);Thread.Sleep(500);// 当发生异常时执行ContinueWith 在task执行结束 主线程内处理异常 处理异常2123var tt = Task.Factory.StartNew(() =&gt; throw new ArgumentException());while (!tt.IsCompleted)&#123;&#125; // tt.Wait()var excoCollection = tt.Exception?.InnerExceptions; 处理异常3 123456789101112131415var task1 = Task.Run(() =&gt; &#123; throw new CustomException(&quot;This exception is expected!&quot;); &#125;);while (!task1.IsCompleted) &#123; &#125;if (task1.Status == TaskStatus.Faulted)&#123; foreach (var e in task1.Exception.InnerExceptions) &#123; // 处理自定义异常 if (e is CustomException) &#123; Console.WriteLine(e.Message); &#125; // 抛去其他异常 else&#123;throw e;&#125; &#125;&#125; 如果不想等到task结束]]></content>
      <tags>
        <tag>Exception handling (parallel library)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql调优]]></title>
    <url>%2F2018%2F03%2F16%2Fsql%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[连接优化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465--beforeSELECT*FROM( SELECT TOP 15 * FROM zbjm_comment WITH (nolock) WHERE AuditStatus = 1 AND IsDeleted = 0 AND path LIKE '1:0000000000000000059-0000000000000107656%' ORDER BY Id) a0UNION ALL SELECT * FROM ( SELECT TOP 15 * FROM zbjm_comment WITH (nolock) WHERE AuditStatus = 1 AND IsDeleted = 0 AND path LIKE '1:0000000000000000059-0000000000000003369%' ORDER BY Id ) a10-- after DECLARE @@temp TABLE (path VARCHAR(MAX)) INSERT INTO @@tempVALUES ('1:0000000000000000059-0000000000000107656%'), ('1:0000000000000000059-0000000000000087470%'), ('1:0000000000000000059-0000000000000040167%'), ('1:0000000000000000059-0000000000000004254%'), ('1:0000000000000000059-0000000000000003377%'), ('1:0000000000000000059-0000000000000003369%') SELECT c.[Id], c.[UserId], c.[NickName], c.[Avatar], c.[TargetType], c.[TargetId], c.[Path], c.[CommentContent], c.[AuditStatus], c.[PraiseCount], c.[CreationTime], c.[ModificationTime], c.[IsDeleted] FROM zbjm_comment c WITH ( nolock, INDEX = [NonClusteredIndex-Path] ) JOIN @@temp t ON c.path LIKE t.path -- 此处c 表path 建立非聚集索引 WHERE c.auditstatus = 1 AND c.isdeleted = 0 插入时去重 insert 1234567891011121314151617181920212223242526272829303132333435INSERT INTO dbo.Learn_CouponSendJob ( UserId, NickName, AddTime, IsDeleted, SendId, CouponId, IsNotice, SendTime, SendStatus, SendToId) SELECT t1.*FROM ( SELECT Id AS UserId, NickName AS NickName, GetDate() AS AddTime, 0 AS IsDeleted, 43 AS SendId, 3 AS CouponId, 0 AS IsNotice ,GETDATE() AS SendTime, 1 AS SendStatus ,- 1 AS SendToId FROM Learn_User WITH (NOLOCK) WHERE Status = 1 AND Id IN (152) ) t1LEFT JOIN dbo.Learn_CouponSendJob t2 ON t1.UserId = t2.UserIdAND t1.SendId = t2.SendIdAND t1.SendToId = t2.SendToIdWHERE t2.id IS NULL 查询父表数据并 统计子表中的数量123456789101112131415161718SELECT ROW_NUMBER () OVER (ORDER BY MAX(eg.addtime)) RowNum, MAX (eg.title) title, MAX (eg.id) id, MAX (eg.ProductType) productType, MAX (eg.ProductId) productId, MAX (eg.addtime) addtime, MAX (eg.begintime) begintime, MAX (eg.endtime) endtime, MAX (eg.total) total, MAX (eg.status) status, COUNT (e.GroupId) GetNum, CASE WHEN COUNT (e.GroupId) = MAX(eg.total) THEN 1 ELSE 0 end IsGenFROM Learn_exchangeGroup eg WITH (nolock)LEFT JOIN Learn_exchange e WITH (nolock) ON e.GroupId = eg.idgroup by eg.id 对版本软件版本管理当前版本和最新之间的版本是否包含强制更新12345678910111213141516171819202122232425DECLARE @Need INTSET @Need = ( SELECT COUNT (1) AS c FROM Learn_AppInfo WITH (nolock) WHERE version &gt;=@version AND Needupdate = 1 AND isdeleted = 0) SELECT TOP 1 a.Version, a.title, a.content, a.systemtype, a.url, a.auditstatus, case when @Need&gt; 0 then 1 else 0 end AS NeedUpdateFROM Learn_AppInfo aWHERE systemtype = 1AND isdeleted = 0ORDER BY version DESC]]></content>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2018%2F03%2F14%2Fredis%2F</url>
    <content type="text"><![CDATA[使用redis 保存文章信息文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。 Redis 没有表的概念将同类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。 Redis UDSLettuce还支持使用Unix Domain Sockets, 这对程序和Redis在同一机器上的情况来说，是一大福音。平时我们连接应用和数据库如Mysql，都是基于TCP/IP套接字的方式，如127.0.0.1:3306，达到进程与进程之间的通信，Redis也不例外。但使用UDS传输不需要经过网络协议栈,不需要打包拆包等操作,只是数据的拷贝过程，也不会出现丢包的情况，更不需要三次握手，因此有比TCP/IP更快的连接与执行速度。当然，仅限Redis进程和程序进程在同一主机上，而且仅适用于Unix及其衍生系统。 123456789101112131415161718private RedisURI createRedisURI() &#123; Builder builder = null;// 判断是否有配置UDS信息，以及判断Redis是否有支持UDS连接方式，是则用UDS，否则用TCP if (StringUtils.isNotBlank(socket) &amp;&amp; Files.exists(Paths.get(socket))) &#123; builder = Builder.socket(socket); System.out.println("connect with Redis by Unix domain Socket"); log.info("connect with Redis by Unix domain Socket"); &#125; else &#123; builder = Builder.redis(hostName, port); System.out.println("connect with Redis by TCP Socket"); log.info("connect with Redis by TCP Socket"); &#125; builder.withDatabase(dbIndex); if (StringUtils.isNotBlank(password)) &#123; builder.withPassword(password); &#125; return builder.build();&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[set]]></title>
    <url>%2F2018%2F03%2F14%2Fset%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930import java.util.*; public static void main(String[] args) &#123; Set&lt;Integer&gt; result = new HashSet&lt;Integer&gt;(); Set&lt;Integer&gt; set1 = new HashSet&lt;Integer&gt;()&#123;&#123; add(1); add(3); add(5); &#125;&#125;; Set&lt;Integer&gt; set2 = new HashSet&lt;Integer&gt;()&#123;&#123; add(1); add(2); add(3); &#125;&#125;; result.clear(); result.addAll(set1); result.retainAll(set2); System.out.println("交集："+result); result.clear(); result.addAll(set1); result.removeAll(set2); System.out.println("差集："+result); result.clear(); result.addAll(set1); result.addAll(set2); System.out.println("并集："+result);]]></content>
      <tags>
        <tag>set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javascript]]></title>
    <url>%2F2018%2F02%2F26%2Fjavascript%2F</url>
    <content type="text"><![CDATA[js 中switch 语句 区分类型1234567891011121314151617function chooseUseRegion() &#123; $('#EffectiveType').change(function() &#123; $('.showdiv').hide(); switch ($(this).val()) &#123; case '1': $('#use_time_2').show(); return; case '2': $('#use_time_1').show(); return; case '0': return; default: return; &#125;; &#125;);&#125; 前端路由实现之 #hash前端路由实现方式 1 使用 h5 提供historyapi 2 使用hash 路由 跨域配置 简单请求 带cookie跨域 origin 不能为 * ajax带cookie 请求 隐藏跨域页面中api地址 /a?b=1&amp;c=2 js map/reduce/filter/sort123456789101112131415161718192021222324252627282930// mapvar arr = [1, 2, 3, 4, 5, 6, 7, 8, 9];var results = arr.map(pow); // [1, 4, 9, 16, 25, 36, 49, 64, 81]console.log(results);// reducevar arr = [1, 3, 5, 7, 9];arr.reduce(function (x, y) &#123; return x + y;&#125;); // 25// filtervar arr = [1, 2, 4, 5, 6, 9, 10, 15];var r = arr.filter(function (x) &#123; return x % 2 !== 0;&#125;);r; // [1, 5, 9, 15]//sortvar arr = [10, 20, 1, 2];arr.sort(function (x, y) &#123; if (x &lt; y) &#123; return -1; &#125; if (x &gt; y) &#123; return 1; &#125; return 0;&#125;);console.log(arr); // [1, 2, 10, 20] html页面注入js12345678910(function(d,s,id)&#123; var js, fjs = d.getElementsByTagName(s)[0]; if (d.getElementById(id))&#123; return; &#125; js = d.createElement(s); js.id = id; js.onload = function()&#123; console.log(11111) &#125;; js.src = "//connect.facebook.net/en_US/sdk.js"; fjs.parentNode.insertBefore(js, fjs);&#125;)(document,'script','facebook-jssdk')]]></content>
      <tags>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm]]></title>
    <url>%2F2018%2F02%2F24%2Fjvm-%E8%AF%8A%E6%96%AD%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[JVM小工具在${JAVA_HOME}/bin/目录下Sun/Oracle给我们提供了一些处理应用程序性能问题、定位故障的工具, 包含 bin 描述 功能 jps 打印Hotspot VM进程 VMID、JVM参数、main()函数参数、主类名/Jar路径 jstat 查看Hotspot VM 运行时信息 类加载、内存、GC[可分代查看]、JIT编译 jinfo 查看和修改虚拟机各项配置 -flag name=value jmap heapdump: 生成VM堆转储快照、查询finalize执行队列、Java堆和永久代详细信息 jmap -dump:live,ormat=b,file=heap.bin [VMID] jstack 查看VM当前时刻的线程快照: 当前VM内每一条线程正在执行的方法堆栈集合 Thread.getAllStackTraces()提供了类似的功能 javap 查看经javac之后产生的JVM字节码代码 自动解析.class文件, 避免了去理解class文件格式以及手动解析 class文件内容 jcmd 一个多功能工具, 可以用来导出堆, 查看Java进程、导出线程信息、 执行GC、查看性能相关数据等 几乎集合了jps、jstat、jinfo、jmap、jstack所有功能 jconsole 基于JMX的可视化监视、管理工具 可以查看内存、线程、类、CPU信息, 以及对JMX MBean进行管理 jvisualvm JDK中最强大运行监视和故障处理工具 可以监控内存泄露、跟踪垃圾回收、执行时内存分析、CPU分析、线程分析… VM常用参数整理 参数 描述 -Xms 最小堆大小 -Xmx 最大堆大小 -Xmn 新生代大小 -XX:PermSize 永久代大小 -XX:MaxPermSize 永久代最大大小 -XX:+PrintGC 输出GC日志 -verbose:gc - -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC时间戳(以基准时间的形式) -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:/path/gc.log 日志文件的输出路径 -XX:+PrintGCApplicationStoppedTime 打印由GC产生的停顿时间 对内存分区情况截图 生产故障JVM 进程cpu 占用率一直100%解决办法( 首先找出问题进程内CPU占用率高的线程 再通过线程栈信息找出该线程当时在运行的问题代码段)jvm负载一直居高不下，先使用了 top -c 查看当前进程详情使用 遍历树时递归结构栈溢出 jvm 参数要修改 -Xssclasspath : String path = this.getClass().getResource(“”).getPath(); Class.forName(xxx.xx.xxx) 返回一个类作用时要求jvm查找并加载指定的类，也就是说JVM会执行该类的静态代码段。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 查看运行的进程jconsole# 查看java堆配置# jmap -heap &lt;pid&gt;&gt; jmap -heap 79376Attaching to process ID 79376, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.144-b01using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2105540608 (2008.0MB) NewSize = 44040192 (42.0MB) MaxNewSize = 701497344 (669.0MB) OldSize = 88080384 (84.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 408420352 (389.5MB) used = 57457288 (54.79553985595703MB) free = 350963064 (334.70446014404297MB) 14.068174545816953% usedFrom Space: capacity = 17301504 (16.5MB) used = 0 (0.0MB) free = 17301504 (16.5MB) 0.0% usedTo Space: capacity = 18350080 (17.5MB) used = 0 (0.0MB) free = 18350080 (17.5MB) 0.0% usedPS Old Generation capacity = 116391936 (111.0MB) used = 40436112 (38.56288146972656MB) free = 75955824 (72.43711853027344MB) 34.741334657411315% used33782 interned Strings occupying 3796288 bytes.# java dump jvm堆内存使用情况 记录内存信息&gt; jmap -dump:format=b,file=dump.bin 79376Dumping heap to E:\Doc\GitRepo\hydra\dump.bin ...Heap dump file created#如果我们只需要将dump中存活的对象导出，那么可以使用:live参数&gt;jmap -dump:live,format=b,file=heapLive.hprof 2576 # thread dumpjstack 79376 &gt; thread.txt heap dump heap dump文件是一个二进制文件，它保存了某一时刻JVM堆中对象使用情况。HeapDump文件是指定时刻的Java堆栈的快照，是一种镜像文件。Heap Analyzer工具通过分析HeapDump文件，哪些对象占用了太多的堆栈空间，来发现导致内存泄露或者可能引起内存泄露的对象。thread dump thread dump文件主要保存的是java应用中各线程在某一时刻的运行的位置，即执行到哪一个类的哪一个方法哪一个行上。thread dump是一个文本文件，打开后可以看到每一个线程的执行栈，以stacktrace的方式显示。通过对thread dump的分析可以得到应用是否“卡”在某一点上，即在某一点运行的时间太长，如数据库查询，长期得不到响应，最终导致系统崩溃。单个的thread dump文件一般来说是没有什么用处的，因为它只是记录了某一个绝对时间点的情况。比较有用的是，线程在一个时间段内的执行情况。 两个thread dump文件在分析时特别有效，困为它可以看出在先后两个时间点上，线程执行的位置，如果发现先后两组数据中同一线程都执行在同一位置，则说明此处可能有问题，因为程序运行是极快的，如果两次均在某一点上，说明这一点的耗时是很大的。通过对这两个文件进行分析，查出原因，进而解决问题。 可视化查看 jvm 内存dump jhat -port 5000 dump.bin java 自带内存/cpu(线程)分析工具jps -vl 展示操作系统里面的java应用 显示pid12345E:\Doc\GitRepo\ioblog\ziggle [master ≡ +0 ~1 -0 !]&gt; jps112292 Launcher48872115272 Jps229016 Main37292 DemoApplication jinfo -flags PID显示 System.getProperties()12345678E:\Doc\GitRepo\ioblog\ziggle [master ≡ +0 ~1 -0 !]&gt; jinfo -sysprops 37292Attaching to process ID 37292, please wait...Debugger attached successfully.Server compiler detected.java.vm.specification.name = Java Virtual Machine SpecificationPID = 37292java.runtime.version = 1.8.0_144-b01........ jstat -gc PID显示JVM的各个内存区使用情况（容量和使用量），GC的次数和耗时。可以通过命令jstat -class PID查看class的加载情况。123E:\Doc\GitRepo\ioblog\ziggle [master ≡ +0 ~1 -0 !]&gt; jstat -gc 37292 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT16384.0 17408.0 0.0 0.0 245760.0 205516.6 138752.0 37613.3 59096.0 58195.3 7640.0 7388.5 13 0.192 3 0.502 0.695 jstack PID查看线程运行情况，检测是否有死锁。 jconsoleJDK提供的一个可视化资源查看，监控工具。 jvisualvmJDK提供的另外一个一站式资源查看，监控，管理工具。支持插件机制，可以自己安装插件，定制jvisualvm。常用的是Visual GC插件。也可以通过该工具dump JVM的堆。也可以导入已经dump出来的堆信息进行分析 动态开启jvm gc 日志123456jinfo -flag +PrintGCDetails &lt;pid&gt;jstat -gcutil &lt;pid&gt; 1000.\jstat -gcutil 15508 5s JVM线程的栈在64位Linux操作系统上的默认大小是多少？1java -XX:+PrintFlagsFinal -version | grep -i &apos;stack&apos;]]></content>
      <tags>
        <tag>-jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bash]]></title>
    <url>%2F2018%2F02%2F07%2Fbash%2F</url>
    <content type="text"><![CDATA[bash 脚本 end of line sequence在windows 中 要把bash脚本换行格式调账为 CRLF -&gt; LF bash 变量类别 局部变量 只在当前shell实例中有效，其他shell启动不能访问局部变量 环境变量 所有的程序包括shell启动的程序都可以访问环境变量，shell也可以定义环境变量 shell变量 单引号的限制 单引号的任何字符都会原样输出，单引号总的变量都是无效的 单引号中不能出现单引号，即使使用转义符 双引号的优点 双引号中可以有变量 双引号中可以出现转移字符 拼接字符串1234567name="ziggle"greeting="hello ,"$name" ! "greetin="hello , $&#123;name&#125; ! "# 提取字符串长度string="abcd"echo $&#123;#string&#125; #输出 4 bash 只支持一维数组 声明 12345arr_name=(value0 value1 value2) # 或者 arr_name=( value1 value2) 读取数组${数组名[下标]}value=${arr_name[n]} 123#!/bin/bashmy_arr=(A B C "D")echo "first ele $&#123;my_arr[0]&#125;" 传递参数在执行脚本时，向脚本传递参数，脚本内获取参数的格式为:$n n 代表一个数字， 1 为执行脚本时传递的第一个参数 … 参数处理 说明 $# 传递到脚本的参数个数 $* 以一个但字符串显示所有向脚本传递的参数 $$ 脚本运行的当前进程ID号 $! 后台运行的最后一个进程ID号 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数 $- 显示shell使用的当前选项，与set 命令功能相同 $? 显示最后命令的退出状态，0表示没错，其他值有错 $USER 运行脚本用户的用户名 $HOSTNAME hostname $SECONDS 脚本运行since开始 的时间长 $LINENO 当前脚本行号 bash中数学运算使用 awk ,exprexpr是表达式计算工具123#!/bin/bashval=`expr 2 + 2`echo " value : $val " bash 使用let进行简单算术运算123456789101112#!/bin/bashlet a=5+4echo $a #9let "a = 5 + 4"echo $a #9let a++echo $a #10let "a = 4 * 5"echo $a # 20 使用双括号计算123#!/bin/basha=$(( 4 + 5 ))echo $a #9 计算变量长度1234567#!/bin/basha='hello world'echo $&#123;#a&#125; #11b=1234echo $&#123;#b&#125; #4 bash 表达式 条件表达式要放在方括号中，并且要有空格， [ a==b ] 123456789#!/bin/basha=12b=23val=`expr $a + $b`if [ $a == $b ]then echo "a equals b"fi if 测试 操作符 说明 ! expression 表达式 is false -n string 字符串长度大于0 -z string 字符串长度等于0 string1 = string2 字符串相等 string1 != string2 字符串不乡等 -d file directory 存在且为文件夹 -e file 文件存在 -r file 文件存在且有读权限 -w file 文件存在且有写权限 -x file 文件存在且有执行权限 -s file 文件存在且文件大小大于0 关系运算符 运算符 说明 举例 -eq 检测两个数是否相当，相等返回true [ $a -eq $b ] 返回 false -ne 检测两个数是否相当，不相等返回true [ $a -ne $b ] 返回 true -gt 检测左边是否大于右边，是返回true [ $a -eq $b ] 返回 false -lt 检测左边是否小于右边，是返回true [ $a -eq $b ] 返回 true -ge 检测左边的数是否大于等于右边，是返回true [ $a -eq $b ] 返回 false -le 检测左边的数是否小于等于右边，如果是返回true [ $a -le $b ] 返回 true 例子1234567891011121314#!/bin/bash# 文件可读 而且大于0if [ -r $1 ] &amp;&amp; [ -s $1 ]then echo this file is usefulfi# if [ $USER == 'nginx' ] || [ $USER == 'root' ]then ls -alhelse lsfi case 语句12345678910111213141516#!/bin/bashcase $1 in start) echo starting ;; stop) echo stoping ;; restart) echo restarting ;; *) echo dont\'t know ;;esac loop 语句 123456789101112131415161718192021222324252627#!/bin/shcounter=1while [ $counter -le 10 ]do echo $counter ((counter++))doneecho all done# Basic until loopcounter=1until [ $counter -gt 10 ]doecho $counter((counter++))doneecho All done# Basic for loopnames='Stan Kyle Cartman'for name in $namesdoecho $namedoneecho All done Range1234567891011121314151617181920212223242526272829303132333435363738# Basic range in for loopfor value in &#123;1..5&#125;doecho $valuedoneecho All done# Basic range with steps for loopfor value in &#123;10..0..2&#125;doecho $valuedoneecho All done# Make a backup set of filesfor value in $1/*doused=$( df $1 | tail -1 | awk '&#123; print $5 &#125;' | sed 's/%//' )if [ $used -gt 90 ]thenecho Low disk space 1&gt;&amp;2breakficp $value $1/backup/done# Make a backup set of filesfor value in $1/*doif [ ! -r $value ]thenecho $value not readable 1&gt;&amp;2continueficp $value $1/backup/done function12345678910111213print_something()&#123; echo hail hydra&#125;print_something# Passing argumentt to a funcprint_something()&#123; echo hail $1&#125;print_something hydra 变量作用范围1234#!/bin/shvar_change()&#123; &#125;]]></content>
      <tags>
        <tag>-bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jquery]]></title>
    <url>%2F2018%2F02%2F07%2Fjquery%2F</url>
    <content type="text"><![CDATA[对img标签值修改 123$('#imageId')[0].src;$('#imageId').attr('src',path) 前端路由 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139var regexps = [ /[\-&#123;&#125;\[\]+?.,\\\^$|#\s]/g, /\((.*?)\)/g, /(\(\?)?:\w+/g, /\*\w+/g,]function extractRoute (route) &#123; var matchs = [] route = route.replace(regexps[0], '\\$&amp;') .replace(regexps[1], '(?:$1)?') .replace(regexps[2], function (match, optional) &#123; if (match) matchs.push(match.replace(':', '')) return optional ? match : '([^/?]+)' &#125;).replace(regexps[3], function (match, optional) &#123; if (match) matchs.push(match.replace('*', '')) return '([^?]*?)' &#125;) return &#123; regexp: new RegExp('^' + route + '(?:\\?([\\s\\S]*))?$'), matchs: matchs &#125;&#125;function extractParams (route, path) &#123; var params = route.exec(path).slice(1) var results = [] for (var i = 0; i &lt; params.length; i++) &#123; results.push(decodeURIComponent(params[i]) || null) &#125; return results&#125;// borwserfunction extractQuery() &#123; var url = location.search var pattern = /(\w+)=([^\?|^\&amp;]+)/ig var query = &#123;&#125; url.replace(pattern, function(a, b, c) &#123; query[b] = c; &#125;) return query;&#125;async function exec() &#123; var match = false for (var i = 0; i &lt; data.routes.length; i++) &#123; var route = extractRoute(data.routes[i].path); if (!route.regexp.test(data.req.path)) &#123; continue &#125; match = true var results = extractParams(route.regexp, data.req.path) data.req.params = data.req.params || &#123;&#125; for (var j = 0; j &lt; route.matchs.length; j++) &#123; data.req.params[route.matchs[j]] = results[j] &#125; await data.routes[i].fn.call(data, data.req, data.res, data.next) &#125; if (!match &amp;&amp; typeof data.next === 'function') await data.next()&#125;// borwserfunction emit() &#123; if (data.req.url === location.href) return data.req.url = location.href data.req.path = location.pathname data.req.query = extractQuery() exec()&#125;var data = &#123;routes: [], resMethods: &#123;&#125;&#125;async function Router(req, res, next) &#123; if (typeof document === 'object') &#123; // browser data.env = 'browser' data.req = &#123;query: &#123;&#125;&#125; data.res = &#123;&#125; window.addEventListener('popstate', emit, false) &#125;else if (next) &#123; // express data.req = req data.req.url = data.req.originalUrl data.res = res data.next = next data.env = 'express' &#125; else &#123; // koa data.ctx = req data.req = data.ctx.request // 替换掉问号后面的参数 data.req.path = data.req.url.replace(/\?.*/g, '') data.res = data.ctx.response data.next = res data.env = 'koa' &#125; for (var m in data.resMethods) &#123; data.res[m] = data.resMethods[m].bind(data) &#125; data.env === 'browser' ? emit() : await exec()&#125;Router.get = function(path, fn) &#123; data.routes.push(&#123; path: path, fn: fn &#125;)&#125;Router.addResMethod = function(name, fn) &#123; data.resMethods[name] = fn&#125;&lt;!-- more --&gt;/** * browser * 需要注意的是调用history.pushState()或history.replaceState()不会触发popstate事件， * 只有在做出浏览器动作时，才会触发该事件，如用户点击浏览器的回退按钮（或者在Javascript代码中调用history.back()） */Router.go = function(path, isReplace) &#123; if (isReplace) &#123; history.replaceState(&#123; path: path &#125;, null, path); &#125; else &#123; history.pushState(&#123; path: path &#125;, null, path); &#125; emit()&#125;// browserRouter.back = function() &#123; history.back()&#125;// browser, 代理链接功能Router.proxyLinks = function(nodes) &#123; for (var i = 0; i &lt; nodes.length; i++) &#123; nodes[i].addEventListener('click', function(e) &#123; Router.go(e.target.href) e.preventDefault() &#125;) &#125;&#125;module.exports = Router model-&gt;view 1art-template 在使用jquery 对select 元素注册事件1234567891011121314151617181920212223242526272829303132var select = document.getElementById('use_time');// 可以对 这个select元素进行select.change(function()&#123; console.log(111); var index = select.selectedIndex; if (index &gt; 0) &#123; console.log(option[index].vlaue); &#125;&#125;)----//如果 var select =$('#use_time');// 对select 绑定change事件时不能用上面的codeselect.on('change',function()&#123; console.log('right');&#125;)----var select = $('#use_time');var option = select.options;select.on('change',function () &#123; console.log(111); var index = select.selectedIndex; if (index &gt; 0) &#123; console.log(option[index].vlaue); &#125;&#125;); 选择input 标签属性1234567891011121314151617$('#table td &gt; input:checked').each((i, j) =&gt; &#123; ids.push(parseInt(j.getAttribute('adid'))) &#125;); &lt;table class="table table-striped table-hover"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class="table-checkbox"&gt; &lt;input id="allUser" control="复选框" type="checkbox" class="group-checkable" data-set=".checkboxes" /&gt; &lt;/th&gt; &lt;th control="序号"&gt;序号&lt;/th&gt; &lt;th control="讲师"&gt;用户&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody id="userTable"&gt;&lt;/tbody&gt; &lt;/table&gt;// 选择 input 上级 tbody 子元素 的input 属性 $(this).parents('thead').next().children().find('input').prop('checked' ,true); 12345678910jQuery.parent(expr) //找父元素jQuery.parents(expr) //找到所有祖先元素，不限于父元素jQuery.children(expr) //查找所有子元素，只会找到直接的孩子节点，不会返回所有子孙jQuery.contents() //查找下面的所有内容，包括节点和文本。jQuery.prev() //查找上一个兄弟节点，不是所有的兄弟节点jQuery.prevAll() //查找所有之前的兄弟节点jQuery.next() //查找下一个兄弟节点，不是所有的兄弟节点jQuery.nextAll() //查找所有之后的兄弟节点jQuery.siblings() //查找兄弟节点，不分前后jQuery.find(expr) //跟jQuery.filter(expr)完全不一样，jQuery.filter(expr)是从初始的 jQuery对象集合中筛选出一部分，而jQuery.find()的返回结果，不会有初始集中 筛选出一部分，而jQuery.find()的返回结果，不会有初始集合中的内容，比如：$("p").find("span")是从元素开始找，等于$("p span")]]></content>
      <tags>
        <tag>jquery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F02%2F04%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker]]></title>
    <url>%2F2018%2F01%2F25%2Fdocker%2F</url>
    <content type="text"><![CDATA[Dockerfile关键字 关键字 含义 FROM base image RUN 执行命令 ADD 添加文件 COPY 拷贝文件 CMD 执行命令 EXPOST 暴露端口 WORKDIR 指定路径 MAINTAINER 维护者 ENV 设定环境变量 ENTRYPOINT 容器入口 USER 指定用户 VOLUME mount point 命令行参数说明|:——|——:||–detach|容器后台运行||–hostname|容器的 hostname||–publish|端口转发规则||–name|容器名称||–restart always|-||–volume|共享目录挂载||–e|配置运行时环境变量|| –rm=false |指定容器停止后自动删除容器(不支持以docker run -d启动的容器)|| –expose=[] |指定容器暴露的端口，即修改镜像的暴露端口|| –dns=[] |指定容器的dns服务器|| –dns-search=[] |指定容器的dns搜索域名，写入到容器的/etc/resolv.conf文件|| –entrypoint=”” | 覆盖image的入口点|| -m | 设置容器使用内存最大值 == –memory=””|| –net=”bridge” | 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型；| 最简实例12345678FROM ubuntuMAINTAINER zRUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.listRUN apt-get updateRUN apt-get install -y nginxCOPY index.html /var/www/htmlENTRYPOINT ["/usr/sbin/nginx","-g","deamon off;"]EXPOSE 80 12345678const port = 9999;let http = require('http');let server = http.createServer((req, res) =&gt; &#123; res.end("hail hydra\n");&#125;)server.listen(port,()=&gt;&#123; console.log(`listen at $&#123;port&#125;`)&#125;) 1234567FROM nodeRUN mkdir -p /home/appWORKDIR /home/appCOPY . /home/appEXPOSE 9999CMD ["node","./server.js"] 1234567# build docker build -t appname .# run docker run -d -p 9999:9999 appname # run container docker start [container-id] 123456789101112131415161718# docker run --detach \# &gt; --env GITLAB_OMNIBUS_CONFIG="gitlab_rails['lfs_enabled'] = true;" \# &gt; --publish 88:80 --publish 222:22 --publish 4433:443 \# &gt; --name gitlab \# &gt; --restart always \# &gt; --volume /root/home/gitlab/config:/etc/gitlab \# &gt; --volume /root/home/gitlab/logs:/var/log/gitlab \# &gt; --volume /root/home/gitlab/data:/var/opt/gitlab \# &gt; --name gitlab gitlab/gitlab-ce:latestdocker run --detach \ --hostname 192.168.192.186 \ --publish 4433:443 --publish 88:80 --publish 2222:22 \ --name gitlab \ --restart always \ --volume /root/home/gitlab/config:/etc/gitlab \ --volume /root/home/gitlab/logs:/var/log/gitlab \ --volume /root/home/gitlab/data:/var/opt/gitlab \ gitlab/gitlab-ce:latest 1234567891011docker logs -f -t --since=&quot;2017-05-31&quot; --tail=10 edu_web_1--since : 此参数指定了输出日志开始日期，即只输出指定日期之后的日志。-f : 查看实时日志-t : 查看日志产生的日期-tail=10 : 查看最后的10条日志。edu_web_1 : 容器名称 三个基本概念 Image 镜像 Container 容器 Repository 仓库 指定endpoint 运行某个镜像1docker run -it --entrypoint /bin/bash example/redis]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-class]]></title>
    <url>%2F2018%2F01%2F19%2Fjava-class%2F</url>
    <content type="text"><![CDATA[JVM类加载机制加载 分配内存 初始化零值 设置对象头 执行&lt;init&gt;方法 类加载检查 执行new指令时, 先去检查这个指令是否能在常量池中定位到这个类的符号引用,并且检查这个符号引用是否已经被加载,解析 初始化过, 如果没有就先执行 验证:确保Class文件的字节流包含的信息是符合当前虚拟机的要求的 分配内存, 初始化零值 设置对象头 类的元数据信息引用, 对象hashcode ,GC分代,锁的级别 执行&lt;init&gt;方法 注意以下几种情况不会执行类初始化： 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。 定义对象数组，不会触发该类的初始化。 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发定义常量所在的类。 通过类名获取 Class 对象，不会触发类的初始化。 通过 Class.forName 加载指定类时，如果指定参数 initialize 为 false 时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否要对类进行初始化。 通过 ClassLoader 默认的 loadClass 方法，也不会触发初始化动作。 类加载器的分类 #### 加载：java虚拟机在加载类的过程中为静态变量分配内存。类变量：static变量在内存中只有一个，存放在方法区，属于类变量，被所有实例所共享销毁：类被卸载时，静态变量被销毁，并释放内存空间。static变量的生命周期取决于类的生命周期类初始化顺序： 静态变量、静态代码块初始化构造函数自定义构造函数结论：想要用static存一个变量，使得下次程序运行时还能使用上次的值是不可行的。因为静态变量生命周期虽然长（就是类的生命周期），但是当程序执行完，也就是该类的所有对象都已经被回收，或者加载类的ClassLoader已经被回收，那么该类就会从jvm的方法区卸载，即生命期终止。 更进一步来说，static变量终究是存在jvm的内存中的，jvm下次重新运行时，肯定会清空里边上次运行的内容，包括方法区、常量区的内容。 要实现某些变量在程序多次运行时都可以读取，那么必须要将变量存下来，即存到本地文件中。常用的数据存取格式：XML、JSON、Propertities类（类似map的键值对）等 ##### 双亲委派加载机制 碰到一个类需要加载时，它们之间是如何协调工作的，即java是如何区分一个类该由哪个类加载器来完成呢。 在这里java采用了委托模型机制，这个机制简单来讲，就是“类装载器有载入类的需求时，会先请示其Parent使用其搜索路径帮忙载入，如果Parent 找不到,那么才由自己依照自己的搜索路径搜索类” 12345678910111213141516171819202122232425public class Test&#123; public static void main(String[] arg)&#123; ClassLoader c = Test.class.getClassLoader(); //获取Test类的类加载器 System.out.println(c); ClassLoader c1 = c.getParent(); //获取c这个类加载器的父类加载器 System.out.println(c1); ClassLoader c2 = c1.getParent();//获取c1这个类加载器的父类加载器 System.out.println(c2); &#125;&#125;/**sun.misc.Launcher$AppClassLoader@73d16e93sun.misc.Launcher$ExtClassLoader@15db9742null */ 成员变量 局部变量 静态变量 定义位置 在类中,方法外 方法中,或者方法的形式参数 在类中,方法外 初始化值 有默认初始化值 无,先定义,赋值后才能使用 有默认初始化值 调用方式 对象调用 —– 对象调用，类名调用 存储位置 堆中 栈中 方法区 生命周期 与对象共存亡 与方法共存亡 与类共存亡 别名 实例变量 —— 类变量 枚举实现的单例类1234567891011121314public enum Singleton&#123; Instance; private String name; Singleton()&#123; this.name="name"; &#125; public static Singleton getInstance()&#123; return Instance; &#125; public String getName()&#123; return this.name; &#125;&#125;]]></content>
      <tags>
        <tag>class</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csharp]]></title>
    <url>%2F2018%2F01%2F15%2Fdotnet%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/// &lt;summary&gt; /// 实体类序列化成xml /// &lt;/summary&gt; /// &lt;param name="enitities"&gt;实体.&lt;/param&gt; /// &lt;param name="headtag"&gt;节点名称 (实体类名)&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; public static string ObjListToXml&lt;T&gt;(List&lt;T&gt; enitities, string headtag) &#123; StringBuilder sb = new StringBuilder(); PropertyInfo[] propinfos = null; //sb.AppendLine("&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;"); sb.AppendLine("&lt;" + headtag + "&gt;"); foreach (T obj in enitities) &#123; //初始化propertyinfo if (propinfos == null) &#123; Type objtype = obj.GetType(); if (objtype != typeof(string) &amp;&amp; objtype != typeof(int)) &#123; propinfos = objtype.GetProperties(); &#125; &#125; sb.AppendLine("&lt;item&gt;"); if (propinfos != null) &#123; foreach (PropertyInfo propinfo in propinfos) &#123; var value = propinfo.GetValue(obj, null); sb.Append("&lt;"); sb.Append(propinfo.Name); sb.Append("&gt;"); sb.Append(value ?? ""); sb.Append("&lt;/"); sb.Append(propinfo.Name); sb.AppendLine("&gt;"); &#125; &#125; else &#123; var value = obj; sb.Append("&lt;i&gt;"); sb.Append(value); sb.Append("&lt;/i&gt;"); &#125; sb.AppendLine("&lt;/item&gt;"); &#125; sb.AppendLine("&lt;/" + headtag + "&gt;"); return sb.ToString(); &#125; Multiple Active Result Sets (MARS)多个活动结果集 (MARS) 是一项用于 SQL Server 的功能，可用来对单个连接执行多个批处理。 如果对 SQL Server 启用了 MARS，使用的每个命令对象将向该连接添加一个会话。 123string connectionString = &quot;Data Source=MSSQL1;&quot; + &quot;Initial Catalog=AdventureWorks;Integrated Security=SSPI;&quot; + &quot;MultipleActiveResultSets=True&quot;;]]></content>
      <tags>
        <tag>cshrap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven]]></title>
    <url>%2F2018%2F01%2F15%2Fmaven%2F</url>
    <content type="text"><![CDATA[pom模板文件123456789101112131415161718192021222324252627282930&lt;?xml version="1.0"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;&lt;!-- 模型版本。maven2.0必须是这样写，现在是maven2唯一支持的版本 --&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- 公司或者组织的唯一标志，并且配置时生成的路径也是由此生成， 如com.winner.trade，maven会将该项目打成的jar包放本地路径：/com/winner/trade --&gt; &lt;groupId&gt;temp.download&lt;/groupId&gt; &lt;!-- 本项目的唯一ID，一个groupId下面可能多个项目，就是靠artifactId来区分的 --&gt; &lt;artifactId&gt;temp-download&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;!-- 打包的机制，如pom,jar, maven-plugin, ejb, war, ear, rar, par，默认为jar --&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;dependencies&gt; &lt;!-- 需要下载什么jar包 添加相应依赖 其余部分无需在意--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpcore&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;!-- 设置指依赖是否可选，默认为false,即子项目默认都继承:为true,则子项目必需显示的引入，与dependencyManagement里定义的依赖类似 --&gt; &lt;optional&gt;&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 为pom定义一些常量，在pom中的其它地方可以直接引用 使用方式 如下 ：$&#123;file.encoding&#125; --&gt; &lt;properties&gt; &lt;file.encoding&gt;UTF-8&lt;/file.encoding&gt; &lt;java.source.version&gt;1.5&lt;/java.source.version&gt; &lt;java.target.version&gt;1.5&lt;/java.target.version&gt; &lt;/properties&gt; &lt;/project&gt; 构建配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151&lt;build&gt; &lt;!-- 产生的构件的文件名，默认值是$&#123;artifactId&#125;-$&#123;version&#125;。 --&gt; &lt;finalName&gt;myPorjectName&lt;/finalName&gt; &lt;!-- 构建产生的所有文件存放的目录,默认为$&#123;basedir&#125;/target，即项目根目录下的target --&gt; &lt;directory&gt;$&#123;basedir&#125;/target&lt;/directory&gt; &lt;!--当项目没有规定目标（Maven2叫做阶段（phase））时的默认值， --&gt; &lt;!--必须跟命令行上的参数相同例如jar:jar，或者与某个阶段（phase）相同例如install、compile等 --&gt; &lt;defaultGoal&gt;install&lt;/defaultGoal&gt; &lt;!--当filtering开关打开时，使用到的过滤器属性文件列表。 --&gt; &lt;!--项目配置信息中诸如$&#123;spring.version&#125;之类的占位符会被属性文件中的实际值替换掉 --&gt; &lt;filters&gt; &lt;filter&gt;../filter.properties&lt;/filter&gt; &lt;/filters&gt; &lt;!--项目相关的所有资源路径列表，例如和项目相关的配置文件、属性文件，这些资源被包含在最终的打包文件里。 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;!--描述了资源的目标路径。该路径相对target/classes目录（例如$&#123;project.build.outputDirectory&#125;）。 --&gt; &lt;!--举个例子，如果你想资源在特定的包里(org.apache.maven.messages)，你就必须该元素设置为org/apache/maven/messages。 --&gt; &lt;!--然而，如果你只是想把资源放到源码目录结构里，就不需要该配置。 --&gt; &lt;targetPath&gt;resources&lt;/targetPath&gt; &lt;!--是否使用参数值代替参数名。参数值取自properties元素或者文件里配置的属性，文件在filters元素里列出。 --&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;!--描述存放资源的目录，该路径相对POM路径 --&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;!--包含的模式列表 --&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;!--排除的模式列表 如果&lt;include&gt;与&lt;exclude&gt;划定的范围存在冲突，以&lt;exclude&gt;为准 --&gt; &lt;excludes&gt; &lt;exclude&gt;jdbc.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;!--单元测试相关的所有资源路径，配制方法与resources类似 --&gt; &lt;testResources&gt; &lt;testResource&gt; &lt;targetPath /&gt; &lt;filtering /&gt; &lt;directory /&gt; &lt;includes /&gt; &lt;excludes /&gt; &lt;/testResource&gt; &lt;/testResources&gt; &lt;!--项目源码目录，当构建项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;sourceDirectory&gt;$&#123;basedir&#125;\src\main\java&lt;/sourceDirectory&gt; &lt;!--项目脚本源码目录，该目录和源码目录不同， &lt;!-- 绝大多数情况下，该目录下的内容会被拷贝到输出目录(因为脚本是被解释的，而不是被编译的)。 --&gt; &lt;scriptSourceDirectory&gt;$&#123;basedir&#125;\src\main\scripts &lt;/scriptSourceDirectory&gt; &lt;!--项目单元测试使用的源码目录，当测试项目的时候，构建系统会编译目录里的源码。该路径是相对于pom.xml的相对路径。 --&gt; &lt;testSourceDirectory&gt;$&#123;basedir&#125;\src\test\java&lt;/testSourceDirectory&gt; &lt;!--被编译过的应用程序class文件存放的目录。 --&gt; &lt;outputDirectory&gt;$&#123;basedir&#125;\target\classes&lt;/outputDirectory&gt; &lt;!--被编译过的测试class文件存放的目录。 --&gt; &lt;testOutputDirectory&gt;$&#123;basedir&#125;\target\test-classes &lt;/testOutputDirectory&gt; &lt;!--项目的一系列构建扩展,它们是一系列build过程中要使用的产品，会包含在running bulid‘s classpath里面。 --&gt; &lt;!--他们可以开启extensions，也可以通过提供条件来激活plugins。 --&gt; &lt;!--简单来讲，extensions是在build过程被激活的产品 --&gt; &lt;extensions&gt; &lt;!--例如，通常情况下，程序开发完成后部署到线上Linux服务器，可能需要经历打包、 --&gt; &lt;!--将包文件传到服务器、SSH连上服务器、敲命令启动程序等一系列繁琐的步骤。 --&gt; &lt;!--实际上这些步骤都可以通过Maven的一个插件 wagon-maven-plugin 来自动完成 --&gt; &lt;!--下面的扩展插件wagon-ssh用于通过SSH的方式连接远程服务器， --&gt; &lt;!--类似的还有支持ftp方式的wagon-ftp插件 --&gt; &lt;extension&gt; &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt; &lt;artifactId&gt;wagon-ssh&lt;/artifactId&gt; &lt;version&gt;2.8&lt;/version&gt; &lt;/extension&gt; &lt;/extensions&gt; &lt;!--使用的插件列表 。 --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;!--在构建生命周期中执行一组目标的配置。每个目标可能有不同的配置。 --&gt; &lt;executions&gt; &lt;execution&gt; &lt;!--执行目标的标识符，用于标识构建过程中的目标，或者匹配继承过程中需要合并的执行目标 --&gt; &lt;id&gt;assembly&lt;/id&gt; &lt;!--绑定了目标的构建生命周期阶段，如果省略，目标会被绑定到源数据里配置的默认阶段 --&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;!--配置的执行目标 --&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;!--配置是否被传播到子POM --&gt; &lt;inherited&gt;false&lt;/inherited&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--作为DOM对象的配置,配置项因插件而异 --&gt; &lt;configuration&gt; &lt;finalName&gt;$&#123;finalName&#125;&lt;/finalName&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptor&gt;assembly.xml&lt;/descriptor&gt; &lt;/configuration&gt; &lt;!--是否从该插件下载Maven扩展（例如打包和类型处理器）， --&gt; &lt;!--由于性能原因，只有在真需要下载时，该元素才被设置成true。 --&gt; &lt;extensions&gt;false&lt;/extensions&gt; &lt;!--项目引入插件所需要的额外依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt;...&lt;/dependency&gt; &lt;/dependencies&gt; &lt;!--任何配置是否被传播到子项目 --&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;!--主要定义插件的共同元素、扩展元素集合，类似于dependencyManagement， --&gt; &lt;!--所有继承于此项目的子项目都能使用。该插件配置项直到被引用时才会被解析或绑定到生命周期。 --&gt; &lt;!--给定插件的任何本地配置都会覆盖这里的配置 --&gt; &lt;pluginManagement&gt; &lt;plugins&gt;...&lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 使用mvn 生成模板项目12ziggle@ziggle MINGW64 /e/code/mvnjava$ mvn archetype:generate -DgroupId=com.zigglle -DartifactId=mymvn 在应用程序用使用多个存储库123456789101112&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;Ibiblio&lt;/id&gt; &lt;name&gt;Ibiblio&lt;/name&gt; &lt;url&gt;http://www.ibiblio.org/maven/&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;PlanetMirror&lt;/id&gt; &lt;name&gt;Planet Mirror&lt;/name&gt; &lt;url&gt;http://public.planetmirror.com/pub/maven/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 依赖scope compile：编译依赖范围，在编译，测试，运行时都需要，依赖范围默认值 test：测试依赖范围，测试时需要。编译和运行不需要，如junit provided：已提供依赖范围，编译和测试时需要。运行时不需要,如servlet-api runtime：运行时依赖范围，测试和运行时需要。编译不需要,例如面向接口编程，JDBC驱动实现jar system：系统依赖范围。本地依赖，不在maven中央仓库，结合systemPath标签使用 依赖排除使用&lt;exclusions&gt;标签下的&lt;exclusion&gt;标签指定GA信息来排除，例如：排除xxx.jar传递依赖过来的yyy.jar 1234567891011&lt;dependency&gt; &lt;groupId&gt;com.xxx&lt;/groupId&gt; &lt;artifactId&gt;xxx&lt;/artifactId&gt; &lt;version&gt;x.version&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.xxx&lt;/groupId&gt; &lt;artifactId&gt;yyy&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; maven 依赖包-SNAPSHOT / release如果在一个项目中，我们依赖了模块A的快照版，还依赖了模块B的正式版本，那么在不更改依赖模块版本号的情况下，我们在进行直接编译打包该项目时：即使本地仓库中已经存在对应版本的依赖模块A，maven还是会自动从镜像服务器上下载最新的依赖模块A的快照版本。而依赖正式版本的模块B，如果本地仓库已经存在该版本的模块B, maven则不会主动去镜像服务器上下载。这也是为什么我们会在本地仓库中快照版本的依赖的目录下会看到带有时间戳的jar包 解决方法 1mvn clean install -U 或者修改本地mvn配置文件的私服配置 123456789101112131415161718192021222324252627282930313233&lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus&lt;/name&gt; &lt;url&gt;http://localhost:8087/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus&lt;/name&gt; &lt;url&gt;http://localhost:8087/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; mvn 构建项目12mvn archetype:generate mvn archetype:gennerate -Dgroupid=公司网址反写+项目名 -DartifactId=项目名-模块名 -Dversion=版本号 -Dpackage=代码所在的包 设置maven 平台编码 win 1set "MAVEN_OPTS=-Duser.language=en -Dfile.encoding=UTF-8" linux 1export MAVEN_OPTS='-Duser.language=en -Dfile.encoding=UTF-8' 下载指定jar jar 1mvn dependency:get -Dartifact=junit:junit:4.12:jar source 1mvn dependency:get -Dartifact=junit:junit:4.12:jar:sources]]></content>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dotnetcore]]></title>
    <url>%2F2018%2F01%2F02%2Fdotnetcore%2F</url>
    <content type="text"><![CDATA[dotent Deploymentself-contained deployment (SCD) 生成是带有运行时dotnet publish -c Release –self-contained -r linux-x64 宿主带有运行时dotnet publish -c Release 判断属性值是否为null 12345678static void CheckNull&lt;T&gt;(Expression&lt;Func&lt;T&gt;&gt; expression)&#123; var name = ((MemberExpression)expression.Body).Member.Name; if(expression.Compile()()==null) &#123; System.Console.WriteLine($"&#123;name&#125; is null"); &#125;&#125; ExceptionQ: Unhandled Exception: System.Net.HttpListenerException: Access is deniedA: netsh http add urlacl url=http://+:11221/ user=everyone]]></content>
      <tags>
        <tag>dotnetcore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[serlization]]></title>
    <url>%2F2018%2F01%2F02%2Fserlization%2F</url>
    <content type="text"><![CDATA[序列化序列化 (Serialization)将对象的状态信息处理为字节流，以便于存储或传输到内存、数据库或文件的一个过程。在序列化期间，对象将其当前状态写入到临时或持久性存储区，主要目的是保存对象的状态。便于日后从该状态中进行还原，创建新的对象，这个过程又称为反序列化]]></content>
  </entry>
  <entry>
    <title><![CDATA[consul]]></title>
    <url>%2F2017%2F12%2F31%2Fconsul%2F</url>
    <content type="text"><![CDATA[consul 启动配置 配置 123456789root@ziggle:~# cat /etc/consul.d/consul.json &#123; "datacenter":"ziggle-dc", "log_level":"INFO", "server":true, "data_dir":"/opt/consul", "bind_addr":"107.182.183.107", "ui":true&#125; 启动命令 1/usr/local/bin/consul agent -config-file /etc/consul.d/consul.json -client=107.107.107.107 管理页面每个数据中心DataCenter 的server推荐3到5 ,所有的server都采用raft一致性算法来确保事务的一致性和线性化,事务修改了集群的状态，且集群的状态保存在每一台server上保证可用性集群配置 一个client 两个server 常用命令command: agent 运行consul agent join 将agent加入consul cluster members 列出consul cluster 集群中的members agent 配置选项 -data-dir指定agent储存状态数据目录 -config-dir -config-file指定service配置文件所在位置 配置文件为json文件 -dev开发调试使用不会持久化数据 -bootstrap-expect该命令通知consul server我们现在准备加入的server节点个数，该参数是为了延迟日志复制的启动直到我们指定数量的server节点成功的加入后启动 -node指定节点在集群中的名称在cluster中唯一,使用机器IP地址 -server指定节点为server -client指定节点为client如果不指定为server 默认为-client -join加入node 进cluster -datacenter指定dc 名默认dc1 -http-addr指定用户接口 包括 http,dns 接口 默认是localhost ,使用 123456789101112131415161718192021spring: cloud: consul: host: localhost port: 8500 discovery: instance-id: $&#123;spring.application.name&#125; health-check-path: /health-check health-check-interval: 10s tags: foo=bar,baz config: enabled: true fail-fast: true prefix: config #consul 自定义配置的文件夹的前缀 默认为config service/"服务名"/"服务tag"/config data-key: configuration # 指定consul配置的配置文件为configuration default-context: $&#123;spring.application.name&#125; # consul配置的配置文件父路径 format: properties acl-token: 59ac2b0e-c500-4c38-8df8-dadfc4505edf application: name: myconfigapp consul 单节点配置 1234567891011121314151617[root@java-web-ci data]# cat /etc/consul/conf/consul.json &#123; "datacenter": "dc-1", "data_dir": "./data", "log_level": "INFO", "node_name": "n01", "server": true, "ui": true, "bind_addr": "0.0.0.0", "client_addr": "0.0.0.0", "bootstrap_expect": 1, "retry_interval": "3s", "raft_protocol": 3, "enable_debug": false, "rejoin_after_leave": true, "enable_syslog": false&#125; acl.json 1234567[root@java-web-ci data]# cat /etc/consul/conf/acl.json &#123; "acl_datacenter": "dc-1", "acl_default_policy": "deny", "acl_down_policy": "deny", "acl_master_token": "TnS03PiBEmvAALZ1fFRW7g==" &#125; consul systemd 12345678910111213141516[root@java-web-ci data]# cat /etc/systemd/system/consul.service [Unit]Description=Consul ServiceAfter=network.target[Service]Type=simple# Another Type option: forkingUser=rootExecStart=/usr/local/bin/consul agent -config-dir=/etc/consul/conf/Restart=alwaysRestartSec=1# Other Restart options: or always, on-abort, etc[Install]WantedBy=multi-user.target 清理不活动的service 依赖库 1npm i request 执行脚本 123456789101112131415161718192021222324252627var request = require('request');request.get(&#123; url: `http://xxx:8500/v1/health/state/critical`, headers: &#123; 'X-Consul-Token': 'hidden' &#125;&#125;, function (e, r, b) &#123; cb(JSON.parse(b))&#125;)function cb(servicesList) &#123; servicesList.forEach(i =&gt; &#123; request.put(&#123; url: `http://xxx:8500/v1/agent/service/deregister/$&#123;i.ServiceID&#125;`, method: 'put', headers: &#123; 'X-Consul-Token': 'hidden' &#125; &#125;, function (e, r, bodby) &#123; console.log('error:', e); // Print the error if one occurred console.log('statusCode:', r &amp;&amp; r.statusCode); // Print the response status code if a response was received &#125;); &#125;)&#125;]]></content>
      <tags>
        <tag>consul</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql]]></title>
    <url>%2F2017%2F12%2F25%2Fmysql%2F</url>
    <content type="text"><![CDATA[开启远程连接(raspberry pi ,ubuntu ) 第一步 12root@rasp:~# vim /etc/mysql/mariadb.conf.d/50-server.cnf# bind-address = 127.0.0.1 第二步进入MySQL 12grant all privileges on *.* to root@"%" identified by "password" with grant option;flush privileges; 第一行命令解释如下，.：第一个代表数据库名；第二个代表表名。这里的意思是所有数据库里的所有表都授权给用户。root：授予root账号。“%”：表示授权的用户IP可以指定，这里代表任意的IP地址都能访问MySQL数据库。“password”：分配账号对应的密码，这里密码自己替换成你的mysql root帐号密码。第二行命令是刷新权限信息，也即是让我们所作的设置马上生效。 完全卸载mysql(Ver 15.1 Distrib 10.1.23-MariaDB)1234sudo apt purge mysql-*sudo rm -rf /etc/mysql/ /var/lib/mysqlsudo apt autoremovesudo apt autoclean 修改表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495create table if not exists emp( name varchar(10), hiredate date, sal decimal(10,2), deptno int(2), primary key(name))# 创建简单索引create INDEX index_name on table_name (column_name)# 创建唯一索引create unique index index_name on table_name (colum_name)# 创建复合索引create index pIndex on persons (lastname, firstname)#删除索引drop index index_name on table_namedesc empalter table emp add column age int(3);alter table emp drop column age;alter table emp change age age1 int(4)alter table emp modify age int(8);alter table emp rename empp;alter table persons alter city set default 'beijing'--dmlinsert into emp(name,hiredate,sal,deptno)values ('ziggle','2000-01-01','100',2);insert into emp (name ,sal) values('foo',99);update emp set sal=199 where name='ziggle';delete from emp where name ='ziggle'select count(1) from emp-- 统计各部门的人士select deptno,count(1) from emp group by deptno;-- 统计各部门的人数，统计总人数select deptno ,count(1) from emp group by deptno with roll up;-- 统计人数大于1的部门 select deptno,count(1) from emp group by deptno having count(1) &gt;1select sum(sal) ,max(sal),man(sal) from emp;create table dept ( deptno int(10), deptname varchar(50))select name ,deptname from emp ,dept where emp.deptno = dep.deptno-- 左连接 ：包含所有左边的记录甚至是右边表中没有和它匹配的几率-- 右连接select name ,deptname from dept right join emp on dept.deptno=emp.deptno--子查询当我们查询的时候，需要的条件是另外一个 select 语句的结果，这个时候，就要用到子查询。用于子查询的关键字主要包括 in、not in、=、!=、exists、not exists 等 select * from emp where deptno in (select deptno from dept)select * from emp where deptno = (select deptno from dept limit 1)-- 把子查询转为表连接select * from emp where deptno in (select deptno from dept)select emp.* from emp,dept where emp.deptnp =dept.deptnoselect * from deptunion allselect * from emp--去重select * from deptunionselect * from emp sql create view1234create view view_name as select column_name (s)from table_name where condition create user首先需要先创建用户, then 添加权限 到指定dbs/tables12create user 'user'@'localhost' identified by 'password' -- connect from localhostcreate user 'user'@'%' identified by 'password' -- connect from local machine only add privileges12grant select ,insert ,update on dabasename.* to 'username'@'localhost' grant all on *.* to 'userName'@'localhost' with grant option insert on duplicate key update插入唯一key更新12345678910111213141516171819CREATE TABLE iodku ( id INT AUTO_INCREMENT NOT NULL, name VARCHAR(99) NOT NULL, misc INT NOT NULL, PRIMARY KEY(id), UNIQUE(name)) ENGINE=InnoDB;INSERT INTO iodku (name, misc) VALUES ('Leslie', 123), ('Sally', 456); INSERT INTO iodku (name, misc) VALUES ('Sally', 3333) -- should update ON DUPLICATE KEY UPDATE -- `name` will trigger "duplicate key" id = LAST_INSERT_ID(id), misc = VALUES(misc);SELECT LAST_INSERT_ID(); -- picking up existing value insert ignore existing rows插入忽略已经存在的行1insert ignore into `people` (`id`, `name`)values('1','anni'),('2','anna'); 123456789101112CREATE TABLE iodku ( id INT AUTO_INCREMENT NOT NULL, name VARCHAR(99) NOT NULL, misc INT NOT NULL, PRIMARY KEY(id), UNIQUE(name)) ENGINE=InnoDB;INSERT INTO iodku (name, misc) VALUES ('Leslie', 123), ('Sally', 456); insert select12345INSERT INTO `tableA` (`field_one`, `field_two`) SELECT `tableB`.`field_one`, `tableB`.`field_two` FROM `tableB` WHERE `tableB`.clmn &lt;&gt; 'someValue' ORDER BY `tableB`.`sorting_clmn`; delete vs Truncatetruncate 会重置 AUTO_INCREMENT index 它比delete from 快在处理大量数据 any_value()1234567SELECT item.item_id, ANY_VALUE(uses.tag) tag, COUNT(*) number_of_uses FROM item JOIN uses ON item.item_id, uses.item_idGROUP BY item.item_idselect username ,any_value(password) from stack GROUP BY username sql join z mysql 查看系统变量1234show GLOBAL VARIABLESshow variablesshow local variablesshow session variables mysql 管理 正在执行的查询1show full processlist]]></content>
      <tags>
        <tag>-mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[auto-deploy]]></title>
    <url>%2F2017%2F12%2F25%2Fauto-deploy%2F</url>
    <content type="text"><![CDATA[systemd 守护进程 node webhook进程 同步接受GitHub post请求123456789101112131415root@ziggle:~# cat /lib/systemd/system/nsync.service[Unit]Description=Sync blog node service[Service]ExecStart=-/usr/bin/node /root/wp/www/nsyncblog.jsExecReload=-/bin/kill -HUP $MAINPIDTimeoutSec=10sType=simpleKillMode=processRestart=on-failureRestartSec=20s[Install]WantedBy=multi-user.target]]></content>
      <tags>
        <tag>-devops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql]]></title>
    <url>%2F2017%2F12%2F23%2Fsql%2F</url>
    <content type="text"><![CDATA[sql 执行顺序 from -&gt; where -&gt; group by -&gt; having -&gt; select -&gt; order byselect子句可以识别 别名 使用cast函数转换数据类型 12SELECT name + CAST(gender as VARCHAR(10) ) ,ageFROM dbo.student 使用* 查询结果 123select bookname ,quantity,book_price ,quantity*book_price as total_pricefrom bookitemorder by bookname 使用case进行条件查询 123456789select name,time,redit=case when time&gt;=40 then 5 when time&gt;=30 then 4 when time&gt;=20 then 3 else 2endfrom courseorder by redit sql函数 字符串运算符 123ASCII,CHAR,LOWER,UPPER,LTRIM,RTRIM,LEFT,RIGHT,SUBSTRING,CHARINDEX,REPLICATE,REVERSE,REPLACE 将结果转成大写 12select upper(bookname) as name ,quantity,book_pricefrom bookitem 去空格 12select rtrim(name) + rtrim(dname) as info ,agefrom teacher 算术运算符 ABA,SIGN(调试参数的正负号),CEILING(最大),FLOORROUND(四舍五入),COS,PI,RAND(0-1 之间浮点数)1select rand() 时间函数 DAY,MONTH,YEAR,DATEADD,DATEDIFF,DATENAME,DATEPART,GETDATE-CONVERT 函数转换日期时间123CONVERT (data_type [(length), expression,style)select CONVERT(VARCHAR ,GETDATE(),8) AS now_time 聚合与分组 SUM,MAX,MIN,AVG(忽略null),COUNT聚合函数处理的是数据组 求和函数只能对数字列进行求和 12select sum(column_name)from table_name 计数函数 count(*) 计算表中行总是，即使为null，也计算在内 count(column) 计算column包含的行的数码，不含null行12select count(*) as totalcountfrom table_name 求均值 12select avg ([all/distinct] column_name)from table_name 1234567891011CREATE TABLE [dbo].[Learn_CouponSendJob] ([Id] bigint NOT NULL PRIMARY KEY ,[UserId] bigint NULL ,[NickName] varchar(255) COLLATE Chinese_PRC_CI_AS NULL ,[AddTime] datetime2(7) NULL ,[IsDeleted] bit NULL ,[SendType] int NULL ,[SendId] int NULL ,[CouponId] int NOT NULL ,[IsNotice] bit NULL ) union (会把相同记录合并)去多个select 结果集的合并 union 中select 语句必须有相同数量的列,列的数据类型要相似(可以转换) 每条select 的列的顺序必须相同 12345-- 从已有表创建表结构 select * into b from person where 1&lt;&gt;1-- 插入 数据 insert into b select * from person mysql 默认事务隔离级别 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 不可重复读（read-committed） 否 是 是 SQL server 默认级别 可重复读（repeatable-read） 否 否 是 mysql 默认级别 串行化（serializable） 否 否 否 1234567mysql&gt; select @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+1 row in set (0.00 sec) set session transaction isolation level read committed; start transaction ; commit; 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，鱼和熊掌不可兼得啊。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed，它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 123456789101112131415161718CREATE PROCEDURE proc_test( @name VARCHAR(12), @money int output)ASBEGINif(@name = '1') set @money=10000ELSE SET @money=2ENDDECLARE @m INTPRINT( CAST( ISNULL(@m ,111) as VARCHAR) + ' a')exec proc_test @name='1' ,@money=@m outputPRINT(CAST(@m as VARCHAR) + ' b') 获取今天 开始/结束时间12345678910111213SELECT COUNT (1)FROM learn_cutdownlog WITH (nolock)WHERE iscaptain = 0AND userid = 307AND addtime &gt;CONVERT(DATETIME,CONVERT(CHAR(10), DATEADD(DAY,-0,GETDATE()),120) + ' 00:00:00',120)AND CONVERT(DATETIME,CONVERT(CHAR(10), GETDATE(),120) + ' 23:59:59',120) &gt; addtimeSELECT CONVERT(DATETIME,CONVERT(CHAR(10), DATEADD(DAY,-0,GETDATE()),120) + ' 00:00:00',120);SELECT CONVERT(DATETIME,CONVERT(CHAR(10), GETDATE(),120) + ' 23:59:59',120); 删除sql server 数据库123456use masterGOalter database test set single_user with rollback immediateGODROP DATABASE testGO csql bulkcopy123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108static void Main() &#123; string connectionString = GetConnectionString(); // Open a sourceConnection to the AdventureWorks database. using (SqlConnection sourceConnection = new SqlConnection(connectionString)) &#123; var sw = new Stopwatch(); sw.Start(); sourceConnection.Open(); // Perform an initial count on the destination table. SqlCommand commandRowCount = new SqlCommand($@"SELECT count(1) FROM Learn_User u WITH (NOLOCK) WHERE u.PayCnt = 0", sourceConnection); long countStart = System.Convert.ToInt32( commandRowCount.ExecuteScalar()); Console.WriteLine("Starting row count = &#123;0&#125;", countStart); // Get data from the source table as a SqlDataReader. SqlCommand commandSourceData = new SqlCommand($@"SELECT u.Id AS UserId, NickName, 80 AS SendId, 3 AS CouponId, 1 AS IsNotice , GETDATE() AS SendTime, 1 AS SendStatus , GetDate() AS AddTime, 0 AS IsDeleted, - 1 AS SendToId FROM Learn_User u WITH (NOLOCK) WHERE u.PayCnt = 0", sourceConnection); SqlDataReader reader = commandSourceData.ExecuteReader(); // Open the destination connection. In the real world you would // not use SqlBulkCopy to move data from one table to the other // in the same database. This is for demonstration purposes only. using (SqlConnection destinationConnection = new SqlConnection(connectionString)) &#123; destinationConnection.Open(); // Set up the bulk copy object. // Note that the column positions in the source // data reader match the column positions in // the destination table so there is no need to // map columns. using (SqlBulkCopy bulkCopy = new SqlBulkCopy(destinationConnection)) &#123; bulkCopy.DestinationTableName ="dbo.Learn_CouponSendJob"; bulkCopy.ColumnMappings.Add(0, 1); bulkCopy.ColumnMappings.Add(1, 2); bulkCopy.ColumnMappings.Add(2, 3); bulkCopy.ColumnMappings.Add(3, 4); bulkCopy.ColumnMappings.Add(4, 5); bulkCopy.ColumnMappings.Add(5, 6); bulkCopy.ColumnMappings.Add(6, 7); bulkCopy.ColumnMappings.Add(7, 8); bulkCopy.ColumnMappings.Add(8, 9); bulkCopy.ColumnMappings.Add(9, 10); bulkCopy.BulkCopyTimeout = 30 * 10; try &#123; // Write from the source to the destination. bulkCopy.WriteToServer(reader); &#125; catch (Exception ex) &#123; Console.WriteLine(ex.Message); &#125; finally &#123; // Close the SqlDataReader. The SqlBulkCopy // object is automatically closed at the end // of the using block. reader.Close(); &#125; &#125; sw.Stop(); Console.WriteLine(" elaps : " + sw.ElapsedMilliseconds/100 + " s"); // Perform a final count on the destination // table to see how many rows were added. long countEnd = System.Convert.ToInt32( commandRowCount.ExecuteScalar()); Console.WriteLine("Ending row count = &#123;0&#125;", countEnd); Console.WriteLine("&#123;0&#125; rows were added.", countEnd - countStart); Console.WriteLine("Press Enter to finish."); Console.ReadLine(); &#125; &#125; &#125; private static string GetConnectionString() // To avoid storing the sourceConnection string in your code, // you can retrieve it from a configuration file. &#123; return $@"Server=d01.xueanquan.cc;Database=LearnOnline;User Id=dev;Password=0F29FF9D-9D91-4420-B2F6-1BFC4B01D6BF;MultipleActiveResultSets=true;"; &#125; 如何解决sqlserver数据库cpu突然上涨 找到当前数据库正在执行的query 123456789SELECT sqltext.TEXT,req.session_id,req.status,req.command,req.cpu_time,req.total_elapsed_timeFROM sys.dm_exec_requests reqCROSS APPLY sys.dm_exec_sql_text(sql_handle) AS sqltextorder by req.cpu_time desc 结束正在执行的占用CPU时间长的query 1kill [session_id] sqlserver 诊断cpu 相关sql####CPU相关视图12345678910111213141516--SELECT * FROM sys.dm_os_sys_info SELECT * FROM sys.dm_exec_sessions SELECT * FROM sys.sysprocesses SELECT * FROM sys.dm_os_tasks SELECT * FROM sys.dm_os_workers SELECT * FROM sys.dm_os_threads SELECT * FROM sys.dm_os_schedulers SELECT * FROM sys.dm_os_memory_objects SELECT * FROM sys.dm_os_nodes SELECT * FROM sys.dm_os_memory_nodes exec sp_configure 'max degree of parallelism'--系统默认并行度 exec sp_configure 'cost threshold for parallelism' --并发阈值 exec sp_configure 'max worker threads'--系统最大工作线程数 exec sp_configure 'affinity mask' --CPU关联 数据库系统 cpu，线程 数量1234select max_workers_count,scheduler_count,cpu_count,hyperthread_ratio ,(hyperthread_ratio/cpu_count) AS physical_cpu_count ,(max_workers_count/scheduler_count) AS workers_per_scheduler_limit from sys.dm_os_sys_info 执行的线程所遇到的所有等待的相关信息12345678910111213141516171819SELECT TOP 10 wait_type,waiting_tasks_count,signal_wait_time_ms FROM sys.dm_os_wait_stats ORDER BY signal_wait_time_ms DESC ``` #### 正在等待某些资源的任务的等待队列的信息 ```sqlSELECT TOP 10 wait_type,wait_duration_ms,session_id,blocking_session_id FROM sys.dm_os_waiting_tasks ORDER BY wait_duration_ms DESC ``` #### CPU或调度器当前分配的工作情况 ```sqlSELECT scheduler_id,cpu_id,status,is_idle ,current_tasks_count AS 当前任务数 --在等待或运行的任务 ,runnable_tasks_count AS 等待调度线程数 --已分配任务并且正在可运行队列中 ,current_workers_count AS 当前线程数 --相关或未分配任何任务的工作线程 ,active_workers_count AS 活动线程数 --在运行、可运行或挂起 ,work_queue_count AS 挂起任务数 --等待工作线程执行 FROM sys.dm_os_schedulers WHERE scheduler_id &lt; 255 当前线程数12345678910111213141516171819202122232425262728293031323334select COUNT(*) as 当前线程数 from sys.dm_os_workers --非SQL server create的threads select * from sys.dm_os_threads where started_by_sqlservr=0 --即scheduler_id &gt; 255 --有task 等待worker去执行 select * from sys.dm_os_tasks where task_state='PENDING' --计数器 select * from sys.dm_os_performance_counters where object_name='SQLServer:SQL Statistics' select * from sys.dm_os_performance_counters where object_name='SQLServer:Plan Cache' --1. 实例累积的信号（线程/CPU）等待比例是否严重 SELECT CAST(100.0 * SUM(signal_wait_time_ms) / SUM (wait_time_ms) AS NUMERIC(20,2)) AS [%signal (cpu) waits], CAST(100.0 * SUM(wait_time_ms - signal_wait_time_ms) / SUM (wait_time_ms) AS NUMERIC(20,2)) AS [%resource waits] FROM sys.dm_os_wait_stats WITH (NOLOCK) OPTION (RECOMPILE); --2. SqlServer各等待类型的线程等待信息 SELECT TOP 20 wait_type,waiting_tasks_count ,wait_time_ms,signal_wait_time_ms ,wait_time_ms - signal_wait_time_ms AS resource_wait_time_ms ,CONVERT(NUMERIC(14,2),100.0 * wait_time_ms /SUM (wait_time_ms ) OVER( )) AS percent_total_waits ,CONVERT(NUMERIC(14,2),100.0 * signal_wait_time_ms /SUM (signal_wait_time_ms) OVER( )) AS percent_total_signal_waits ,CONVERT(NUMERIC(14,2),100.0 * ( wait_time_ms - signal_wait_time_ms )/SUM (wait_time_ms ) OVER( )) AS percent_total_resource_waits FROM sys .dm_os_wait_stats WHERE wait_time_ms &gt; 0 ORDER BY percent_total_signal_waits DESC --3. 闩锁(latch)等待的信息 select top 20 latch_class,waiting_requests_count,wait_time_ms,max_wait_time_ms from sys.dm_os_latch_stats order by wait_time_ms desc 使用最多处理器时间的用户数据库123456789101112131415;WITH DB_CPU_Stats AS ( SELECT DatabaseID, DB_Name(DatabaseID) AS [DatabaseName], SUM(total_worker_time) AS [CPU_Time_Ms] FROM sys.dm_exec_query_stats AS qs CROSS APPLY (SELECT CONVERT(int, value) AS [DatabaseID] FROM sys.dm_exec_plan_attributes(qs.plan_handle) WHERE attribute = N'dbid') AS F_DB GROUP BY DatabaseID) SELECT ROW_NUMBER() OVER(ORDER BY [CPU_Time_Ms] DESC) AS [row_num], DatabaseName , [CPU_Time_Ms], CAST([CPU_Time_Ms] * 1.0 / SUM([CPU_Time_Ms])OVER() * 100.0 AS DECIMAL(5,2)) AS [CPUPercent] FROM DB_CPU_Stats WHERE DatabaseID &gt; 4 -- system databases AND DatabaseID &lt;&gt; 32767 -- ResourceDB ORDER BY row_num OPTION (RECOMPILE); 缓存中最耗CPU的语句123456789101112select total_cpu_time,total_execution_count,number_of_statements,[text] from ( select top 20 sum(qs.total_worker_time) as total_cpu_time, sum(qs.execution_count) as total_execution_count, count(*) as number_of_statements, qs.plan_handle from sys.dm_exec_query_stats qs group by qs.plan_handle order by total_cpu_time desc ) eqs cross apply sys.dm_exec_sql_text(eqs.plan_handle) as est order by total_cpu_time desc 当前正在执行的语句12345678910111213141516171819202122232425262728293031323334353637383940SELECT der.[session_id],der.[blocking_session_id], sp.lastwaittype,sp.hostname,sp.program_name,sp.loginame, der.[start_time] AS '开始时间', der.[status] AS '状态', der.[command] AS '命令', dest.[text] AS 'sql语句', DB_NAME(der.[database_id]) AS '数据库名', der.[wait_type] AS '等待资源类型', der.[wait_time] AS '等待时间', der.[wait_resource] AS '等待的资源', der.[reads] AS '物理读次数', der.[writes] AS '写次数', der.[logical_reads] AS '逻辑读次数', der.[row_count] AS '返回结果行数' FROM sys.[dm_exec_requests] AS der INNER JOIN master.dbo.sysprocesses AS sp on der.session_id=sp.spid CROSS APPLY sys.[dm_exec_sql_text](der.[sql_handle]) AS dest WHERE [session_id]&gt;50 AND session_id&lt;&gt;@@SPID AND DB_NAME(der.[database_id])='platform' ORDER BY [cpu_time] DESC --实例级最大的瓶颈 WITH Waits AS ( SELECT wait_type , wait_time_ms / 1000. AS wait_time_s , 100.* wait_time_ms / SUM(wait_time_ms) OVER ( ) AS pct , ROW_NUMBER() OVER ( ORDER BY wait_time_ms DESC ) AS rn FROM sys.dm_os_wait_stats WHERE wait_type NOT IN ( 'CLR_SEMAPHORE', 'LAZYWRITER_SLEEP', 'RESOURCE_QUEUE', 'SLEEP_TASK', 'SLEEP_SYSTEMTASK', 'SQLTRACE_BUFFER_FLUSH', 'WAITFOR', 'LOGMGR_QUEUE', 'CHECKPOINT_QUEUE', 'REQUEST_FOR_DEADLOCK_SEARCH', 'XE_TIMER_EVENT', 'BROKER_TO_FLUSH', 'BROKER_TASK_STOP', 'CLR_MANUAL_EVENT', 'CLR_AUTO_EVENT', 'DISPATCHER_QUEUE_SEMAPHORE', 'FT_IFTS_SCHEDULER_IDLE_WAIT', 'XE_DISPATCHER_WAIT', 'XE_DISPATCHER_JOIN' ) ) SELECT W1.wait_type , CAST(W1.wait_time_s AS DECIMAL(12, 2)) AS wait_time_s , CAST(W1.pct AS DECIMAL(12, 2)) AS pct ,CAST(SUM(W2.pct) AS DECIMAL(12, 2)) AS running_pct FROM Waits AS W1 INNER JOIN Waits AS W2 ON W2.rn &lt;= W1.rn GROUP BY W1.rn , W1.wait_type , W1.wait_time_s , W1.pct HAVING SUM(W2.pct) - W1.pct &lt; 95 ; -- percentage threshold 获取数据库服务器CPU核数12345678910111213141516---SQL 3:获取数据库服务器CPU核数(适用于所有版本)CREATE TABLE #TempTable ( [Index] VARCHAR(2000) , [Name] VARCHAR(2000) , [Internal_Value] VARCHAR(2000) , [Character_Value] VARCHAR(2000) );INSERT INTO #TempTable EXEC xp_msver;SELECT Internal_Value AS VirtualCPUCountFROM #TempTableWHERE Name = 'ProcessorCount';DROP TABLE #TempTable;GO 获取数据库服务器磁盘详情12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455DECLARE @Result INT;DECLARE @objectInfo INT;DECLARE @DriveInfo CHAR(1);DECLARE @TotalSize VARCHAR(20);DECLARE @OutDrive INT;DECLARE @UnitMB BIGINT;DECLARE @FreeRat FLOAT;SET @UnitMB = 1048576;--创建临时表保存服务器磁盘容量信息CREATE TABLE #DiskCapacity([DiskCD] CHAR(1) ,FreeSize INT ,TotalSize INT );INSERT #DiskCapacity([DiskCD], FreeSize ) EXEC master.dbo.xp_fixeddrives;EXEC sp_configure 'show advanced options', 1RECONFIGURE WITH OVERRIDE;EXEC sp_configure 'Ole Automation Procedures', 1;RECONFIGURE WITH OVERRIDE;EXEC @Result = master.sys.sp_OACreate 'scripting.FileSystemObject',@objectInfo OUT;DECLARE CR_DiskInfo CURSOR LOCAL FAST_FORWARDFOR SELECT DiskCD FROM #DiskCapacityORDER by DiskCDOPEN CR_DiskInfo;FETCH NEXT FROM CR_DiskInfo INTO @DriveInfoWHILE @@FETCH_STATUS=0BEGINEXEC @Result = sp_OAMethod @objectInfo,'GetDrive', @OutDrive OUT, @DriveInfoEXEC @Result = sp_OAGetProperty @OutDrive,'TotalSize', @TotalSize OUTUPDATE #DiskCapacitySET TotalSize=@TotalSize/@UnitMBWHERE DiskCD=@DriveInfoFETCH NEXT FROM CR_DiskInfo INTO @DriveInfoENDCLOSE CR_DiskInfoDEALLOCATE CR_DiskInfo;EXEC @Result=sp_OADestroy @objectInfoEXEC sp_configure 'show advanced options', 1RECONFIGURE WITH OVERRIDE;EXEC sp_configure 'Ole Automation Procedures', 0;RECONFIGURE WITH OVERRIDE;EXEC sp_configure 'show advanced options', 0RECONFIGURE WITH OVERRIDE;SELECT DiskCD AS [Drive CD] , STR(TotalSize*1.0/1024,6,2) AS [Total Size(GB)] , STR((TotalSize - FreeSize)*1.0/1024,6,2) AS [Used Space(GB)] , STR(FreeSize*1.0/1024,6,2) AS [Free Space(GB)] , STR(( TotalSize - FreeSize)*1.0/(TotalSize)* 100.0,6,2) AS [Used Rate(%)] , STR(( FreeSize * 1.0/ ( TotalSize ) ) * 100.0,6,2) AS [Free Rate(%)]FROM #DiskCapacity;DROP TABLE #DiskCapacity; 常用sys_procedure sp_who (需要 View server status ) 结束正在指定语句sp_who ‘active’kill {SPID value} 常见问题的解法1https://docs.microsoft.com/zh-cn/sql/relational-databases/system-catalog-views/querying-the-sql-server-system-catalog-faq?view=sql-server-2017#_FAQ6 重命名表属性12EXEC sp_rename 'Sales.SalesTerritory.TerritoryID', 'TerrID', 'COLUMN'; -- EXEC sp_rename 'table.column', 'expect column name', 'COLUMN'; grant 存储过程权限12GRANT EXECUTE ON dbo.procname TO username;select 'GRANT EXECUTE ON '+name+' TO wangping;' from sys.procedures]]></content>
      <tags>
        <tag>-sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vimdiff]]></title>
    <url>%2F2017%2F12%2F22%2Fvimdiff%2F</url>
    <content type="text"><![CDATA[git 全局配置位置 文件内容1234567891011121314[filter "lfs"] clean = git-lfs clean -- %f smudge = git-lfs smudge -- %f process = git-lfs filter-process required = true[user] name = ziggle email = muyue1125@gmail.com# [https]# proxy = https://127.0.0.1:8087# [http]# proxy = http://127.0.0.1:8087[merge] tool = vimdiff #vim diff tools 配置git diff 工具 git config –global merge.tool 使用vim diff git mergetool 位置 含义 代指 ↖ local 文件 LOCAL / LO ↑ base 基部文件 nil ↗ remote 远程文件 REMOTE /RE ↓ 显示merge的内容 nil 命令123456[c :转到上一个冲突]a :转到下一个冲突:-qa :全部退出vim:diffget LOCAL 接受本地修改:diffget REMOTE 接受远程修改 使用:buffes 确定4个窗口编号]]></content>
      <tags>
        <tag>-vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hooktest]]></title>
    <url>%2F2017%2F12%2F15%2Fhooktest%2F</url>
    <content type="text"><![CDATA[deploy node webhook script通过GitHub webhook 同步博客 12345678910var http = require('http');var exec = require('child_process').exec;var server = http.createClient((req,res)=&gt;&#123; if(req.url==='webhooks/push')&#123; exec("") console.log(123); &#125;&#125;);server.listen(4000)]]></content>
      <tags>
        <tag>autodeploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-basic]]></title>
    <url>%2F2017%2F12%2F15%2Fjava-basic%2F</url>
    <content type="text"><![CDATA[Class.forName() 返回一个类JVM会加载制定的类,并执行类的静态代码段 A a = (A)Class.forName(“pacage.A”).newInstance();A a = new A()通过包名和类名,实例对象 String className = readfromXMLConfig;class c = Class.forName(className);factory = (Exampleinterface)c.newInstence(); 使用newInstance() 要保证: 类已经加载 类已经连接 使用Class类的静态方法forName 完成这两步 newInstance(): 弱类型,低效率,只能调用无参构造。 new: 强类型,相对高效,能调用任何public构造。 Class.forName(“”)返回的是类。 Class.forName(“”).newInstance()返回的是object 。ReentrantLockReentrantLock是一个基于AQS的可重入的互斥锁，公平锁将确保等待时间最长的线程优先获取锁，将会使整体的吞吐量下降非公平锁将不能确定哪一个线程将获取锁，可能会导致某些线程饥饿。 12345678910111213public class ReentrantLockTest &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void doSomething() &#123; lock.lock(); // block until condition holds try &#123; // ... method body &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; java 静态代理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class ProxyDemo &#123; public static void main(String[] args) &#123; //1.创建委托对象 AbsSubject real = new RealSub(); //2.创建调用处理器对象 ProxyHandler handler = new ProxyHandler(real); //3.动态生成代理对象 AbsSubject proxySub = (AbsSubject)Proxy.newProxyInstance(real.getClass().getClassLoader(), real.getClass().getInterfaces(), handler); //4.通过代理对象调用方法 proxySub.doJob(); proxySub.sum(3, 9); int m = proxySub.multiply(3, 7); System.out.println("multiply result is:"+m); &#125;&#125;//被代理类的接口interface AbsSubject &#123; void doJob(); void sum(int a, int b); int multiply(int a, int b);&#125;//实际的被代理类class RealSub implements AbsSubject &#123; @Override public void doJob() &#123; // TODO Auto-generated method stub System.out.println("i am doing something"); &#125; @Override public void sum(int a, int b) &#123; System.out.println(a+" + "+b+" = "+(a+b)); &#125; @Override public int multiply(int a, int b) &#123; // TODO Auto-generated method stub System.out.println(a+" * "+ b); return a*b; &#125; &#125;//动态代理的内部实现,调用处理器类，即实现 InvocationHandler 接口//这个类的目的是指定运行时将生成的代理类需要完成的具体任务（包括Preprocess和Postprocess）//即代理类调用任何方法都会经过这个调用处理器类class ProxyHandler implements InvocationHandler &#123; private Object realSub; public ProxyHandler(Object object) &#123; realSub = object; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("before"); Object res = method.invoke(realSub, args); System.out.println("after"); return res; &#125; &#125; Stream(Java 8)1234List&lt;String&gt; lists =Arrays.asList("a","b","c");Stream&lt;String&gt; streamList = lists.stream();lists = streamList.distinct().finter(str-&gt;!str.equals("a").sorted(String::CompareTo).collect(ollectors.toList());lists.forEach(System.out::println); 创建Stream 集合 Stream: stream() 数组 12345678//lambdaCollections.sort(strList, (s1, s2)-&gt;s1.compareTo(s2));//方法引用Collections.sort(strList, String::compareTo);//lambdastrList.forEach(x-&gt;System.out.println(x));//方法引用strList.forEach(System.out::println); 方法引用的语法 对象::实例方法=&gt;等价于”提供方法参数的lambda表达式” 类::静态方法=&gt;等价于”提供方法参数的lambda表达式” eg. System.out::println等价于x-&gt;System.out.println(x) 类::实例方法=&gt;第一个参数是执行方法的对象 eg. String::compareTo等价于(s1, s2)-&gt;s1.compareTo(s2) aop 使用场景Authentication 权限 Caching 缓存 Context passing 内容传递 Error handling 错误处理 Lazy loading 懒加载 Debugging 调试 logging, tracing, profiling and monitoring 记录跟踪 优化 校准 Performance optimization 性能优化 Persistence 持久化 Resource pooling 资源池 Synchronization 同步 Transactions 事务 AOP相关概念方面（Aspect）：一个关注点的模块化，这个关注点实现可能另外横切多个对象。事务管理是J2EE应用中一个很好的横切关注点例子。方面用spring的 Advisor或拦截器实现。 连接点（Joinpoint）: 程序执行过程中明确的点，如方法的调用或特定的异常被抛出。 通知（Advice）: 在特定的连接点，AOP框架执行的动作。各种类型的通知包括“around”、“before”和“throws”通知。通知类型将在下面讨论。许多AOP框架包括Spring都是以拦截器做通知模型，维护一个“围绕”连接点的拦截器链。Spring中定义了四个advice: BeforeAdvice, AfterAdvice, ThrowAdvice和DynamicIntroductionAdvice 切入点（Pointcut）: 指定一个通知将被引发的一系列连接点的集合。AOP框架必须允许开发者指定切入点：例如，使用正则表达式。 Spring定义了Pointcut接口，用来组合MethodMatcher和ClassFilter，可以通过名字很清楚的理解， MethodMatcher是用来检查目标类的方法是否可以被应用此通知，而ClassFilter是用来检查Pointcut是否应该应用到目标类上 引入（Introduction）: 添加方法或字段到被通知的类。 Spring允许引入新的接口到任何被通知的对象。例如，你可以使用一个引入使任何对象实现 IsModified接口，来简化缓存。Spring中要使用Introduction, 可有通过DelegatingIntroductionInterceptor来实现通知，通过DefaultIntroductionAdvisor来配置Advice和代理类要实现的接口 目标对象（Target Object）: 包含连接点的对象。也被称作被通知或被代理对象。POJO AOP代理（AOP Proxy）: AOP框架创建的对象，包含通知。 在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）: 组装方面来创建一个被通知对象。这可以在编译时完成（例如使用AspectJ编译器），也可以在运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 Spring提供了两种方式来生成代理对象: JDKProxy和Cglib，具体使用哪种方式生成由AopProxyFactory根据AdvisedSupport对象的配置来决定。默认的策略是如果目标类是接口，则使用JDK动态代理技术，否则使用Cglib来生成代理。下面我们来研究一下Spring如何使用JDK来生成代理对象，具体的生成代码放在JdkDynamicAopProxy这个类中 12345678910111213141516171819修饰符介绍Java修饰符主要分为两类:访问修饰符非访问修饰符其中访问修饰符主要包括 private、default、protected、public。非访问修饰符主要包括 static、final、abstract、synchronized。访问修饰符访问修饰符可以使用下图这张表来说明访问权限:修饰符 当前类 同一包内 子类 其它包public Y Y Y Yprotected Y Y Y Ndefault Y Y N Nprivate Y N N N简单点查看访问级别的话，级别是由低到高。 private＜default＜protected＜public title: java-basicdate: 2017-12-15 10:19:56 tags:Class.forName() 返回一个类JVM会加载制定的类,并执行类的静态代码段 A a = (A)Class.forName(“pacage.A”).newInstance();A a = new A()通过包名和类名,实例对象 String className = readfromXMLConfig;class c = Class.forName(className);factory = (Exampleinterface)c.newInstence(); 使用newInstance() 要保证: 类已经加载 类已经连接 使用Class类的静态方法forName 完成这两步 newInstance(): 弱类型,低效率,只能调用无参构造。 new: 强类型,相对高效,能调用任何public构造。 Class.forName(“”)返回的是类。 Class.forName(“”).newInstance()返回的是object 。ReentrantLockReentrantLock是一个基于AQS的可重入的互斥锁，公平锁将确保等待时间最长的线程优先获取锁，将会使整体的吞吐量下降非公平锁将不能确定哪一个线程将获取锁，可能会导致某些线程饥饿。 12345678910111213public class ReentrantLockTest &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void doSomething() &#123; lock.lock(); // block until condition holds try &#123; // ... method body &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; java 静态代理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class ProxyDemo &#123; public static void main(String[] args) &#123; //1.创建委托对象 AbsSubject real = new RealSub(); //2.创建调用处理器对象 ProxyHandler handler = new ProxyHandler(real); //3.动态生成代理对象 AbsSubject proxySub = (AbsSubject)Proxy.newProxyInstance(real.getClass().getClassLoader(), real.getClass().getInterfaces(), handler); //4.通过代理对象调用方法 proxySub.doJob(); proxySub.sum(3, 9); int m = proxySub.multiply(3, 7); System.out.println("multiply result is:"+m); &#125;&#125;//被代理类的接口interface AbsSubject &#123; void doJob(); void sum(int a, int b); int multiply(int a, int b);&#125;//实际的被代理类class RealSub implements AbsSubject &#123; @Override public void doJob() &#123; // TODO Auto-generated method stub System.out.println("i am doing something"); &#125; @Override public void sum(int a, int b) &#123; System.out.println(a+" + "+b+" = "+(a+b)); &#125; @Override public int multiply(int a, int b) &#123; // TODO Auto-generated method stub System.out.println(a+" * "+ b); return a*b; &#125; &#125;//动态代理的内部实现,调用处理器类，即实现 InvocationHandler 接口//这个类的目的是指定运行时将生成的代理类需要完成的具体任务（包括Preprocess和Postprocess）//即代理类调用任何方法都会经过这个调用处理器类class ProxyHandler implements InvocationHandler &#123; private Object realSub; public ProxyHandler(Object object) &#123; realSub = object; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("before"); Object res = method.invoke(realSub, args); System.out.println("after"); return res; &#125; &#125; Stream(Java 8)1234List&lt;String&gt; lists =Arrays.asList("a","b","c");Stream&lt;String&gt; streamList = lists.stream();lists = streamList.distinct().finter(str-&gt;!str.equals("a").sorted(String::CompareTo).collect(ollectors.toList());lists.forEach(System.out::println); 创建Stream 集合 Stream: stream() 数组 12345678//lambdaCollections.sort(strList, (s1, s2)-&gt;s1.compareTo(s2));//方法引用Collections.sort(strList, String::compareTo);//lambdastrList.forEach(x-&gt;System.out.println(x));//方法引用strList.forEach(System.out::println); 方法引用的语法 对象::实例方法=&gt;等价于”提供方法参数的lambda表达式” 类::静态方法=&gt;等价于”提供方法参数的lambda表达式” eg. System.out::println等价于x-&gt;System.out.println(x) 类::实例方法=&gt;第一个参数是执行方法的对象 eg. String::compareTo等价于(s1, s2)-&gt;s1.compareTo(s2) aop 使用场景Authentication 权限Caching 缓存Context passing 内容传递Error handling 错误处理Lazy loading 懒加载Debugging 调试logging, tracing, profiling and monitoring 记录跟踪 优化 校准Performance optimization 性能优化Persistence 持久化Resource pooling 资源池Synchronization 同步Transactions 事务 AOP相关概念方面（Aspect）：一个关注点的模块化，这个关注点实现可能另外横切多个对象。事务管理是J2EE应用中一个很好的横切关注点例子。方面用spring的 Advisor或拦截器实现。 连接点（Joinpoint）: 程序执行过程中明确的点，如方法的调用或特定的异常被抛出。 通知（Advice）: 在特定的连接点，AOP框架执行的动作。各种类型的通知包括“around”、“before”和“throws”通知。通知类型将在下面讨论。许多AOP框架包括Spring都是以拦截器做通知模型，维护一个“围绕”连接点的拦截器链。Spring中定义了四个advice: BeforeAdvice, AfterAdvice, ThrowAdvice和DynamicIntroductionAdvice 切入点（Pointcut）: 指定一个通知将被引发的一系列连接点的集合。AOP框架必须允许开发者指定切入点：例如，使用正则表达式。 Spring定义了Pointcut接口，用来组合MethodMatcher和ClassFilter，可以通过名字很清楚的理解， MethodMatcher是用来检查目标类的方法是否可以被应用此通知，而ClassFilter是用来检查Pointcut是否应该应用到目标类上 引入（Introduction）: 添加方法或字段到被通知的类。 Spring允许引入新的接口到任何被通知的对象。例如，你可以使用一个引入使任何对象实现 IsModified接口，来简化缓存。Spring中要使用Introduction, 可有通过DelegatingIntroductionInterceptor来实现通知，通过DefaultIntroductionAdvisor来配置Advice和代理类要实现的接口 目标对象（Target Object）: 包含连接点的对象。也被称作被通知或被代理对象。POJO AOP代理（AOP Proxy）: AOP框架创建的对象，包含通知。 在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。 织入（Weaving）: 组装方面来创建一个被通知对象。这可以在编译时完成（例如使用AspectJ编译器），也可以在运行时完成。Spring和其他纯Java AOP框架一样，在运行时完成织入。 Spring提供了两种方式来生成代理对象: JDKProxy和Cglib，具体使用哪种方式生成由AopProxyFactory根据AdvisedSupport对象的配置来决定。默认的策略是如果目标类是接口，则使用JDK动态代理技术，否则使用Cglib来生成代理。下面我们来研究一下Spring如何使用JDK来生成代理对象，具体的生成代码放在JdkDynamicAopProxy这个类中 private static void demo2() { Thread A = new Thread(new Runnable() { @Override public void run() { printNumber(“A”); } }); Thread B = new Thread(new Runnable() { @Override public void run() { System.out.println(“B 开始等待 A”); try { A.join(); } catch (InterruptedException e) { e.printStackTrace(); } printNumber(“B”); } }); B.start(); A.start();}得到的结果如下： B 开始等待 AA print: 1A print: 2A print: 3 B print: 1B print: 2B print: 3所以我们能看到 A.join() 方法会让 B 一直等待直到 A 运行完毕。 为什么hashmap中数组的长的是16 或2的n次幂我们假设HaspMap的初始长度为10，重复前面的运算步骤： 单独看这个结果，表面上并没有问题。我们再来尝试一个新的HashCode 101110001110101110 1011 ：然后我们再换一个HashCode 101110001110101110 1111 试试 ： 这样我们可以看到，虽然HashCode的倒数第二第三位从0变成了1，但是运算的结果都是1001。也就是说，当HashMap长度为10的时候，有些index结果的出现几率会更大，而有些index结果永远不会出现（比如0111）！ 所以这样显然不符合Hash算法均匀分布的原则。 而长度是16或者其他2的次幂，Length - 1的值的所有二进制位全为1（如15的二进制是1111，31的二进制为11111），这种情况下，index的结果就等同于HashCode后几位的值。只要输入的HashCode本身分布均匀，Hash算法的结果就是均匀的。这也是HashMap设计的玄妙之处。 HashMap的死锁我们知道HashMap是非线程安全的，那么原因是什么呢？ 由于HashMap的容量是有限的，如果HashMap中的数组的容量很小，假如只有2个，那么如果要放进10个keys的话，碰撞就会非常频繁，此时一个O(1)的查找算法，就变成了链表遍历，性能变成了O(n)，这是Hash表的缺陷。 为了解决这个问题,HashMap设计了一个阈值，其值为容量的0.75，当HashMap所用容量超过了阈值后，就会自动扩充其容量。 在多线程的情况下，当重新调整HashMap大小的时候，就会存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历。如果条件竞争发生了，那么就会产生死循环了。 java 编译参数1javac -encoding utf-8 ./a.java Arthas 原文连接 https://alibaba.github.io/arthas/arthas-tutorials?language=cn&amp;id=arthas-advanced 1watch com.example.demo.arthas.user.UserController * &apos;&#123;params, throwExp&#125;&apos; -x 2 第一个参数是类名，支持通配 第二个参数是函数名，支持通配 第三个参数是 返回值 ognl 第四个参数是 命令支持在第4个参数里写条件表达式 watch com.example.demo.arthas.user.UserController * returnObj &#39;params[0] &gt; 100&#39; 12345678910`loader``clazz``method``target``params``returnObj``throwExp``isBefore``isThrow``isReturn` 热更新代码下面介绍通过jad/mc/redefine 命令实现动态更新代码的功能。 目前，访问 http://localhost/user/0 ，会返回500异常： 1curl http://localhost/user/0 1&#123;"timestamp":1550223186170,"status":500,"error":"Internal Server Error","exception":"java.lang.IllegalArgumentException","message":"id &lt; 1","path":"/user/0"&#125; 下面通过热更新代码，修改这个逻辑。 jad反编译UserController1jad --source-only com.example.demo.arthas.user.UserController &gt; /tmp/UserController.java jad反编译的结果保存在 /tmp/UserController.java文件里了。 再打开一个Terminal 3，然后用vim来编辑/tmp/UserController.java：1vim /tmp/UserController.java 比如当 user id 小于1时，也正常返回，不抛出异常：123456789@GetMapping(value=&#123;"/user/&#123;id&#125;"&#125;)public User findUserById(@PathVariable Integer id) &#123; logger.info("id: &#123;&#125;", (Object)id); if (id != null &amp;&amp; id &lt; 1) &#123; return new User(id, "name" + id); // throw new IllegalArgumentException("id &lt; 1"); &#125; return new User(id.intValue(), "name" + id);&#125; sc查找加载UserController的ClassLoader 1sc -d *UserController | grep classLoaderHash 1$ sc -d *UserController | grep classLoaderHash classLoaderHash 1be6f5c3可以发现是 spring boot LaunchedURLClassLoader@1be6f5c3 加载的。 mc 保存好/tmp/UserController.java之后，使用mc(Memory Compiler)命令来编译，并且通过-c参数指定ClassLoader： 1mc -c 1be6f5c3 /tmp/UserController.java -d /tmp $ mc -c 1be6f5c3 /tmp/UserController.java -d /tmp Memory compiler output:/tmp/com/example/demo/arthas/user/UserController.classAffect(row-cnt:1) cost in 346 ms redefine 再使用redefine命令重新加载新编译好的UserController.class： redefine /tmp/com/example/demo/arthas/user/UserController.class $ redefine /tmp/com/example/demo/arthas/user/UserController.classredefine success, size: 1 热修改代码结果redefine成功之后，再次访问 curl http://localhost/user/0 ，结果是：1234&#123; "id": 0, "name": "name0"&#125; 1因为static类型的属性会在类被加载之后被初始化，我们在深度分析Java的ClassLoader机制（源码级别）和Java类的加载、链接和初始化两个文章中分别介绍过，当一个Java类第一次被真正使用到的时候静态资源被初始化、Java类的加载和初始化过程都是线程安全的。所以，创建一个enum类型是线程安全的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[RedisTemplate]]></title>
    <url>%2F2017%2F12%2F15%2FRedisTemplate%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.ziggle.fan.service;import com.google.common.util.concurrent.RateLimiter;import io.lettuce.core.RedisCommandTimeoutException;import io.lettuce.core.RedisConnectionException;import lombok.extern.slf4j.Slf4j;import org.springframework.dao.QueryTimeoutException;import org.springframework.data.redis.connection.Message;import org.springframework.data.redis.connection.MessageListener;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.stereotype.Service;import javax.annotation.PostConstruct;import javax.annotation.PreDestroy;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;/** * redis 消息订阅 * * @author: wp * @date: 2019-03-13 14:31 */@Slf4j@Servicepublic class RedisMessageSubscriber implements MessageListener &#123; private final StringRedisTemplate stringRedisTemplate; private final RateLimiter rateLimiter; public RedisMessageSubscriber(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; this.rateLimiter = RateLimiter.create(1); &#125; @Override public void onMessage(Message message, byte[] pattern) &#123; log.info("Received &gt;&gt; " + message + ", " + Thread.currentThread().getName()); &#125; private ExecutorService executorService = Executors.newSingleThreadExecutor(r -&gt; &#123; Thread thread = new Thread(r); thread.setName("insert-contact-th"); thread.setDaemon(true); return thread; &#125;); private AtomicInteger flag = new AtomicInteger(0); @PreDestroy private void destroy() &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; flag.getAndAdd(1); &#125; &#125;); if (!executorService.isShutdown()) &#123; executorService.shutdown(); &#125; &#125;// @PostConstruct public void init() &#123; RateLimiter errorRate = RateLimiter.create(1); executorService .submit(() -&gt; &#123; for (; ; ) &#123; if (flag.get() != 0) &#123; break; &#125; try &#123; // todo bug String key = stringRedisTemplate.opsForList().leftPop("key", 3, TimeUnit.SECONDS); if (key != null) &#123; log.warn(key); rateLimiter.acquire(); &#125; log.info("nothing.... "); &#125; catch (Exception e) &#123; errorRate.acquire(); if (!(e instanceof RedisCommandTimeoutException || e instanceof QueryTimeoutException)) &#123; log.error(e.getMessage(), e); &#125; else &#123; //ignore &#125; &#125; &#125; &#125;); &#125;&#125; How to solve this problemhttps://jira.spring.io/browse/DATAREDIS-961?focusedCommentId=182689&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-182689https://stackoverflow.com/questions/55476087/stringredistemplate-opsforlist-leftpopkey-3-timeunit-seconds-can-not-clo]]></content>
  </entry>
  <entry>
    <title><![CDATA[sql-lock]]></title>
    <url>%2F2017%2F12%2F14%2Fsql-lock%2F</url>
    <content type="text"><![CDATA[数据库锁*InnoDB存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁开销、加锁速度、死锁、粒度、并发性能123表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 共享锁共享锁的代号是S，是Share的缩写，共享锁的锁粒度是行或者元组（多个行）。一个事务获取了共享锁之后，可以对锁定范围内的数据执行读操作。 排它锁排它锁的代号是X，是eXclusive的缩写，排它锁的粒度与共享锁相同，也是行或者元组。一个事务获取了排它锁之后，可以对锁定范围内的数据执行写操作。 例：假设有两个事务t1和t2如果事务t1获取了一个元组的共享锁，事务t2还可以立即获取这个元组的共享锁，但不能立即获取这个元组的排它锁（必须等到t1释放共享锁之后）。如果事务t1获取了一个元组的排它锁，事务t2不能立即获取这个元组的排共享锁，也不能立即获取这个元组的排它锁（必须等到t1释放排它锁之后）。 sqlsql server 中转义系统关键字是用“[]”来实现的但是在mysql中用这个是行不通的，最开始我用“’’”单引号，发现语法高亮的特点是没有了，我以为成功了，谁知一执行还是不行，最后查了下资料，原来是用的“``”，数字键1旁边的那个键，呵呵，看来这又是一个与sql server的区别啊]]></content>
  </entry>
  <entry>
    <title><![CDATA[vim-basic]]></title>
    <url>%2F2017%2F12%2F12%2Fvim-basic%2F</url>
    <content type="text"><![CDATA[全部格式化 1gg=G 格式化当前行 1== 退出vim 121 esc2 shift + zz 查找 12345678910111213141516171819202122232425262728293031/text 向后查?text 向前查yy 辅助p 粘贴dd 删除当前行o 另开一行u 撤销&#123; 在第一列插入 &#123; 来定义一个段落[[ 回到段落的开头处]] 向前移到下一个段落的开头处h &lt;l &gt;k ^j vd$ 删到最后r 替换n nextx 删除当前字符3x 删除当前光标开始向后3dh 删除前一个字符dj 删除上一行dk 删除下一行dw 删除当前光标下的单词/空格dG 删除至文件尾ddp 交换当前行和下一行ctrl+ ww 切换窗口guu 当前行转小写gUU 当前行转大写 – 替换 123456:%s/YouMeek/Judasn/g，把文件中所有 YouMeek 替换为：Judasn:%s/YouMeek/Judasn/，把文件中所有行中第一个 YouMeek 替换为：Judasn:s/YouMeek/Judasn/，把光标当前行第一个 YouMeek 替换为 Judasn:s/YouMeek/Judasn/g，把光标当前行所有 YouMeek 替换为 Judasn:s#YouMeek/#Judasn/#，除了使用斜杠作为分隔符之外，还可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符，该命令表示：把光标当前行第一个 YouMeek/ 替换为 Judasn/:10,31s/YouMeek/Judasn/g，把第 10 行到 31 行之间所有 YouMeek 替换为 Judasn vim注释多行123451. 首先按esc进入命令行模式下，按下Ctrl + v，进入列（也叫区块）模式;2. 在行首使用上下键选择需要注释的多行;3. 按下键盘（大写）“I”键，进入插入模式；4. 然后输入注释符（“//”、“#”等）;5. 最后按下“Esc”键。]]></content>
      <tags>
        <tag>-vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git-flight-rules]]></title>
    <url>%2F2017%2F12%2F11%2Fgit-flight-rules%2F</url>
    <content type="text"><![CDATA[引用地址 https://github.com/k88hudson/git-flight-rules Git飞行规则(Flight Rules)🌍English ∙ Русский ∙ 简体中文 前言 英文原版README 翻译可能存在错误或不标准的地方，欢迎大家指正和修改，谢谢！ 什么是”飞行规则”?一个 宇航员指南 (现在, 程序员们都在使用GIT) 是关于出现问题过后应该怎么操作。 飞行规则(Flight Rules) 是记录在手册上的来之不易的一系列知识，记录了某个事情发生的原因，以及怎样一步一步的进行处理。本质上, 它们是特定场景的非常详细的标准处理流程。 […] 自20世纪60年代初以来，NASA一直在捕捉(capturing)我们的失误，灾难和解决方案, 当时水星时代(Mercury-era)的地面小组首先开始将“经验教训”收集到一个纲要(compendium)中，该纲现在已经有上千个问题情景，从发动机故障到破损的舱口把手到计算机故障，以及它们对应的解决方案。 &mdash; Chris Hadfield, 一个宇航员的生活指南(An Astronaut’s Guide to Life)。 这篇文章的约定为了清楚的表述，这篇文档里的所有例子使用了自定义的bash 提示，以便指示当前分支和是否有暂存的变化(changes)。分支名用小括号括起来，分支名后面跟的*表示暂存的变化(changes)。 Table of Contents generated with DocToc 编辑提交(editting commits) 我刚才提交了什么? 我的提交信息(commit message)写错了 我提交(commit)里的用户名和邮箱不对 我想从一个提交(commit)里移除一个文件 我想删除我的的最后一次提交(commit) 删除任意提交(commit) 我尝试推一个修正后的提交(amended commit)到远程，但是报错： 我意外的做了一次硬重置(hard reset)，我想找回我的内容 暂存(Staging) 我需要把暂存的内容添加到上一次的提交(commit) 我想要暂存一个新文件的一部分，而不是这个文件的全部 我想把在一个文件里的变化(changes)加到两个提交(commit)里 我想把暂存的内容变成未暂存，把未暂存的内容暂存起来 未暂存(Unstaged)的内容 我想把未暂存的内容移动到一个新分支 我想把未暂存的内容移动到另一个已存在的分支 我想丢弃本地未提交的变化(uncommitted changes) 我想丢弃某些未暂存的内容 分支(Branches) 我从错误的分支拉取了内容，或把内容拉取到了错误的分支 我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致 我需要提交到一个新分支，但错误的提交到了master 我想保留来自另外一个ref-ish的整个文件 我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里 我想删除上游(upstream)分支被删除了的本地分支 我不小心删除了我的分支 我想删除一个分支 我想从别人正在工作的远程分支签出(checkout)一个分支 Rebasing 和合并(Merging) 我想撤销rebase/merge 我已经rebase过, 但是我不想强推(force push) 我需要组合(combine)几个提交(commit) 安全合并(merging)策略 我需要将一个分支合并成一个提交(commit) 我只想组合(combine)未推的提交(unpushed commit) 检查是否分支上的所有提交(commit)都合并(merge)过了 交互式rebase(interactive rebase)可能出现的问题 这个rebase 编辑屏幕出现’noop’ 有冲突的情况 杂项(Miscellaneous Objects) 克隆所有子模块 删除标签(tag) 恢复已删除标签(tag) 已删除补丁(patch) 跟踪文件(Tracking Files) 我只想改变一个文件名字的大小写，而不修改内容 我想从Git删除一个文件，但保留该文件 配置(Configuration) 我想给一些Git命令添加别名(alias) 我想缓存一个仓库(repository)的用户名和密码 我不知道我做错了些什么 其它资源(Other Resources) 书(Books) 教程(Tutorials) 脚本和工具(Scripts and Tools) GUI客户端(GUI Clients) 编辑提交(editting commits) 我刚才提交了什么?如果你用 git commit -a 提交了一次变化(changes)，而你又不确定到底这次提交了哪些面容。 你就可以用下面的命令显示当前HEAD上的最近一次的提交(commit): 1(master)$ git show 或者 1$ git log -n1 -p 我的提交信息(commit message)写错了如果你的提交信息(commit message)写错了且这次提交(commit)还没有推(push), 你可以通过下面的方法来修改提交信息(commit message): 1$ git commit --amend 这会打开你的默认编辑器, 在这里你可以编辑信息. 另一方面, 你也可以用一条命令一次完成: 1$ git commit --amend -m 'xxxxxxx' 如果你已经推(push)了这次提交(commit), 你可以修改这次提交(commit)然后强推(force push), 但是不推荐这么做。 我提交(commit)里的用户名和邮箱不对如果这只是单个提交(commit)，修改它： 1$ git commit --amend --author "New Authorname &lt;authoremail@mydomain.com&gt;" 如果你需要修改所有历史, 参考 ‘git filter-branch’的指南页. 我想从一个提交(commit)里移除一个文件通过下面的方法，从一个提交(commit)里移除一个文件: 123$ git checkout HEAD^ myfile$ git add -A$ git commit --amend 这将非常有用，当你有一个开放的补丁(open patch)，你往上面提交了一个不必要的文件，你需要强推(force push)去更新这个远程补丁。 我想删除我的的最后一次提交(commit)如果你需要删除推了的提交(pushed commits)，你可以使用下面的方法。可是，这会不可逆的改变你的历史，也会搞乱那些已经从该仓库拉取(pulled)了的人的历史。简而言之，如果你不是很确定，千万不要这么做。 12$ git reset HEAD^ --hard$ git push -f [remote] [branch] 如果你还没有推到远程, 把Git重置(reset)到你最后一次提交前的状态就可以了(同时保存暂存的变化): 1(my-branch*)$ git reset --soft HEAD@&#123;1&#125; 这只能在没有推送之前有用. 如果你已经推了, 唯一安全能做的是 git revert SHAofBadCommit， 那会创建一个新的提交(commit)用于撤消前一个提交的所有变化(changes)； 或者, 如果你推的这个分支是rebase-safe的 (例如： 其它开发者不会从这个分支拉), 只需要使用 git push -f； 更多, 请参考 the above section。 删除任意提交(commit)同样的警告：不到万不得已的时候不要这么做. 12$ git rebase --onto SHA1_OF_BAD_COMMIT^ SHA1_OF_BAD_COMMIT$ git push -f [remote] [branch] 或者做一个 交互式rebase 删除那些你想要删除的提交(commit)里所对应的行。 我尝试推一个修正后的提交(amended commit)到远程，但是报错：1234567To https://github.com/yourusername/repo.git! [rejected] mybranch -&gt; mybranch (non-fast-forward)error: failed to push some refs to 'https://github.com/tanay1337/webmaker.org.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 注意, rebasing(见下面)和修正(amending)会用一个新的提交(commit)代替旧的, 所以如果之前你已经往远程仓库上推过一次修正前的提交(commit)，那你现在就必须强推(force push) (-f)。 注意 &ndash; 总是 确保你指明一个分支! 1(my-branch)$ git push origin mybranch -f 一般来说, 要避免强推. 最好是创建和推(push)一个新的提交(commit)，而不是强推一个修正后的提交。后者会使那些与该分支或该分支的子分支工作的开发者，在源历史中产生冲突。 我意外的做了一次硬重置(hard reset)，我想找回我的内容如果你意外的做了 git reset --hard, 你通常能找回你的提交(commit), 因为Git对每件事都会有日志，且都会保存几天。 1(master)$ git reflog 你将会看到一个你过去提交(commit)的列表, 和一个重置的提交。 选择你想要回到的提交(commit)的SHA，再重置一次: 1(master)$ git reset --hard SHA1234 这样就完成了。 暂存(Staging) 我需要把暂存的内容添加到上一次的提交(commit)1(my-branch*)$ git commit --amend 我想要暂存一个新文件的一部分，而不是这个文件的全部一般来说, 如果你想暂存一个文件的一部分, 你可这样做: 1$ git add --patch filename.x -p 简写。这会打开交互模式， 你将能够用 s 选项来分隔提交(commit)； 然而, 如果这个文件是新的, 会没有这个选择， 添加一个新文件时, 这样做: 1$ git add -N filename.x 然后, 你需要用 e 选项来手动选择需要添加的行，执行 git diff --cached 将会显示哪些行暂存了哪些行只是保存在本地了。 我想把在一个文件里的变化(changes)加到两个提交(commit)里git add 会把整个文件加入到一个提交. git add -p 允许交互式的选择你想要提交的部分. 我想把暂存的内容变成未暂存，把未暂存的内容暂存起来这个有点困难， 我能想到的最好的方法是先stash未暂存的内容， 然后重置(reset)，再pop第一步stashed的内容, 最后再add它们。 1234$ git stash -k$ git reset --hard$ git stash pop$ git add -A 未暂存(Unstaged)的内容 我想把未暂存的内容移动到一个新分支1$ git checkout -b my-branch 我想把未暂存的内容移动到另一个已存在的分支123$ git stash$ git checkout my-branch$ git stash pop 我想丢弃本地未提交的变化(uncommitted changes)如果你只是想重置源(origin)和你本地(local)之间的一些提交(commit)，你可以： 12345678# one commit(my-branch)$ git reset --hard HEAD^# two commits(my-branch)$ git reset --hard HEAD^^# four commits(my-branch)$ git reset --hard HEAD~4# or(master)$ git checkout -f 重置某个特殊的文件, 你可以用文件名做为参数: 1$ git reset filename 我想丢弃某些未暂存的内容如果你想丢弃工作拷贝中的一部分内容，而不是全部。 签出(checkout)不需要的内容，保留需要的。 12$ git checkout -p# Answer y to all of the snippets you want to drop 另外一个方法是使用 stash， Stash所有要保留下的内容, 重置工作拷贝, 重新应用保留的部分。 1234$ git stash -p# Select all of the snippets you want to save$ git reset --hard$ git stash pop 或者, stash 你不需要的部分, 然后stash drop。 123$ git stash -p# Select all of the snippets you don't want to save$ git stash drop 分支(Branches) 我从错误的分支拉取了内容，或把内容拉取到了错误的分支这是另外一种使用 git reflog 情况，找到在这次错误拉(pull) 之前HEAD的指向。 123(master)$ git reflogab7555f HEAD@&#123;0&#125;: pull origin wrong-branch: Fast-forwardc5bc55a HEAD@&#123;1&#125;: checkout: checkout message goes here 重置分支到你所需的提交(desired commit): 1$ git reset --hard c5bc55a 完成。 我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致先确认你没有推(push)你的内容到远程。 git status 会显示你领先(ahead)源(origin)多少个提交: 12345(my-branch)$ git status# On branch my-branch# Your branch is ahead of 'origin/my-branch' by 2 commits.# (use "git push" to publish your local commits)# 一种方法是: 1(master)$ git reset --hard origin/my-branch 我需要提交到一个新分支，但错误的提交到了master在master下创建一个新分支，不切换到新分支,仍在master下: 1(master)$ git branch my-branch 把master分支重置到前一个提交: 1(master)$ git reset --hard HEAD^ HEAD^ 是 HEAD^1 的简写，你可以通过指定要设置的HEAD来进一步重置。 或者, 如果你不想使用 HEAD^, 找到你想重置到的提交(commit)的hash(git log 能够完成)， 然后重置到这个hash。 使用git push 同步内容到远程。 例如, master分支想重置到的提交的hash为a13b85e: 12(master)$ git reset --hard a13b85eHEAD is now at a13b85e 签出(checkout)刚才新建的分支继续工作: 1(master)$ git checkout my-branch 我想保留来自另外一个ref-ish的整个文件假设你正在做一个原型方案(原文为working spike (see note)), 有成百的内容，每个都工作得很好。现在, 你提交到了一个分支，保存工作内容: 1(solution)$ git add -A &amp;&amp; git commit -m "Adding all changes from this spike into one big commit." 当你想要把它放到一个分支里 (可能是feature, 或者 develop), 你关心是保持整个文件的完整，你想要一个大的提交分隔成比较小。 假设你有: 分支 solution, 拥有原型方案， 领先 develop 分支。 分支 develop, 在这里你应用原型方案的一些内容。 我去可以通过把内容拿到你的分支里，来解决这个问题: 1(develop)$ git checkout solution -- file1.txt 这会把这个文件内容从分支 solution 拿到分支 develop 里来: 123456# On branch develop# Your branch is up-to-date with 'origin/develop'.# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## modified: file1.txt 然后, 正常提交。 Note: Spike solutions are made to analyze or solve the problem. These solutions are used for estimation and discarded once everyone gets clear visualization of the problem. ~ Wikipedia. 我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里假设你有一个master分支， 执行git log, 你看到你做过两次提交: 12345678910111213141516171819(master)$ git logcommit e3851e817c451cc36f2e6f3049db528415e3c114Author: Alex Lee &lt;alexlee@example.com&gt;Date: Tue Jul 22 15:39:27 2014 -0400 Bug #21 - Added CSRF protectioncommit 5ea51731d150f7ddc4a365437931cd8be3bf3131Author: Alex Lee &lt;alexlee@example.com&gt;Date: Tue Jul 22 15:39:12 2014 -0400 Bug #14 - Fixed spacing on titlecommit a13b85e984171c6e2a1729bb061994525f626d14Author: Aki Rose &lt;akirose@example.com&gt;Date: Tue Jul 21 01:12:48 2014 -0400 First commit 让我们用提交hash(commit hash)标记bug (e3851e8 for #21, 5ea5173 for #14). 首先, 我们把master分支重置到正确的提交(a13b85e): 12(master)$ git reset --hard a13b85eHEAD is now at a13b85e 现在, 我们对 bug #21 创建一个新的分支: 12(master)$ git checkout -b 21(21)$ 接着, 我们用 cherry-pick 把对bug #21的提交放入当前分支。 这意味着我们将应用(apply)这个提交(commit)，仅仅这一个提交(commit)，直接在HEAD上面。 1(21)$ git cherry-pick e3851e8 这时候, 这里可能会产生冲突， 参见交互式 rebasing 章 冲突节 解决冲突. 再者， 我们为bug #14 创建一个新的分支, 也基于master分支 123(21)$ git checkout master(master)$ git checkout -b 14(14)$ 最后, 为 bug #14 执行 cherry-pick: 1(14)$ git cherry-pick 5ea5173 我想删除上游(upstream)分支被删除了的本地分支一旦你在github 上面合并(merge)了一个pull request, 你就可以删除你fork里被合并的分支。 如果你不准备继续在这个分支里工作, 删除这个分支的本地拷贝会更干净，使你不会陷入工作分支和一堆陈旧分支的混乱之中。 1$ git fetch -p 我不小心删除了我的分支如果你定期推送到远程, 多数情况下应该是安全的，但有些时候还是可能删除了还没有推到远程的分支。 让我们先创建一个分支和一个新的文件: 12345(master)$ git checkout -b my-branch(my-branch)$ git branch(my-branch)$ touch foo.txt(my-branch)$ lsREADME.md foo.txt 添加文件并做一次提交 123456789101112131415161718(my-branch)$ git add .(my-branch)$ git commit -m 'foo.txt added'(my-branch)$ foo.txt added 1 files changed, 1 insertions(+) create mode 100644 foo.txt(my-branch)$ git logcommit 4e3cd85a670ced7cc17a2b5d8d3d809ac88d5012Author: siemiatj &lt;siemiatj@example.com&gt;Date: Wed Jul 30 00:34:10 2014 +0200 foo.txt addedcommit 69204cdf0acbab201619d95ad8295928e7f411d5Author: Kate Hudson &lt;katehudson@example.com&gt;Date: Tue Jul 29 13:14:46 2014 -0400 Fixes #6: Force pushing after amending commits 现在我们切回到主(master)分支，‘不小心的’删除my-branch分支 1234567(my-branch)$ git checkout masterSwitched to branch 'master'Your branch is up-to-date with 'origin/master'.(master)$ git branch -D my-branchDeleted branch my-branch (was 4e3cd85).(master)$ echo oh noes, deleted my branch!oh noes, deleted my branch! 在这时候你应该想起了reflog, 一个升级版的日志，它存储了仓库(repo)里面所有动作的历史。 1234(master)$ git reflog69204cd HEAD@&#123;0&#125;: checkout: moving from my-branch to master4e3cd85 HEAD@&#123;1&#125;: commit: foo.txt added69204cd HEAD@&#123;2&#125;: checkout: moving from master to my-branch 正如你所见，我们有一个来自删除分支的提交hash(commit hash)，接下来看看是否能恢复删除了的分支。 123456(master)$ git checkout -b my-branch-helpSwitched to a new branch 'my-branch-help'(my-branch-help)$ git reset --hard 4e3cd85HEAD is now at 4e3cd85 foo.txt added(my-branch-help)$ lsREADME.md foo.txt 看! 我们把删除的文件找回来了。 Git的 reflog 在rebasing出错的时候也是同样有用的。 我想删除一个分支删除一个远程分支: 1(master)$ git push origin --delete my-branch 你也可以: 1(master)$ git push origin :my-branch 删除一个本地分支: 1(master)$ git branch -D my-branch 我想从别人正在工作的远程分支签出(checkout)一个分支首先, 从远程拉取(fetch) 所有分支: 1(master)$ git fetch --all 假设你想要从远程的daves分支签出到本地的daves 123(master)$ git checkout --track origin/davesBranch daves set up to track remote branch daves from origin.Switched to a new branch 'daves' (--track 是 git checkout -b [branch] [remotename]/[branch] 的简写) 这样就得到了一个daves分支的本地拷贝, 任何推过(pushed)的更新，远程都能看到. Rebasing 和合并(Merging) 我想撤销rebase/merge你可以合并(merge)或rebase了一个错误的分支, 或者完成不了一个进行中的rebase/merge。 Git 在进行危险操作的时候会把原始的HEAD保存在一个叫ORIG_HEAD的变量里, 所以要把分支恢复到rebase/merge前的状态是很容易的。 1(my-branch)$ git reset --hard ORIG_HEAD 我已经rebase过, 但是我不想强推(force push)不幸的是，如果你想把这些变化(changes)反应到远程分支上，你就必须得强推(force push)。 是因你快进(Fast forward)了提交，改变了Git历史, 远程分支不会接受变化(changes)，除非强推(force push)。这就是许多人使用 merge 工作流, 而不是 rebasing 工作流的主要原因之一， 开发者的强推(force push)会使大的团队陷入麻烦。使用时需要注意，一种安全使用 rebase 的方法是，不要把你的变化(changes)反映到远程分支上, 而是按下面的做: 1234(master)$ git checkout my-branch(my-branch)$ git rebase -i master(my-branch)$ git checkout master(master)$ git merge --ff-only my-branch 更多, 参见 this SO thread. 我需要组合(combine)几个提交(commit)假设你的工作分支将会做对于 master 的pull-request。 一般情况下你不关心提交(commit)的时间戳，只想组合 所有 提交(commit) 到一个单独的里面, 然后重置(reset)重提交(recommit)。 确保主(master)分支是最新的和你的变化都已经提交了, 然后: 12(my-branch)$ git reset --soft master(my-branch)$ git commit -am "New awesome feature" 如果你想要更多的控制, 想要保留时间戳, 你需要做交互式rebase (interactive rebase): 1(my-branch)$ git rebase -i master 如果没有相对的其它分支， 你将不得不相对自己的HEAD 进行 rebase。 例如：你想组合最近的两次提交(commit), 你将相对于HEAD~2 进行rebase， 组合最近3次提交(commit), 相对于HEAD~3, 等等。 1(master)$ git rebase -i HEAD~2 在你执行了交互式 rebase的命令(interactive rebase command)后, 你将在你的编辑器里看到类似下面的内容: 12345678910111213141516171819202122pick a9c8a1d Some refactoringpick 01b2fd8 New awesome featurepick b729ad5 fixuppick e3851e8 another fix# Rebase 8074d12..b729ad5 onto 8074d12## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like "squash", but discard this commit's log message# x, exec = run command (the rest of the line) using shell## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out 所有以 # 开头的行都是注释, 不会影响 rebase. 然后，你可以用任何上面命令列表的命令替换 pick, 你也可以通过删除对应的行来删除一个提交(commit)。 例如, 如果你想 单独保留最旧(first)的提交(commit),组合所有剩下的到第二个里面, 你就应该编辑第二个提交(commit)后面的每个提交(commit) 前的单词为 f: 1234pick a9c8a1d Some refactoringpick 01b2fd8 New awesome featuref b729ad5 fixupf e3851e8 another fix 如果你想组合这些提交(commit) 并重命名这个提交(commit), 你应该在第二个提交(commit)旁边添加一个r，或者更简单的用s 替代 f: 1234pick a9c8a1d Some refactoringpick 01b2fd8 New awesome features b729ad5 fixups e3851e8 another fix 你可以在接下来弹出的文本提示框里重命名提交(commit)。 12345678910Newer, awesomer features# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.# rebase in progress; onto 8074d12# You are currently editing a commit while rebasing branch 'master' on '8074d12'.## Changes to be committed:# modified: README.md# 如果成功了, 你应该看到类似下面的内容: 1(master)$ Successfully rebased and updated refs/heads/master. 安全合并(merging)策略--no-commit 执行合并(merge)但不自动提交, 给用户在做提交前检查和修改的机会。 no-ff 会为特性分支(feature branch)的存在过留下证据, 保持项目历史一致。 1(master)$ git merge --no-ff --no-commit my-branch 我需要将一个分支合并成一个提交(commit)1(master)$ git merge --squash my-branch 我只想组合(combine)未推的提交(unpushed commit)有时候，在将数据推向上游之前，你有几个正在进行的工作提交(commit)。这时候不希望把已经推(push)过的组合进来，因为其他人可能已经有提交(commit)引用它们了。 1(master)$ git rebase -i @&#123;u&#125; 这会产生一次交互式的rebase(interactive rebase), 只会列出没有推(push)的提交(commit)， 在这个列表时进行reorder/fix/squash 都是安全的。 检查是否分支上的所有提交(commit)都合并(merge)过了检查一个分支上的所有提交(commit)是否都已经合并(merge)到了其它分支, 你应该在这些分支的head(或任何 commits)之间做一次diff: 1(master)$ git log --graph --left-right --cherry-pick --oneline HEAD...feature/120-on-scroll 这会告诉你在一个分支里有而另一个分支没有的所有提交(commit), 和分支之间不共享的提交(commit)的列表。 另一个做法可以是: 1(master)$ git log master ^feature/120-on-scroll --no-merges 交互式rebase(interactive rebase)可能出现的问题 这个rebase 编辑屏幕出现’noop’如果你看到的是这样:1noop 这意味着你rebase的分支和当前分支在同一个提交(commit)上, 或者 领先(ahead) 当前分支。 你可以尝试: 检查确保主(master)分支没有问题 rebase HEAD~2 或者更早 有冲突的情况如果你不能成功的完成rebase, 你可能必须要解决冲突。 首先执行 git status 找出哪些文件有冲突: 1234567(my-branch)$ git statusOn branch my-branchChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: README.md 在这个例子里面, README.md 有冲突。 打开这个文件找到类似下面的内容: 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADsome code=========some code&gt;&gt;&gt;&gt;&gt;&gt;&gt; new-commit 你需要解决新提交的代码(示例里, 从中间==线到new-commit的地方)与HEAD 之间不一样的地方. 有时候这些合并非常复杂，你应该使用可视化的差异编辑器(visual diff editor): 1(master*)$ git mergetool -t opendiff 在你解决完所有冲突和测试过后, git add 变化了的(changed)文件, 然后用git rebase --continue 继续rebase。 12(my-branch)$ git add README.md(my-branch)$ git rebase --continue 如果在解决完所有的冲突过后，得到了与提交前一样的结果, 可以执行git rebase --skip。 任何时候你想结束整个rebase 过程，回来rebase前的分支状态, 你可以做: 1(my-branch)$ git rebase --abort 杂项(Miscellaneous Objects) 克隆所有子模块1$ git clone --recursive git://github.com/foo/bar.git 如果已经克隆了: 1$ git submodule update --init --recursive 删除标签(tag)12$ git tag -d &lt;tag_name&gt;$ git push &lt;remote&gt; :refs/tags/&lt;tag_name&gt; 恢复已删除标签(tag)如果你想恢复一个已删除标签(tag), 可以按照下面的步骤: 首先, 需要找到无法访问的标签(unreachable tag): 1$ git fsck --unreachable | grep tag 记下这个标签(tag)的hash，然后用Git的 update-ref: 1$ git update-ref refs/tags/&lt;tag_name&gt; &lt;hash&gt; 这时你的标签(tag)应该已经恢复了。 已删除补丁(patch)如果某人在 GitHub 上给你发了一个pull request, 但是然后他删除了他自己的原始 fork, 你将没法克隆他们的提交(commit)或使用 git am。在这种情况下, 最好手动的查看他们的提交(commit)，并把它们拷贝到一个本地新分支，然后做提交。 做完提交后, 再修改作者，参见变更作者。 然后, 应用变化, 再发起一个新的pull request。 跟踪文件(Tracking Files) 我只想改变一个文件名字的大小写，而不修改内容1(master)$ git mv --force myfile MyFile 我想从Git删除一个文件，但保留该文件1(master)$ git rm --cached log.txt 配置(Configuration) 我想给一些Git命令添加别名(alias)在 OS X 和 Linux 下, 你的 Git的配置文件储存在 部分添加了一些快捷别名(和一些我容易拼写错误的)，如下:12345678910111213141516171819202122```vim[alias] a = add amend = commit --amend c = commit ca = commit --amend ci = commit -a co = checkout d = diff dc = diff --changed ds = diff --staged f = fetch loll = log --graph --decorate --pretty=oneline --abbrev-commit m = merge one = log --pretty=oneline outstanding = rebase -i @&#123;u&#125; s = status unpushed = log @&#123;u&#125; wc = whatchanged wip = rebase -i @&#123;u&#125; zap = fetch -p 我想缓存一个仓库(repository)的用户名和密码你可能有一个仓库需要授权，这时你可以缓存用户名和密码，而不用每次推/拉(push/pull)的时候都输入，Credential helper能帮你。 12$ git config --global credential.helper cache# Set git to use the credential memory cache 12$ git config --global credential.helper 'cache --timeout=3600'# Set the cache to timeout after 1 hour (setting is in seconds) 我不知道我做错了些什么你把事情搞砸了：你 重置(reset) 了一些东西, 或者你合并了错误的分支, 亦或你强推了后找不到你自己的提交(commit)了。有些时候, 你一直都做得很好, 但你想回到以前的某个状态。 这就是 git reflog 的目的， reflog 记录对分支顶端(the tip of a branch)的任何改变, 即使那个顶端没有被任何分支或标签引用。基本上, 每次HEAD的改变, 一条新的记录就会增加到reflog。遗憾的是，这只对本地分支起作用，且它只跟踪动作 (例如，不会跟踪一个没有被记录的文件的任何改变)。 1234(master)$ git reflog0a2e358 HEAD@&#123;0&#125;: reset: moving to HEAD~20254ea7 HEAD@&#123;1&#125;: checkout: moving from 2.2 to masterc10f740 HEAD@&#123;2&#125;: checkout: moving from master to 2.2 上面的reflog展示了从master分支签出(checkout)到2.2 分支，然后再签回。 那里，还有一个硬重置(hard reset)到一个较旧的提交。最新的动作出现在最上面以 HEAD@{0}标识. 如果事实证明你不小心回移(move back)了提交(commit), reflog 会包含你不小心回移前master上指向的提交(0254ea7)。 1$ git reset --hard 0254ea7 然后使用git reset就可以把master改回到之前的commit，这提供了一个在历史被意外更改情况下的安全网。 (摘自). 其它资源(Other Resources)书(Books) Pro Git - Scott Chacon’s excellent git book Git Internals - Scott Chacon’s other excellent git book 教程(Tutorials) Learn Git branching 一个基于网页的交互式 branching/merging/rebasing 教程 Getting solid at Git rebase vs. merge git-workflow - Aaron Meurer的怎么使用Git为开源仓库贡献 GitHub as a workflow - 使用GitHub做为工作流的趣事, 尤其是空PRs 脚本和工具(Scripts and Tools) firstaidgit.io 一个可搜索的最常被问到的Git的问题 git-extra-commands - 一堆有用的额外的Git脚本 git-extras - GIT 工具集 – repo summary, repl, changelog population, author commit percentages and more git-fire - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。 git-tips - Git小提示 git-town - 通用，高级Git工作流支持！ http://www.git-town.com GUI客户端(GUI Clients) GitKraken - 豪华的Git客户端 Windows, Mac &amp; Linux git-cola - 另外一个Git客户端 Windows &amp; OS X GitUp - 一个新的Git客户端，在处理Git的复杂性上有自己的特点 gitx-dev - 图形化的Git客户端 OS X Source Tree - 免费的图形化Git客户端 Windows &amp; OS X Tower - 图形化Git客户端 OS X(付费) Sectiongit 分支操作 1 git fetch 2 git branch -a 3.1 git branch -d demo # 删除已经merge的分支 3.2 git branch -D demo # 删除分支忽略是否合并 4 删除远程分支 4.1 git push –delete origin foo 4.2 git push origin :foo git log –pretty=oneline –abbrev-commitgit commit –amend 修改提交msggit merge 出现冲突时 git状态为merging ,使用 1git mergetool 查看并解决冲突 .gitignore 添加忽略文件无效123git rm -r --cached . #注意 .# 重新添加文件git add . 删除git缓存区文件1git clean -f 分支基本操作 git fetch git pull 会把fetch的数据做到版本库 git branch -a 查看所有分支 git checkout -b branch 以当前分支为基础创建分支并转到分支 git branch -d 删除已经merge的分支 git branch -D 删除分支 git merge branchname merge 一个分支 git push origin :branchname #删除远程分支 git push origin –delete {the_remote_branch} 创建并检出分支: git checkout -b 新分支名称 新分支源分支 git checkout -b newbranchName master 分支合并 合并方法 1、直接合并：把两条分支上的历史轨迹合并，交汇到一起 2、压合合并：一条分支上若干提交条目压合成一个提交条目，提交到另一条分支的末梢 3、拣选合并：拣选另一条分支上的某个提交条目的改动带到当前分支上 直接合并 git merge 分支名称 git checkout alternate git add about.html git commit -m “add about page” git checkout master git merge alternate 压合合并 git merge –squash 分支名称 git checkout -b contact master git add contact.html git commit -m “add contact file” git commit -m “add contact file 2” -a git checkout master git merge –squash contact git status git commit -m “add contact page” -m “has primary and secondary email” 拣选合并 git cherry-pick 提交名称 git checkout contact git commit -m “add contact 3” -a [contact 6dbaf82]…… git checkout master git cherry-pick 6dbaf82 / git cherry-pick -n 6dbaf82 git checkout git checkout git 检出文件 git checkout branchname –file git pull 出现冲突,放弃本地修改,使用远程内容强制覆盖本地代码12git fetch --all # 只是下载代码到本地，不进行合并操作git reset --hard origin/master # 把HEAD指向最新下载的版本 git 指定克隆目录 并指定克隆深度 git clone https://github.com/magicmonty/bash-git-prompt.git .directory_name –depth=1 git clone 指定分支1git clone -b dev https://github.com/cheft/minrouter.git git 错误解决 Error: Warning: Permanently added the RSA host key for IP address ‘52.74.223.119’ to the list of known hosts. title: git-flight-rulesdate: 2017-12-11 11:16:36 tags:引用地址 https://github.com/k88hudson/git-flight-rules Git飞行规则(Flight Rules)🌍English ∙ Русский ∙ 简体中文 前言 英文原版README 翻译可能存在错误或不标准的地方，欢迎大家指正和修改，谢谢！ 什么是”飞行规则”?一个 宇航员指南 (现在, 程序员们都在使用GIT) 是关于出现问题过后应该怎么操作。 飞行规则(Flight Rules) 是记录在手册上的来之不易的一系列知识，记录了某个事情发生的原因，以及怎样一步一步的进行处理。本质上, 它们是特定场景的非常详细的标准处理流程。 […] 自20世纪60年代初以来，NASA一直在捕捉(capturing)我们的失误，灾难和解决方案, 当时水星时代(Mercury-era)的地面小组首先开始将“经验教训”收集到一个纲要(compendium)中，该纲现在已经有上千个问题情景，从发动机故障到破损的舱口把手到计算机故障，以及它们对应的解决方案。 &mdash; Chris Hadfield, 一个宇航员的生活指南(An Astronaut’s Guide to Life)。 这篇文章的约定为了清楚的表述，这篇文档里的所有例子使用了自定义的bash 提示，以便指示当前分支和是否有暂存的变化(changes)。分支名用小括号括起来，分支名后面跟的*表示暂存的变化(changes)。 Table of Contents generated with DocToc 编辑提交(editting commits) 我刚才提交了什么? 我的提交信息(commit message)写错了 我提交(commit)里的用户名和邮箱不对 我想从一个提交(commit)里移除一个文件 我想删除我的的最后一次提交(commit) 删除任意提交(commit) 我尝试推一个修正后的提交(amended commit)到远程，但是报错： 我意外的做了一次硬重置(hard reset)，我想找回我的内容 暂存(Staging) 我需要把暂存的内容添加到上一次的提交(commit) 我想要暂存一个新文件的一部分，而不是这个文件的全部 我想把在一个文件里的变化(changes)加到两个提交(commit)里 我想把暂存的内容变成未暂存，把未暂存的内容暂存起来 未暂存(Unstaged)的内容 我想把未暂存的内容移动到一个新分支 我想把未暂存的内容移动到另一个已存在的分支 我想丢弃本地未提交的变化(uncommitted changes) 我想丢弃某些未暂存的内容 分支(Branches) 我从错误的分支拉取了内容，或把内容拉取到了错误的分支 我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致 我需要提交到一个新分支，但错误的提交到了master 我想保留来自另外一个ref-ish的整个文件 我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里 我想删除上游(upstream)分支被删除了的本地分支 我不小心删除了我的分支 我想删除一个分支 我想从别人正在工作的远程分支签出(checkout)一个分支 Rebasing 和合并(Merging) 我想撤销rebase/merge 我已经rebase过, 但是我不想强推(force push) 我需要组合(combine)几个提交(commit) 安全合并(merging)策略 我需要将一个分支合并成一个提交(commit) 我只想组合(combine)未推的提交(unpushed commit) 检查是否分支上的所有提交(commit)都合并(merge)过了 交互式rebase(interactive rebase)可能出现的问题 这个rebase 编辑屏幕出现’noop’ 有冲突的情况 杂项(Miscellaneous Objects) 克隆所有子模块 删除标签(tag) 恢复已删除标签(tag) 已删除补丁(patch) 跟踪文件(Tracking Files) 我只想改变一个文件名字的大小写，而不修改内容 我想从Git删除一个文件，但保留该文件 配置(Configuration) 我想给一些Git命令添加别名(alias) 我想缓存一个仓库(repository)的用户名和密码 我不知道我做错了些什么 其它资源(Other Resources) 书(Books) 教程(Tutorials) 脚本和工具(Scripts and Tools) GUI客户端(GUI Clients) 编辑提交(editting commits) 我刚才提交了什么?如果你用 git commit -a 提交了一次变化(changes)，而你又不确定到底这次提交了哪些面容。 你就可以用下面的命令显示当前HEAD上的最近一次的提交(commit): 1(master)$ git show 或者 1$ git log -n1 -p 我的提交信息(commit message)写错了如果你的提交信息(commit message)写错了且这次提交(commit)还没有推(push), 你可以通过下面的方法来修改提交信息(commit message): 1$ git commit --amend 这会打开你的默认编辑器, 在这里你可以编辑信息. 另一方面, 你也可以用一条命令一次完成: 1$ git commit --amend -m 'xxxxxxx' 如果你已经推(push)了这次提交(commit), 你可以修改这次提交(commit)然后强推(force push), 但是不推荐这么做。 我提交(commit)里的用户名和邮箱不对如果这只是单个提交(commit)，修改它： 1$ git commit --amend --author "New Authorname &lt;authoremail@mydomain.com&gt;" 如果你需要修改所有历史, 参考 ‘git filter-branch’的指南页. 我想从一个提交(commit)里移除一个文件通过下面的方法，从一个提交(commit)里移除一个文件: 123$ git checkout HEAD^ myfile$ git add -A$ git commit --amend 这将非常有用，当你有一个开放的补丁(open patch)，你往上面提交了一个不必要的文件，你需要强推(force push)去更新这个远程补丁。 我想删除我的的最后一次提交(commit)如果你需要删除推了的提交(pushed commits)，你可以使用下面的方法。可是，这会不可逆的改变你的历史，也会搞乱那些已经从该仓库拉取(pulled)了的人的历史。简而言之，如果你不是很确定，千万不要这么做。 12$ git reset HEAD^ --hard$ git push -f [remote] [branch] 如果你还没有推到远程, 把Git重置(reset)到你最后一次提交前的状态就可以了(同时保存暂存的变化): 1(my-branch*)$ git reset --soft HEAD@&#123;1&#125; 这只能在没有推送之前有用. 如果你已经推了, 唯一安全能做的是 git revert SHAofBadCommit， 那会创建一个新的提交(commit)用于撤消前一个提交的所有变化(changes)； 或者, 如果你推的这个分支是rebase-safe的 (例如： 其它开发者不会从这个分支拉), 只需要使用 git push -f； 更多, 请参考 the above section。 删除任意提交(commit)同样的警告：不到万不得已的时候不要这么做. 12$ git rebase --onto SHA1_OF_BAD_COMMIT^ SHA1_OF_BAD_COMMIT$ git push -f [remote] [branch] 或者做一个 交互式rebase 删除那些你想要删除的提交(commit)里所对应的行。 我尝试推一个修正后的提交(amended commit)到远程，但是报错：1234567To https://github.com/yourusername/repo.git! [rejected] mybranch -&gt; mybranch (non-fast-forward)error: failed to push some refs to 'https://github.com/tanay1337/webmaker.org.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 注意, rebasing(见下面)和修正(amending)会用一个新的提交(commit)代替旧的, 所以如果之前你已经往远程仓库上推过一次修正前的提交(commit)，那你现在就必须强推(force push) (-f)。 注意 &ndash; 总是 确保你指明一个分支! 1(my-branch)$ git push origin mybranch -f 一般来说, 要避免强推. 最好是创建和推(push)一个新的提交(commit)，而不是强推一个修正后的提交。后者会使那些与该分支或该分支的子分支工作的开发者，在源历史中产生冲突。 我意外的做了一次硬重置(hard reset)，我想找回我的内容如果你意外的做了 git reset --hard, 你通常能找回你的提交(commit), 因为Git对每件事都会有日志，且都会保存几天。 1(master)$ git reflog 你将会看到一个你过去提交(commit)的列表, 和一个重置的提交。 选择你想要回到的提交(commit)的SHA，再重置一次: 1(master)$ git reset --hard SHA1234 这样就完成了。 暂存(Staging) 我需要把暂存的内容添加到上一次的提交(commit)1(my-branch*)$ git commit --amend 我想要暂存一个新文件的一部分，而不是这个文件的全部一般来说, 如果你想暂存一个文件的一部分, 你可这样做: 1$ git add --patch filename.x -p 简写。这会打开交互模式， 你将能够用 s 选项来分隔提交(commit)； 然而, 如果这个文件是新的, 会没有这个选择， 添加一个新文件时, 这样做: 1$ git add -N filename.x 然后, 你需要用 e 选项来手动选择需要添加的行，执行 git diff --cached 将会显示哪些行暂存了哪些行只是保存在本地了。 我想把在一个文件里的变化(changes)加到两个提交(commit)里git add 会把整个文件加入到一个提交. git add -p 允许交互式的选择你想要提交的部分. 我想把暂存的内容变成未暂存，把未暂存的内容暂存起来这个有点困难， 我能想到的最好的方法是先stash未暂存的内容， 然后重置(reset)，再pop第一步stashed的内容, 最后再add它们。 1234$ git stash -k$ git reset --hard$ git stash pop$ git add -A 未暂存(Unstaged)的内容 我想把未暂存的内容移动到一个新分支1$ git checkout -b my-branch 我想把未暂存的内容移动到另一个已存在的分支123$ git stash$ git checkout my-branch$ git stash pop 我想丢弃本地未提交的变化(uncommitted changes)如果你只是想重置源(origin)和你本地(local)之间的一些提交(commit)，你可以： 12345678# one commit(my-branch)$ git reset --hard HEAD^# two commits(my-branch)$ git reset --hard HEAD^^# four commits(my-branch)$ git reset --hard HEAD~4# or(master)$ git checkout -f 重置某个特殊的文件, 你可以用文件名做为参数: 1$ git reset filename 我想丢弃某些未暂存的内容如果你想丢弃工作拷贝中的一部分内容，而不是全部。 签出(checkout)不需要的内容，保留需要的。 12$ git checkout -p# Answer y to all of the snippets you want to drop 另外一个方法是使用 stash， Stash所有要保留下的内容, 重置工作拷贝, 重新应用保留的部分。 1234$ git stash -p# Select all of the snippets you want to save$ git reset --hard$ git stash pop 或者, stash 你不需要的部分, 然后stash drop。 123$ git stash -p# Select all of the snippets you don't want to save$ git stash drop 分支(Branches) 我从错误的分支拉取了内容，或把内容拉取到了错误的分支这是另外一种使用 git reflog 情况，找到在这次错误拉(pull) 之前HEAD的指向。 123(master)$ git reflogab7555f HEAD@&#123;0&#125;: pull origin wrong-branch: Fast-forwardc5bc55a HEAD@&#123;1&#125;: checkout: checkout message goes here 重置分支到你所需的提交(desired commit): 1$ git reset --hard c5bc55a 完成。 我想扔掉本地的提交(commit)，以便我的分支与远程的保持一致先确认你没有推(push)你的内容到远程。 git status 会显示你领先(ahead)源(origin)多少个提交: 12345(my-branch)$ git status# On branch my-branch# Your branch is ahead of 'origin/my-branch' by 2 commits.# (use "git push" to publish your local commits)# 一种方法是: 1(master)$ git reset --hard origin/my-branch 我需要提交到一个新分支，但错误的提交到了master在master下创建一个新分支，不切换到新分支,仍在master下: 1(master)$ git branch my-branch 把master分支重置到前一个提交: 1(master)$ git reset --hard HEAD^ HEAD^ 是 HEAD^1 的简写，你可以通过指定要设置的HEAD来进一步重置。 或者, 如果你不想使用 HEAD^, 找到你想重置到的提交(commit)的hash(git log 能够完成)， 然后重置到这个hash。 使用git push 同步内容到远程。 例如, master分支想重置到的提交的hash为a13b85e: 12(master)$ git reset --hard a13b85eHEAD is now at a13b85e 签出(checkout)刚才新建的分支继续工作: 1(master)$ git checkout my-branch 我想保留来自另外一个ref-ish的整个文件假设你正在做一个原型方案(原文为working spike (see note)), 有成百的内容，每个都工作得很好。现在, 你提交到了一个分支，保存工作内容: 1(solution)$ git add -A &amp;&amp; git commit -m "Adding all changes from this spike into one big commit." 当你想要把它放到一个分支里 (可能是feature, 或者 develop), 你关心是保持整个文件的完整，你想要一个大的提交分隔成比较小。 假设你有: 分支 solution, 拥有原型方案， 领先 develop 分支。 分支 develop, 在这里你应用原型方案的一些内容。 我去可以通过把内容拿到你的分支里，来解决这个问题: 1(develop)$ git checkout solution -- file1.txt 这会把这个文件内容从分支 solution 拿到分支 develop 里来: 123456# On branch develop# Your branch is up-to-date with 'origin/develop'.# Changes to be committed:# (use "git reset HEAD &lt;file&gt;..." to unstage)## modified: file1.txt 然后, 正常提交。 Note: Spike solutions are made to analyze or solve the problem. These solutions are used for estimation and discarded once everyone gets clear visualization of the problem. ~ Wikipedia. 我把几个提交(commit)提交到了同一个分支，而这些提交应该分布在不同的分支里假设你有一个master分支， 执行git log, 你看到你做过两次提交: 12345678910111213141516171819(master)$ git logcommit e3851e817c451cc36f2e6f3049db528415e3c114Author: Alex Lee &lt;alexlee@example.com&gt;Date: Tue Jul 22 15:39:27 2014 -0400 Bug #21 - Added CSRF protectioncommit 5ea51731d150f7ddc4a365437931cd8be3bf3131Author: Alex Lee &lt;alexlee@example.com&gt;Date: Tue Jul 22 15:39:12 2014 -0400 Bug #14 - Fixed spacing on titlecommit a13b85e984171c6e2a1729bb061994525f626d14Author: Aki Rose &lt;akirose@example.com&gt;Date: Tue Jul 21 01:12:48 2014 -0400 First commit 让我们用提交hash(commit hash)标记bug (e3851e8 for #21, 5ea5173 for #14). 首先, 我们把master分支重置到正确的提交(a13b85e): 12(master)$ git reset --hard a13b85eHEAD is now at a13b85e 现在, 我们对 bug #21 创建一个新的分支: 12(master)$ git checkout -b 21(21)$ 接着, 我们用 cherry-pick 把对bug #21的提交放入当前分支。 这意味着我们将应用(apply)这个提交(commit)，仅仅这一个提交(commit)，直接在HEAD上面。 1(21)$ git cherry-pick e3851e8 这时候, 这里可能会产生冲突， 参见交互式 rebasing 章 冲突节 解决冲突. 再者， 我们为bug #14 创建一个新的分支, 也基于master分支 123(21)$ git checkout master(master)$ git checkout -b 14(14)$ 最后, 为 bug #14 执行 cherry-pick: 1(14)$ git cherry-pick 5ea5173 我想删除上游(upstream)分支被删除了的本地分支一旦你在github 上面合并(merge)了一个pull request, 你就可以删除你fork里被合并的分支。 如果你不准备继续在这个分支里工作, 删除这个分支的本地拷贝会更干净，使你不会陷入工作分支和一堆陈旧分支的混乱之中。 1$ git fetch -p 我不小心删除了我的分支如果你定期推送到远程, 多数情况下应该是安全的，但有些时候还是可能删除了还没有推到远程的分支。 让我们先创建一个分支和一个新的文件: 12345(master)$ git checkout -b my-branch(my-branch)$ git branch(my-branch)$ touch foo.txt(my-branch)$ lsREADME.md foo.txt 添加文件并做一次提交 123456789101112131415161718(my-branch)$ git add .(my-branch)$ git commit -m 'foo.txt added'(my-branch)$ foo.txt added 1 files changed, 1 insertions(+) create mode 100644 foo.txt(my-branch)$ git logcommit 4e3cd85a670ced7cc17a2b5d8d3d809ac88d5012Author: siemiatj &lt;siemiatj@example.com&gt;Date: Wed Jul 30 00:34:10 2014 +0200 foo.txt addedcommit 69204cdf0acbab201619d95ad8295928e7f411d5Author: Kate Hudson &lt;katehudson@example.com&gt;Date: Tue Jul 29 13:14:46 2014 -0400 Fixes #6: Force pushing after amending commits 现在我们切回到主(master)分支，‘不小心的’删除my-branch分支 1234567(my-branch)$ git checkout masterSwitched to branch 'master'Your branch is up-to-date with 'origin/master'.(master)$ git branch -D my-branchDeleted branch my-branch (was 4e3cd85).(master)$ echo oh noes, deleted my branch!oh noes, deleted my branch! 在这时候你应该想起了reflog, 一个升级版的日志，它存储了仓库(repo)里面所有动作的历史。 1234(master)$ git reflog69204cd HEAD@&#123;0&#125;: checkout: moving from my-branch to master4e3cd85 HEAD@&#123;1&#125;: commit: foo.txt added69204cd HEAD@&#123;2&#125;: checkout: moving from master to my-branch 正如你所见，我们有一个来自删除分支的提交hash(commit hash)，接下来看看是否能恢复删除了的分支。 123456(master)$ git checkout -b my-branch-helpSwitched to a new branch 'my-branch-help'(my-branch-help)$ git reset --hard 4e3cd85HEAD is now at 4e3cd85 foo.txt added(my-branch-help)$ lsREADME.md foo.txt 看! 我们把删除的文件找回来了。 Git的 reflog 在rebasing出错的时候也是同样有用的。 我想删除一个分支删除一个远程分支: 1(master)$ git push origin --delete my-branch 你也可以: 1(master)$ git push origin :my-branch 删除一个本地分支: 1(master)$ git branch -D my-branch 我想从别人正在工作的远程分支签出(checkout)一个分支首先, 从远程拉取(fetch) 所有分支: 1(master)$ git fetch --all 假设你想要从远程的daves分支签出到本地的daves 123(master)$ git checkout --track origin/davesBranch daves set up to track remote branch daves from origin.Switched to a new branch 'daves' (--track 是 git checkout -b [branch] [remotename]/[branch] 的简写) 这样就得到了一个daves分支的本地拷贝, 任何推过(pushed)的更新，远程都能看到. Rebasing 和合并(Merging) 我想撤销rebase/merge你可以合并(merge)或rebase了一个错误的分支, 或者完成不了一个进行中的rebase/merge。 Git 在进行危险操作的时候会把原始的HEAD保存在一个叫ORIG_HEAD的变量里, 所以要把分支恢复到rebase/merge前的状态是很容易的。 1(my-branch)$ git reset --hard ORIG_HEAD 我已经rebase过, 但是我不想强推(force push)不幸的是，如果你想把这些变化(changes)反应到远程分支上，你就必须得强推(force push)。 是因你快进(Fast forward)了提交，改变了Git历史, 远程分支不会接受变化(changes)，除非强推(force push)。这就是许多人使用 merge 工作流, 而不是 rebasing 工作流的主要原因之一， 开发者的强推(force push)会使大的团队陷入麻烦。使用时需要注意，一种安全使用 rebase 的方法是，不要把你的变化(changes)反映到远程分支上, 而是按下面的做: 1234(master)$ git checkout my-branch(my-branch)$ git rebase -i master(my-branch)$ git checkout master(master)$ git merge --ff-only my-branch 更多, 参见 this SO thread. 我需要组合(combine)几个提交(commit)假设你的工作分支将会做对于 master 的pull-request。 一般情况下你不关心提交(commit)的时间戳，只想组合 所有 提交(commit) 到一个单独的里面, 然后重置(reset)重提交(recommit)。 确保主(master)分支是最新的和你的变化都已经提交了, 然后: 12(my-branch)$ git reset --soft master(my-branch)$ git commit -am "New awesome feature" 如果你想要更多的控制, 想要保留时间戳, 你需要做交互式rebase (interactive rebase): 1(my-branch)$ git rebase -i master 如果没有相对的其它分支， 你将不得不相对自己的HEAD 进行 rebase。 例如：你想组合最近的两次提交(commit), 你将相对于HEAD~2 进行rebase， 组合最近3次提交(commit), 相对于HEAD~3, 等等。 1(master)$ git rebase -i HEAD~2 在你执行了交互式 rebase的命令(interactive rebase command)后, 你将在你的编辑器里看到类似下面的内容: 12345678910111213141516171819202122pick a9c8a1d Some refactoringpick 01b2fd8 New awesome featurepick b729ad5 fixuppick e3851e8 another fix# Rebase 8074d12..b729ad5 onto 8074d12## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like "squash", but discard this commit's log message# x, exec = run command (the rest of the line) using shell## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out 所有以 # 开头的行都是注释, 不会影响 rebase. 然后，你可以用任何上面命令列表的命令替换 pick, 你也可以通过删除对应的行来删除一个提交(commit)。 例如, 如果你想 单独保留最旧(first)的提交(commit),组合所有剩下的到第二个里面, 你就应该编辑第二个提交(commit)后面的每个提交(commit) 前的单词为 f: 1234pick a9c8a1d Some refactoringpick 01b2fd8 New awesome featuref b729ad5 fixupf e3851e8 another fix 如果你想组合这些提交(commit) 并重命名这个提交(commit), 你应该在第二个提交(commit)旁边添加一个r，或者更简单的用s 替代 f: 1234pick a9c8a1d Some refactoringpick 01b2fd8 New awesome features b729ad5 fixups e3851e8 another fix 你可以在接下来弹出的文本提示框里重命名提交(commit)。 12345678910Newer, awesomer features# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.# rebase in progress; onto 8074d12# You are currently editing a commit while rebasing branch 'master' on '8074d12'.## Changes to be committed:# modified: README.md# 如果成功了, 你应该看到类似下面的内容: 1(master)$ Successfully rebased and updated refs/heads/master. 安全合并(merging)策略--no-commit 执行合并(merge)但不自动提交, 给用户在做提交前检查和修改的机会。 no-ff 会为特性分支(feature branch)的存在过留下证据, 保持项目历史一致。 1(master)$ git merge --no-ff --no-commit my-branch 我需要将一个分支合并成一个提交(commit)1(master)$ git merge --squash my-branch 我只想组合(combine)未推的提交(unpushed commit)有时候，在将数据推向上游之前，你有几个正在进行的工作提交(commit)。这时候不希望把已经推(push)过的组合进来，因为其他人可能已经有提交(commit)引用它们了。 1(master)$ git rebase -i @&#123;u&#125; 这会产生一次交互式的rebase(interactive rebase), 只会列出没有推(push)的提交(commit)， 在这个列表时进行reorder/fix/squash 都是安全的。 检查是否分支上的所有提交(commit)都合并(merge)过了检查一个分支上的所有提交(commit)是否都已经合并(merge)到了其它分支, 你应该在这些分支的head(或任何 commits)之间做一次diff: 1(master)$ git log --graph --left-right --cherry-pick --oneline HEAD...feature/120-on-scroll 这会告诉你在一个分支里有而另一个分支没有的所有提交(commit), 和分支之间不共享的提交(commit)的列表。 另一个做法可以是: 1(master)$ git log master ^feature/120-on-scroll --no-merges 交互式rebase(interactive rebase)可能出现的问题 这个rebase 编辑屏幕出现’noop’如果你看到的是这样:1noop 这意味着你rebase的分支和当前分支在同一个提交(commit)上, 或者 领先(ahead) 当前分支。 你可以尝试: 检查确保主(master)分支没有问题 rebase HEAD~2 或者更早 有冲突的情况如果你不能成功的完成rebase, 你可能必须要解决冲突。 首先执行 git status 找出哪些文件有冲突: 1234567(my-branch)$ git statusOn branch my-branchChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: README.md 在这个例子里面, README.md 有冲突。 打开这个文件找到类似下面的内容: 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADsome code=========some code&gt;&gt;&gt;&gt;&gt;&gt;&gt; new-commit 你需要解决新提交的代码(示例里, 从中间==线到new-commit的地方)与HEAD 之间不一样的地方. 有时候这些合并非常复杂，你应该使用可视化的差异编辑器(visual diff editor): 1(master*)$ git mergetool -t opendiff 在你解决完所有冲突和测试过后, git add 变化了的(changed)文件, 然后用git rebase --continue 继续rebase。 12(my-branch)$ git add README.md(my-branch)$ git rebase --continue 如果在解决完所有的冲突过后，得到了与提交前一样的结果, 可以执行git rebase --skip。 任何时候你想结束整个rebase 过程，回来rebase前的分支状态, 你可以做: 1(my-branch)$ git rebase --abort 杂项(Miscellaneous Objects) 克隆所有子模块1$ git clone --recursive git://github.com/foo/bar.git 如果已经克隆了: 1$ git submodule update --init --recursive 删除标签(tag)12$ git tag -d &lt;tag_name&gt;$ git push &lt;remote&gt; :refs/tags/&lt;tag_name&gt; 恢复已删除标签(tag)如果你想恢复一个已删除标签(tag), 可以按照下面的步骤: 首先, 需要找到无法访问的标签(unreachable tag): 1$ git fsck --unreachable | grep tag 记下这个标签(tag)的hash，然后用Git的 update-ref: 1$ git update-ref refs/tags/&lt;tag_name&gt; &lt;hash&gt; 这时你的标签(tag)应该已经恢复了。 已删除补丁(patch)如果某人在 GitHub 上给你发了一个pull request, 但是然后他删除了他自己的原始 fork, 你将没法克隆他们的提交(commit)或使用 git am。在这种情况下, 最好手动的查看他们的提交(commit)，并把它们拷贝到一个本地新分支，然后做提交。 做完提交后, 再修改作者，参见变更作者。 然后, 应用变化, 再发起一个新的pull request。 跟踪文件(Tracking Files) 我只想改变一个文件名字的大小写，而不修改内容1(master)$ git mv --force myfile MyFile 我想从Git删除一个文件，但保留该文件1(master)$ git rm --cached log.txt 配置(Configuration) 我想给一些Git命令添加别名(alias)在 OS X 和 Linux 下, 你的 Git的配置文件储存在 部分添加了一些快捷别名(和一些我容易拼写错误的)，如下:12345678910111213141516171819202122```vim[alias] a = add amend = commit --amend c = commit ca = commit --amend ci = commit -a co = checkout d = diff dc = diff --changed ds = diff --staged f = fetch loll = log --graph --decorate --pretty=oneline --abbrev-commit m = merge one = log --pretty=oneline outstanding = rebase -i @&#123;u&#125; s = status unpushed = log @&#123;u&#125; wc = whatchanged wip = rebase -i @&#123;u&#125; zap = fetch -p 我想缓存一个仓库(repository)的用户名和密码你可能有一个仓库需要授权，这时你可以缓存用户名和密码，而不用每次推/拉(push/pull)的时候都输入，Credential helper能帮你。 12$ git config --global credential.helper cache# Set git to use the credential memory cache 12$ git config --global credential.helper 'cache --timeout=3600'# Set the cache to timeout after 1 hour (setting is in seconds) 我不知道我做错了些什么你把事情搞砸了：你 重置(reset) 了一些东西, 或者你合并了错误的分支, 亦或你强推了后找不到你自己的提交(commit)了。有些时候, 你一直都做得很好, 但你想回到以前的某个状态。 这就是 git reflog 的目的， reflog 记录对分支顶端(the tip of a branch)的任何改变, 即使那个顶端没有被任何分支或标签引用。基本上, 每次HEAD的改变, 一条新的记录就会增加到reflog。遗憾的是，这只对本地分支起作用，且它只跟踪动作 (例如，不会跟踪一个没有被记录的文件的任何改变)。 1234(master)$ git reflog0a2e358 HEAD@&#123;0&#125;: reset: moving to HEAD~20254ea7 HEAD@&#123;1&#125;: checkout: moving from 2.2 to masterc10f740 HEAD@&#123;2&#125;: checkout: moving from master to 2.2 上面的reflog展示了从master分支签出(checkout)到2.2 分支，然后再签回。 那里，还有一个硬重置(hard reset)到一个较旧的提交。最新的动作出现在最上面以 HEAD@{0}标识. 如果事实证明你不小心回移(move back)了提交(commit), reflog 会包含你不小心回移前master上指向的提交(0254ea7)。 1$ git reset --hard 0254ea7 然后使用git reset就可以把master改回到之前的commit，这提供了一个在历史被意外更改情况下的安全网。 (摘自). 其它资源(Other Resources)书(Books) Pro Git - Scott Chacon’s excellent git book Git Internals - Scott Chacon’s other excellent git book 教程(Tutorials) Learn Git branching 一个基于网页的交互式 branching/merging/rebasing 教程 Getting solid at Git rebase vs. merge git-workflow - Aaron Meurer的怎么使用Git为开源仓库贡献 GitHub as a workflow - 使用GitHub做为工作流的趣事, 尤其是空PRs 脚本和工具(Scripts and Tools) firstaidgit.io 一个可搜索的最常被问到的Git的问题 git-extra-commands - 一堆有用的额外的Git脚本 git-extras - GIT 工具集 – repo summary, repl, changelog population, author commit percentages and more git-fire - git-fire 是一个 Git 插件，用于帮助在紧急情况下添加所有当前文件, 做提交(committing), 和推(push)到一个新分支(阻止合并冲突)。 git-tips - Git小提示 git-town - 通用，高级Git工作流支持！ http://www.git-town.com GUI客户端(GUI Clients) GitKraken - 豪华的Git客户端 Windows, Mac &amp; Linux git-cola - 另外一个Git客户端 Windows &amp; OS X GitUp - 一个新的Git客户端，在处理Git的复杂性上有自己的特点 gitx-dev - 图形化的Git客户端 OS X Source Tree - 免费的图形化Git客户端 Windows &amp; OS X Tower - 图形化Git客户端 OS X(付费) Sectiongit 分支操作 1 git fetch 2 git branch -a 3.1 git branch -d demo # 删除已经merge的分支 3.2 git branch -D demo # 删除分支忽略是否合并 4 删除远程分支 4.1 git push –delete origin foo 4.2 git push origin :foo git log –pretty=oneline –abbrev-commitgit commit –amend 修改提交msggit merge 出现冲突时 git状态为merging ,使用 1git mergetool 查看并解决冲突 .gitignore 添加忽略文件无效123git rm -r --cached . #注意 .# 重新添加文件git add . 删除git缓存区文件1git clean -f 分支基本操作 git fetch git pull 会把fetch的数据做到版本库 git branch -a 查看所有分支 git checkout -b branch 以当前分支为基础创建分支并转到分支 git branch -d 删除已经merge的分支 git branch -D 删除分支 git merge branchname merge 一个分支 git push origin :branchname #删除远程分支 git push origin –delete {the_remote_branch} 创建并检出分支: git checkout -b 新分支名称 新分支源分支 git checkout -b newbranchName master 分支合并 合并方法 1、直接合并：把两条分支上的历史轨迹合并，交汇到一起 2、压合合并：一条分支上若干提交条目压合成一个提交条目，提交到另一条分支的末梢 3、拣选合并：拣选另一条分支上的某个提交条目的改动带到当前分支上 直接合并 git merge 分支名称 git checkout alternate git add about.html git commit -m “add about page” git checkout master git merge alternate 压合合并 git merge –squash 分支名称 git checkout -b contact master git add contact.html git commit -m “add contact file” git commit -m “add contact file 2” -a git checkout master git merge –squash contact git status git commit -m “add contact page” -m “has primary and secondary email” 拣选合并 git cherry-pick 提交名称 git checkout contact git commit -m “add contact 3” -a [contact 6dbaf82]…… git checkout master git cherry-pick 6dbaf82 / git cherry-pick -n 6dbaf82 git checkout git checkout git 检出文件 git checkout branchname –file git pull 出现冲突,放弃本地修改,使用远程内容强制覆盖本地代码12git fetch --all # 只是下载代码到本地，不进行合并操作git reset --hard origin/master # 把HEAD指向最新下载的版本 git 指定克隆目录 并指定克隆深度 git clone https://github.com/magicmonty/bash-git-prompt.git .directory_name –depth=1 git clone 指定分支1git clone -b dev https://github.com/cheft/minrouter.git git CRLF /LF1234567891011git config core.autocrlf true# 全局配置 CRLF配置 windows git config --global core.autocrlf true git config --global core.safecrlf true#linux git config --global core.autocrlf input git config --global core.safecrlf true]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux-basic]]></title>
    <url>%2F2017%2F12%2F08%2Flinux-basic%2F</url>
    <content type="text"><![CDATA[重要文件以及位置 /dev/null # 输入流 blackhole /etc/profile 系统环境变量,为每个用户设置环境信息,当用户登陆时执行,并从/etc/profile.d 目录的配置文件中搜集 shell 的设置. /etc/bashrc 在执行完/etc/profile 之后如果用户是shell 运行的是bash,就会执行.可以用来设置每次登陆的时候都会去获取这些新的环境变量或者做某些特殊的操作,但是仅仅在登陆时 ~/.bashrc该文件包含专用于单个人的bash shell 的bash 信息,当登录时以及每次打开一个新的shell 时, 该该文件被读取.单个用户此文件的修改会影响到他以后的每一次登陆系统和每一次新开一个bash .因此,可以在这里设置单个用户的特殊的环境变量或者特殊的操作,那么每次它新登陆系统或者新开一个bash ,都会去获取相应的特殊的环境变量和特殊操作 ~/.bash_logout当每次退出系统( 退出bash shell) 时, 执行该文件 shell 基本命令变量类型运行shell时,会同时存在三种变量: 局部变量 局部变量在脚本或命令中定义,仅在当前shell实例中有效,其他shell启动的程序不能访问局部变量. 环境变量 所有的程序,包括shell启动的程序,都能访问环境变量,有些程序需要环境变量来保证其正常运行.必要的时候shell脚本也可以定义环境变量. shell变量 shell变量是由shell程序设置的特殊变量.shell变量中有一部分是环境变量,有一部分是局部变量,这些变量保证了shell的正常运行. 查看shell变量1root@ziggle:~# env set设置bash变量 unset清除 本地|系统变量 exprot用于把变量变成当前shell 和其子shell 的环境变量,存活期是当前的shell 及其子shell ,因此重新登陆以后,它所设定的环境变量就消失了 source 特殊变量 变量 含义 $0 当前脚本的名字 $n 传递给脚本或函数的参数, n是一个数字, 表示第几个参数, 例如, 第一个参数是$1, 第二个参数是$2。 $# 传递给脚本或函数的参数个数 $* 传递给脚本或函数的所有参数 $@ 传递给脚本或函数的所有参数。被双引号(“ “)包含时, 与 $* 稍有不同 $? 上个命令的退出状态, 或函数的返回值 $$ 当前Shell进程ID。对于 Shell 脚本, 就是这些脚本所在的进程ID 转义字符123456789转义字符 含义\\ 反斜杠\a 警报, 响铃\b 退格（删除键）\f 换页(FF), 将当前位置移到下页开头\n 换行\r 回车\t 水平制表符（tab键） \v 垂直制表符 命令替换命令替换是指Shell可以先执行命令, 将输出结果暂时保存, 在适当的地方输出。语法：123456789101112`command````ZZ- eg: ```bash#!/bin/bashDATA=`date`echo &quot;Date is $&#123;$DATA&#125;&quot;``` # CentOS 升级python3.x## 源码安装注意 tar Jxvf Python-3.5.1.tar.xz cd Python-3.5.1 ./configure –prefix=/usr/local/python3 make &amp;&amp; make install12## - 备份旧版本 `Python` mv /usr/bin/python /usr/bin/python2.71- 新建指向新版本 Python 以及 pip 的软连接 ln -s /usr/local/python3/bin/python3.5 /usr/bin/pythonln -s /usr/local/python3/bin/pip3 /usr/bin/pip1- 检验 `Python` 及 `pip` 版本 python -Vpip -V1234567891011&gt; `CentOS` `yum` 会使用 `python` 更改错误文件 `python` 版本为老版本## 清空文件内容&gt; 方法1 ```bash$ &gt; access.log #or$ true &gt; access.log #or$ cat /dev/null &gt; access.log # or$ cp /dev/null access.log #or$ echo &gt; access.log 方法21$ truncate -s 0 access.log source当修改 /etc/profile 文件时,立即生效文件12source filename # 或. filename source命令通常用于重新执行刚修改的初始化文件, 使之立即生效, 而不必注销并重新登录 Linux network config123456789101112131415161718192021pi@raspberrypi:~ $ sudo cat /etc/network/interfaces# interfaces(5) file used by ifup(8) and ifdown(8)# Please note that this file is written to be used with dhcpcd# For static IP, consult /etc/dhcpcd.conf and &apos;man dhcpcd.conf&apos;# Include files from /etc/network/interfaces.d:# source-directory /etc/network/interfaces.dauto loiface lo inet loopbackiface eth0 inet dhcpallow-hotplug wlan0iface wlan0 inet manualwpa-roam /etc/wpa_supplicant/wpa_supplicant.confiface default inet staticaddress 192.168.192.113netmask 255.255.255.0gateway 192.168.192.168 wpa_supplicant.conf12345678910pi@raspberrypi:~ $ sudo cat /etc/wpa_supplicant/wpa_supplicant.conf country=GBctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network=&#123; ssid=&quot;wifi-name&quot; key_mgmt=WPA-PSK psk=&quot;12345678&quot;&#125; vim plugins curl https://j.mp/spf13-vim3 -L &gt; spf13-vim.sh &amp;&amp; sh spf13-vim.sh 配置DNS1234 GNU nano 2.7.4 File: /etc/resolv.conf # Generated by resolvconf# nameserver 192.168.2.1nameserver 114.114.114.114 再环境变量中设置代理 配置环境变量12export http_proxy=http://127.0.0.1:1080export https_proxy=http://127.0.0.1:1080 使用配置文件(wget) 为wget配置代理1234567~/.wgetrc# They will override the value in the environment. http_proxy=http://127.0.0.1:1080 https_proxy=http://127.0.0.1:1080 ftp_proxy=http://127.0.0.1:1080# If you do not want to use proxy at all, set this to off. use_proxy=on top 命令的使用top 命令是 Linux 下常用的性能分析工具, 能够实时显示系统中各个进程的资源占用状况, 常用于服务端性能分析。top 结果分为两个部分 统计信息： 前五行是系统整体的统计信息； 进程信息：统计信息下方类似表格区域显示的是各个进程的详细信息, 默认5秒刷新一次。 top 命令选项: -b：以批处理模式操作；-c：显示完整的治命令；-d：屏幕刷新间隔时间；-I：忽略失效过程；-s：保密模式；-S：累积模式；-i&lt;时间&gt;：设置间隔时间；-u&lt;用户名&gt;：指定用户名；-p&lt;进程号&gt;：指定进程；-n&lt;次数&gt;：循环显示的次数 top 命令交互: Systemd 使用 首先 ssocks.service123456789101112root@ziggle:~# cat /lib/systemd/system/ssocks.service[Service]ExecStart=-/usr/local/bin/ssserver -c /etc/shadowsocks.json startExecReload=-/bin/kill -HUP $MAINPIDTimeoutSec=10sType=simpleKillMode=processRestart=alwaysRestartSec=2s[Install]WantedBy=multi-user.target 显示所有已启动的服务 systemctl list-units –type=service启动某服务systemctl start httpd.service修改配置文件后重启12345# 重新加载配置文件$ sudo systemctl daemon-reload# 重启相关服务$ sudo systemctl restart foobar 开机启动123456789101112root@ziggle:~# systemctl enable ssocks.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/ssocks.service to /lib/systemd/system/ssocks.service.# 开机禁用systemctl disable ssocks.service#查看开机是否启动systemctl is-enabled ssocks.service #查询服务是否开机启动# systemd查看开机自启动的程序ls /etc/systemd/system/multi-user.target.wants/ 一旦修改配置文件, 就要让 SystemD 重新加载配置文件, 然后重新启动, 否则修改不会生效。12$ sudo systemctl daemon-reload$ sudo systemctl restart ssocks.service systemd 系统管理命令1234567891011121314151617181920# 重启系统$ sudo systemctl reboot# 关闭系统, 切断电源$ sudo systemctl poweroff# CPU停止工作$ sudo systemctl halt# 暂停系统$ sudo systemctl suspend# 让系统进入冬眠状态$ sudo systemctl hibernate# 让系统进入交互式休眠状态$ sudo systemctl hybrid-sleep# 启动进入救援状态（单用户状态）$ sudo systemctl rescue 配置文件的区块12345678910Description：简短描述Documentation：文档地址Requires：当前 Unit 依赖的其他 Unit, 如果它们没有运行, 当前 Unit 会启动失败Wants：与当前 Unit 配合的其他 Unit, 如果它们没有运行, 当前 Unit 不会启动失败BindsTo：与Requires类似, 它指定的 Unit 如果退出, 会导致当前 Unit 停止运行Before：如果该字段指定的 Unit 也要启动, 那么必须在当前 Unit 之后启动After：如果该字段指定的 Unit 也要启动, 那么必须在当前 Unit 之前启动Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行Condition：当前 Unit 运行必须满足的条件, 否则不会运行Assert：当前 Unit 运行必须满足的条件, 否则会报启动失败 [Service]区块用来 Service 的配置, 只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。*1234567891011121314151617Type：定义启动时的进程行为。它有以下几种值。Type=simple：默认值, 执行ExecStart指定的命令, 启动主进程Type=forking：以 fork 方式从父进程创建子进程, 创建后父进程会立即退出Type=oneshot：一次性进程, Systemd 会等当前服务退出, 再继续往下执行Type=dbus：当前服务通过D-Bus启动Type=notify：当前服务启动完毕, 会通知Systemd, 再继续往下执行Type=idle：若有其他任务执行完毕, 当前服务才会运行ExecStart：启动当前服务的命令ExecStartPre：启动当前服务之前执行的命令ExecStartPost：启动当前服务之后执行的命令ExecReload：重启当前服务时执行的命令ExecStop：停止当前服务时执行的命令ExecStopPost：停止当其服务之后执行的命令RestartSec：自动重启当前服务间隔的秒数Restart：定义何种情况 Systemd 会自动重启当前服务, 可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdogTimeoutSec：定义 Systemd 停止当前服务之前等待的秒数Environment：指定环境变量 ref : http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html sh 使用公钥连接 1ssh -i ~/.ssh/my-ssh-key [USERNAME]@[EXTERNAL_IP_ADDRESS] wsl 连接配置 装上 ubuntu on windows 后, 默认要先打开 cmd, 再运行 bash 进入 ubuntu 的 shell。但是这个shell很难看, 配色不好就算了, 还存在各种复制粘贴麻烦、默认没进入 home 目录、各种报警声等问题。所以尝试用 xshell 登陆 ubuntu这里主要讲几个关键步骤 卸载 ssh-server sudo apt-get remove openssh-server 安装 ssh-server sudo apt-get install openssh-server 修改 ssh server 配置 sudo vim /etc/ssh/sshd_config需要修改以下几项： Port 2222 #默认的是22, 但是windows有自己的ssh服务, 也是监听的22端口, 所以这里要改一下UsePrivilegeSeparation noPasswordAuthentication yesAllowUsers youusername # 这里改成你登陆WSL用的 启动 ssh server sudo service ssh –full-restart现在就可以用 xshell 登陆 ubuntu on windows 了, IP 是 127.0.0.1, 但是要注意, cmd 的窗口还不能关掉。关掉后 sshd 服务也会关掉, 连接就断开了。这个问题目前还没找到解决办法。 rpm -ivh jdk-10.interim.update.patch_linux-x64_bin.rpm 用户、权限-相关命令 修改普通用户 youmeek 的权限跟 root 权限一样 常用方法（原理是把该用户加到可以直接使用 sudo 的一个权限状态而已）： 编辑配置文件：vim /etc/sudoers找到 98 行（预估）, 有一个：root ALL=(ALL) ALL, 在这一行下面再增加一行, 效果如下：12root ALL=(ALL) ALLyoumeek ALL=(ALL) ALL 另一种方法： 编辑系统用户的配置文件：vim /etc/passwd, 找到 root 和 youmeek 各自开头的那一行, 比如 root 是：root:x:0:0:root:/root:/bin/zsh, 这个代表的含义为：用户名:密码:UserId:GroupId:描述:家目录:登录使用的 shell通过这两行对比, 我们可以直接修改 youmeek 所在行的 UserId 值 和 GroupId 值, 都改为 0。]]></content>
      <tags>
        <tag>-linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx-config]]></title>
    <url>%2F2017%2F12%2F08%2Fnginx-config%2F</url>
    <content type="text"><![CDATA[Nginx settings12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576user www-data;# 与cpu核心数一致worker_processes 1;pid /run/nginx.pid;events &#123; use epoll; worker_connections 768;&#125;http &#123; # 开启gzip gzip on; gzip_vary on; gzip_proxied any; gzip_comp_level 6; gzip_buffers 16 8k; gzip_http_version 1.1; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; # websocket 配置 map $http_upgrade $connection_upgrade &#123; default upgrade; '' close; &#125; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; server &#123; # 监听端口 listem 80; # 域名 server_name ziggle.com www.ziggle.com; # 站点目录 root /root/ziggle; location / &#123; index index.html; &#125; # 正则匹配路径, 用~开头 location ~* \.(gif|jpg|png)$ &#123; expires 10d; &#125; # websocket conf location /chat/ &#123; # 默认websocket超时时间是 60s proxy_read_timeout 3600s; proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; &#125; &#125; include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; gzip on; gzip_disable "msie6";# 包含配置 include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*;&#125;mail &#123;&#125; 包含文件的例子 rails.conf 12 所有的动态页面交给tomcat 等处理1234567location ~ .(jsp|jspx|do)?$ &#123; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; &#125; 所有静态文件由nginx直接读取1234567location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt |pdf|xls|mp3|wma)$ &#123; expires 15d; # 静态文件缓存时间 &#125; location ~ .*.(js|css)?$ &#123; expires 1h; &#125; nginx 中upstream轮询机制 轮询 后端服务器down掉,可以自动删除 1234upstream bakend &#123; server 192.168.1.10; server 192.168.1.11; &#125; weight 1234upstream bakend &#123; server 192.168.1.10 weight=1; server 192.168.1.11 weight=2; &#125; ip_hash 每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session不能跨服务器的问题。如果后端服务器down掉，要手工down掉 123456upstream resinserver&#123; ip_hash; server 192.168.1.10:8080; server 192.168.1.11:8080; &#125; fair (插件) 根据相应时间有限分配 12345upstream resinserver&#123; server 192.168.1.10:8080; server 192.168.1.11:8080; fair; &#125; 定义错误页面123error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; shadowsocks进程检测 1 添加cron 任务 1crontab -e 2 编辑crontab 1* * * * * /bin/sh /root/wp/ssscript.sh 3 编辑ssscript.sh 并且 chmod 775 ./ssscript.sh 123456789 #!/bin/shps -fe|grep /usr/local/bin/ssserver|grep -v grepif [ $? -ne 0 ]then /usr/local/bin/ssserver -c /etc/shadowsocks.json -d start echo `date "+%Y-%m-%d %H:%M:%S"beenfunked` &gt;&gt; /root/wp/ss.logelse echo `date "+%Y-%m-%d %H:%M:%S"OK` &gt;&gt; /root/wp/ss.logfi python 调用系统命令 123456789#!/bin/pythonimport osif __name__ == "__main__": # os.chdir('/root/www/blog/') os.system('git pull origin master') # os.chdir('/root/www/resource/') os.system('git pull origin master') nginx 配置ssl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 upstream consul &#123; server 127.0.0.1:8500; &#125; # https 监听80端口可以让http重定向到https端口上。 server &#123; # listen at port 80 listen 80; # server_name www.example.com; rewrite ^(.*) https://$host$1 permanent; # 不用 $server_name ,使用$host 变量 # location / &#123;# root /root/wp/www/public;# &#125;# location /consul/ &#123;# proxy_pass http://consul/;# &#125; error_page 404 /404.html; location = /404.html &#123; root /root/wp/www/; internal; &#125; &#125;server &#123; listen 443 ssl; ssl on; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #指定SSL服务器端支持 协议版本 ssl_ciphers HIGH:!aNULL:!MD5; ssl_certificate /root/sslcert/ssl/certificate.crt; ssl_certificate_key /root/sslcert/ssl/private.key; ssl_prefer_server_ciphers on; location / &#123; root /root/wp/www/public; &#125; location /consul/ &#123; proxy_pass http://consul/; &#125; error_page 404 /404.html; location = /404.html &#123; root /root/wp/www/; internal; &#125; &#125; nginx websocket configuration123456789101112131415161718192021222324252627http &#123; server &#123; listen 3000; server_name io.yourhost.com; location / &#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $host; proxy_pass http://nodes; # enable WebSockets proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; &#125; &#125; upstream nodes &#123; # enable sticky session based on IP ip_hash; server app01:3000; server app02:3000; server app03:3000; &#125;&#125; nginx 日志记录自定义请求头Client send to server a custom header. Sample, send a device_id header to server. In nginx, we capture this header and write to access_log for debug, monitor, route request. We need to config nginx that: Enable underscores directive in http selection. 1underscores_in_headers on; Set header to a variable (if header is customheader, variable is http + custom_header 1proxy_set_header device_id $http_device_id; Re-config log format in nginx config 1log_format main &apos;&quot;device_id:$http_device_id&quot;&apos; Send a request with custom header 1curl -X POST -H 'device_id:57cfd83ee4b094690a1db5d6' https://localhost.local/api/log/write]]></content>
      <tags>
        <tag>nginx</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins-config]]></title>
    <url>%2F2017%2F12%2F05%2Fjenkins-config%2F</url>
    <content type="text"><![CDATA[jenkins agent 配置 项目配置 项目源码 项目构建命令 项目构建trigger]]></content>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发布proget私库]]></title>
    <url>%2F2017%2F11%2F29%2F%E5%8F%91%E5%B8%83proget%E7%A7%81%E5%BA%93%2F</url>
    <content type="text"><![CDATA[发布proget私库1. 生成 .nupkg 文件项目根目录运行 1nuget pack .\[library-project.csproj] -Build -Properties owners=ziggle;version=&quot;1.0.0&quot; 示例 2. push包到proget1nuget push .\[library-project.1.0.0.nupkg] -source https://api.nuget.org/v3/index.json -apikey [your-secoret-key] 3. netcore 使用步骤 3.1 pack &amp; push 1234&#123; "pack": "dotnet pack -c Release", "push" : "dotnet nuget push [library-project.1.0.0.nupkg] -source https://api.nuget.org/v3/index.json -apikey [your-secoret-key]"&#125; 参考 nuget netcore]]></content>
      <tags>
        <tag>nuget ,netcore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo document tag plugins]]></title>
    <url>%2F2017%2F11%2F28%2Fhexo-document-tag-plugins%2F</url>
    <content type="text"><![CDATA[使用引用123&#123;% blockquote [author[, source]] [link] [source_link_title] %&#125;content&#123;% endblockquote %&#125; 样例Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem. 加入Image123&#123;% img [class names] /path/to/image [width] [height] [title text [alt text]] %&#125;&#123;% asset_img example.jpg This is an example image %&#125; # 通过这种方式，图片将会同时出现在文章和主页以及归档页中。 加入 jsFiddle1&#123;% jsfiddle shorttag [tabs] [skin] [width] [height] %&#125; 显示文章预览1Next 使用 &lt;!-- more --&gt; 超链接 文字 1[链接文章](链接地址 "链接标题") 图片 1![图片说明](图片链接 "图片标题") 视频 12&lt;script src="/js/youtube-autoresizer.js"&gt;&lt;/script&gt;&lt;iframe width="640" height="360" src="https://www.youtube.com/embed/HfElOZSEqn4" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt; 引用使用 &gt; 表示文字引用 野火烧不尽，春风吹又生 绘制表格 绘制表格格式如下，| 控制分列，- 控制分行，: 控制对齐方式。12345| Item | Value | Qty || :------- | --------: | :---: || Computer | 1600 USD | 5 || Phone | 12 USD | 12 || Pipe | 1 USD | 234 | Item Value Qty Computer 1600 USD 5 Phone 12 USD 12 Pipe 1 USD 234 强调字体倾斜 字体倾斜 字体加粗 字体加粗_ 分割线 删除线 删除线 图片连接: 链接: 123456789101112131415*字体倾斜*_字体倾斜_**字体加粗**__字体加粗___&gt;分割线____ 或者 ***&gt; 删除线~~删除线~~]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iis express custom hostname]]></title>
    <url>%2F2017%2F11%2F28%2Fiis-express-custom-hostname%2F</url>
    <content type="text"><![CDATA[使用vs 2017 iisexpress 调试web项目是如果项目绑定某个域名这时候就需要编辑1$(solutionDir)\.vs\config\applicationhost.config 找到文件中site 节点12345678&lt;site name=”MyWebApp” id=”2”&gt; &lt;application path=”/“ applicationPool=”Clr4IntegratedAppPool”&gt; &lt;virtualDirectory path="/" physicalPath="rpoject root path" /&gt; &lt;/application&gt; &lt;bindings&gt; &lt;binding protocol="http" bindingInformation="*:12345:customerhostname" /&gt; &lt;/bindings&gt;&lt;/site&gt; 然后配置项目属性 更改project url .]]></content>
      <tags>
        <tag>iis-express</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paraller]]></title>
    <url>%2F2017%2F11%2F27%2Fparaller%2F</url>
    <content type="text"><![CDATA[Paraller 并发请求123456789var url = &quot;http://112.124.121.109:11250/order/1230a611-e189-4296-a50d-b48910bc1850&quot;; var client = new HttpClient(); Enumerable.Range(1, 10).AsParallel().ToList().ForEach(async (item) =&gt; &#123; var watch = Stopwatch.StartNew(); var res = await client.GetStringAsync(url); watch.Stop(); Console.WriteLine($&quot;&#123;watch.ElapsedMilliseconds&#125; : &#123;item&#125;&quot;); &#125;);]]></content>
  </entry>
</search>
